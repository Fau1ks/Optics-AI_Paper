[
    {
        "title": "Self-attention-based mixture-of-experts framework for non-invasive prediction of MGMT promoter methylation in glioblastoma using multi-modal MRI",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Self-attention-based mixture-of-experts framework for non-invasive prediction of MGMT promoter methylation in glioblastoma using multi-modal MRI"
        },
        "summary": "[New Paper] Abstract not indexed yet. Please visit the official website.",
        "summary_detail": {
            "type": "text/html",
            "language": null,
            "base": "",
            "value": "<p>Publication date: April 2026</p><p><b>Source:</b> Displays, Volume 92</p><p>Author(s): Yuehui Liao, Yun Zheng, Jingyu Zhu, Yu Chen, Feng Gao, Yaning Feng, Weiji Yang, Guang Yang, Xiaobo Lai, Panfei Li</p>"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.sciencedirect.com/science/article/pii/S0141938226000211?dgcid=rss_sd_all"
            }
        ],
        "link": "https://www.sciencedirect.com/science/article/pii/S0141938226000211",
        "id": "https://www.sciencedirect.com/science/article/pii/S0141938226000211",
        "guidislink": false,
        "journal": "Displays",
        "title_cn": "基于自注意力的专家混合框架，利用多模态 MRI 无创预测胶质母细胞瘤 MGMT 启动子甲基化",
        "abstract_cn": "【摘要未收录】新文章暂无数据库记录，请点击链接直达官网。"
    },
    {
        "title": "S3CRAD: Superpixel-guided background inpainting and spatial-spectral constrained representation for hyperspectral anomaly detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "S3CRAD: Superpixel-guided background inpainting and spatial-spectral constrained representation for hyperspectral anomaly detection"
        },
        "summary": "[New Paper] Abstract not indexed yet. Please visit the official website.",
        "summary_detail": {
            "type": "text/html",
            "language": null,
            "base": "",
            "value": "<p>Publication date: June 2026</p><p><b>Source:</b> Optics and Lasers in Engineering, Volume 201</p><p>Author(s): Mingtao You, Yiming Yao, Dong Zhao, Zhe Zhao, Pattathal V. Arun, Yian Wang, Huixin Zhou, Ronghua Chi</p>"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.sciencedirect.com/science/article/pii/S014381662600062X?dgcid=rss_sd_all"
            }
        ],
        "link": "https://www.sciencedirect.com/science/article/pii/S014381662600062X",
        "id": "https://www.sciencedirect.com/science/article/pii/S014381662600062X",
        "guidislink": false,
        "journal": "Optics and Lasers in Engineering",
        "title_cn": "S3CRAD：用于高光谱异常检测的超像素引导背景修复和空间光谱约束表示",
        "abstract_cn": "【摘要未收录】新文章暂无数据库记录，请点击链接直达官网。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647829",
        "title": "Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation",
        "link": "https://doi.org/10.1109/tpami.2025.3647829",
        "published": "2026",
        "author": "Yifei Shi, Boyan Wan, Xin Xu, Kai Xu",
        "summary": "Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the $5^{\\circ }2$cm metric on NOCS-REAL275, 0.62 in the $5^{\\circ }5$cm metric on ShapeNet-C, and 77.3 in the AR metric on LineMOD-O. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "学习神经隐式场中的正激励点采样以进行物体姿态估计",
        "abstract_cn": "学习 3D 形状的神经隐式场是一个快速新兴的领域，可以实现任意分辨率的形状表示。由于灵活性，神经隐式领域在许多研究领域取得了成功，包括形状重建、新颖的视图图像合成以及最近的物体姿态估计。神经隐式场能够学习相机空间和物体规范空间之间的密集对应关系（包括相机空间中未观察到的区域），从而在高度遮挡的物体和新颖形状等具有挑战性的场景中显着提高物体姿态估计性能。尽管取得了进展，但由于缺乏直接观测信号，预测未观测到的相机空间区域的规范坐标仍然具有挑战性。这需要严重依赖模型的泛化能力，从而导致很高的不确定性。因此，整个相机空间上的密集采样点可能会产生不准确的估计，从而阻碍学习过程并损害性能。为了缓解这个问题，我们提出了一种结合 SO(3) 等变卷积隐式网络和正激励点采样策略的方法。 SO(3) 等变卷积隐式网络在任意查询位置估计具有 SO(3) 等变的点级属性，与大多数现有基线相比，表现出了卓越的性能。该策略根据输入动态确定采样位置，从而提高网络的准确性和训练效率。该策略是通过估计网络来实现的，该网络生成具有独特特征的稀疏样本点，能够以高确定性确定所有物体姿态自由度。为了收集估计网络的训练数据，我们建议使用教师模型自动生成伪地面实况。我们的方法在三个姿态估计数据集上优于最先进的方法。它在 NOCS-REAL275 上的 $5^{\\circ }2$cm 指标中达到 0.63，在 ShapeNet-C 上的 $5^{\\circ }5$cm 指标中达到 0.62，在 LineMOD-O 上的 AR 指标中达到 77.3。值得注意的是，它在具有挑战性的场景中展示了显着的改进，例如以看不见的姿势捕获的对象、高遮挡、新颖的几何形状和严重的噪声。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647921",
        "title": "Robust Semi-supervised Feature Selection with Multi-granularity Zentropy Modeling",
        "link": "https://doi.org/10.1109/tpami.2025.3647921",
        "published": "2025",
        "author": "Kehua Yuan, Duoqian Miao, Weiping Ding, Witold Pedrycz, Yiyu Yao",
        "summary": "High-dimensional and weakly supervised (HiDWS) data present significant challenges for traditional machine learning and pattern recognition. Although semi-supervised feature selection has shown effectiveness in improving the quality of HiDWS data, existing methods remain sensitive and lack robustness due to the unreliability of unlabeled data learning and the uncertainty in modeling processes. Hence, this study focuses on a multi-granularity zentropy modeling (Ze-MGM) framework with model-agnostic for highly-accuracy and robust semi-supervised feature selection. Unlike existing methods, Ze-MGM does not rely on specific settings such as rough or fuzzy set assumptions and can effectively capture the granularity of information under high-dimensional and weakly supervised data scenarios. Specifically, we first introduce a strategic soft label ($S2-$Label) learning method that integrates object proximity and classification certainty to reduce uncertainty between features and labels. This method also enables the selection of compatible instances, thereby mitigating the negative impact of incompatible objects on label learning. Subsequently, a multi-granularity knowledge space and zentropy uncertainty measure are constructed by analyzing the hierarchical relationships among labels, decisions, and specific classes, which enables accurate multi-granularity knowledge representation and multi-granularity uncertainty characterization in HiDWS data modeling processing. Finally, two multi-granularity significance measures based on multi-granularity uncertainty are defined for feature evaluation and selection via a semi-supervised paradigm. Extensive experiments on multiple benchmark datasets demonstrate that the proposed Ze-MGM method achieves superior generalization performance and robustness compared to state-of-the-art methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "具有多粒度 Zentropy 建模的鲁棒半监督特征选择",
        "abstract_cn": "高维弱监督（HiDWS）数据给传统机器学习和模式识别带来了重大挑战。尽管半监督特征选择在提高 HiDWS 数据质量方面显示出有效性，但由于无标签数据学习的不可靠性和建模过程的不确定性，现有方法仍然敏感且缺乏鲁棒性。因此，本研究重点关注多粒度熵建模（Ze-MGM）框架，该框架与模型无关，可实现高精度和鲁棒的半监督特征选择。与现有方法不同，Ze-MGM不依赖于粗糙或模糊集假设等特定设置，可以有效捕获高维和弱监督数据场景下的信息粒度。具体来说，我们首先引入一种策略性软标签（$S2-$Label）学习方法，该方法集成了对象邻近度和分类确定性，以减少特征和标签之间的不确定性。该方法还可以选择兼容的实例，从而减轻不兼容对象对标签学习的负面影响。随后，通过分析标签、决策和特定类之间的层次关系构建多粒度知识空间和熵不确定性测度，从而在HiDWS数据建模处理中实现准确的多粒度知识表示和多粒度不确定性表征。最后，定义了两种基于多粒度不确定性的多粒度显着性度量，用于通过半监督范式进行特征评估和选择。对多个基准数据集的大量实验表明，与最先进的方法相比，所提出的 Ze-MGM 方法实现了卓越的泛化性能和鲁棒性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648020",
        "title": "Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking",
        "link": "https://doi.org/10.1109/tpami.2025.3648020",
        "published": "2025",
        "author": "Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du",
        "summary": "Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model's sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "Causal HyperPrompter：无偏高光谱伪装对象跟踪框架",
        "abstract_cn": "由于物体和复制品在纹理和颜色上的高度相似性，高光谱伪装物体跟踪仍然是一个重大挑战。尽管最近取得了进展，但跟踪器和嵌入令牌中存在的偏差阻碍了模型训练。具体来说，大多数方法依赖于伪彩色三通道图像来微调基于 RGB 的跟踪器。然而，它在 RGB 域内引入了混杂效应，可能会导致有害的偏差，从而将模型误导至虚假相关性，同时忽略了高光谱图像中固有的关键光谱辨别力。此外，当前的令牌类型嵌入方法忽略了模板和搜索之间的关键相关性，最终混淆了相关性并损害了跟踪性能。为了应对这些挑战，本文提出了一种新的无偏见跟踪框架，名为 Causal HyperPrompter。它首先引入了结构因果模型来理清和控制跟踪过程中的排他性因果因素，并采用反事实干预策略来消除混杂变量并减轻基于 RGB 的模型继承的偏差。此外，我们提出了一种新颖的令牌型嵌入模块，该模块集成了局部光谱角度建模，以增强模板和搜索令牌之间的语义链接，从而提高模型对对象定位的敏感性。最后，为了克服手动初始化边界框和解决数据稀缺的困难，我们引入了大规模高光谱伪装对象检测和跟踪数据集 BihoT-130 k，由跨各种伪装场景的 130750 个带注释的帧组成。对多个大规模数据集的广泛实验证明了我们提出的方法的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647880",
        "title": "Toward Unified Expertise: Learning a Single Vision Model from Diverse Perception",
        "link": "https://doi.org/10.1109/tpami.2025.3647880",
        "published": "2025",
        "author": "Zitian Chen, Mingyu Ding, Yikang Shen, Erik Learned-Miller, Chuang Gan",
        "summary": "Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a \"squad\" of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model's generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad's modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "迈向统一的专业知识：从不同的感知中学习单一视觉模型",
        "abstract_cn": "由于任务之间的梯度冲突，多任务学习 (MTL) 比单任务学习 (STL) 提出了更大的优化挑战。虽然参数共享促进了相关任务之间的合作，但许多任务需要专门的表示。为了平衡合作和专业化，我们提出了 Mod-Squad [1]，这是一种由专家“小队”组成的基于模块化变压器的模型。每个任务都会通过可微的匹配过程激活稀疏的专家子集，并以新颖的基于相互信息的损失为指导。这种模块化结构避免了完全骨干共享，并可根据任务数量和数据集大小有效扩展。在此扩展版本中，我们概括了 Mod-Squad 以支持多数据集预训练，从而能够跨不相交的单任务数据集（例如 ImageNet、COCO、ADE20K）进行联合学习。这是通过一种新的互信息损失公式来实现的，该公式统一了跨异构源的学习。更重要的是，虽然大型模型的大多数先前工作都集中在效率上，但很少有人探索可调节的效率。在本研究中，我们进一步评估了模型对下游任务的泛化能力，并引入了一套高效的适应技术，利用 Mod-Squad 的模块化进行灵活的微调，实现模型大小、参数数量和计算成本的动态调整。此外，我们提出了一种混合适应方案，该方案结合了这些技术以实现有利的性能效率权衡。总之，Mod-Squad 为稀疏模块化模型提供了坚实的基础，可以从不同的监督和数据集中学习。其新兴的模块化能够实现强大的泛化、分解为高性能组件，以及对下游应用程序的快速、资源高效的适应。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647855",
        "title": "Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation",
        "link": "https://doi.org/10.1109/tpami.2025.3647855",
        "published": "2026",
        "author": "Jingtao Sun, Yaonan Wang, Mingtao Feng, Chao Ding, Mike Zheng Shou, Ajmal Mian",
        "summary": "Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于形状重建和姿势估计的扩散驱动自监督学习",
        "abstract_cn": "完全监督的类别级姿态估计旨在确定已知类别中未见过的实例的 6-DoF 姿态，需要昂贵的手动标记成本。最近，人们提出了各种自监督类别级姿态估计方法来减少对注释数据集的需求。然而，大多数方法依赖于合成数据或 3D CAD 模型，并且通常仅限于解决单对象姿势问题，而不考虑多目标任务或形状重建。为了克服这些挑战和限制，我们引入了一种扩散驱动的自监督网络，用于多对象形状重建和分类姿势估计，仅利用形状先验。具体来说，为了捕获 SE(3) 等变姿态特征和 3D 尺度不变形状信息，我们提出了一种先验金字塔 3D 点变换器。该模块采用具有径向内核的点卷积层进行姿势感知学习，并采用 3D 尺度不变图卷积层进行对象级形状表示。此外，我们引入了预训练到细化自我监督训练范式来训练我们的网络。它使所提出的网络能够捕获形状先验和观察之间的关联，通过利用扩散机制解决类内形状变化的挑战。在四个公共数据集和一个自建数据集上进行的广泛实验表明，我们的方法显着优于最先进的自监督类别级基线，甚至超过了一些完全监督的实例级和类别级方法。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647857",
        "title": "You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts",
        "link": "https://doi.org/10.1109/tpami.2025.3647857",
        "published": "2025",
        "author": "Hongkun Dou, Zeyu Li, Xingyu Jiang, Hongjue Li, Lijun Yang, Wen Yao, Yue Deng",
        "summary": "Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Shortcut Diffusion Optimization (SDO)</b>, is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\sim 90\\%$</tex-math></inline-formula> while maintaining superior performance.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "您只看一步：使用梯度快捷方式加速扩散采样中的反向传播",
        "abstract_cn": "扩散模型 (DM) 最近在大规模数据分布建模方面取得了显着的成功。然而，许多下游任务需要根据特定的可微指标指导生成的内容，通常需要在生成过程中进行反向传播。这种方法的计算成本很高，因为使用 DM 生成通常需要数十到数百次递归网络调用，从而导致高内存使用率和大量时间消耗。在本文中，我们提出了一种更有效的替代方案，从并行去噪的角度解决该问题。我们证明，在整个生成过程中完全反向传播是不必要的。可以通过在生成过程中仅保留一步的计算图来优化下游指标，从而为梯度传播提供一种捷径。由此产生的方法，我们称之为<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">快捷扩散优化(SDO)</b>，是通用的、高性能的、计算量轻的，能够优化扩散采样中的所有参数类型。我们展示了 SDO 在多个实际任务中的有效性，包括通过优化潜在网络来控制生成以及通过微调网络参数来调整 DM。与完全反向传播相比，我们的方法通过 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\sim 90\\%$</tex-math></inline-formula> 降低了计算成本，同时保持了卓越的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647707",
        "title": "BEVTrack: Multi-View Multi-Human Registration and Tracking in the Bird's Eye View",
        "link": "https://doi.org/10.1109/tpami.2025.3647707",
        "published": "2025",
        "author": "Zekun Qian, Wei Feng, Feifan Wang, Ruize Han",
        "summary": "We handle a new problem of multi-view multi-human tracking in the bird's eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "BEVTrack：鸟瞰多视图多人注册和跟踪",
        "abstract_cn": "我们解决了鸟瞰（BEV）中多视图多人跟踪的新问题。与之前的工作不同，我们既不需要多视点相机之间的校准，也不需要实际捕获的 BEV 视频。这使得所研究的问题更接近现实世界的应用，但也更具挑战性。为此，在这项工作中，我们提出了一种新颖的 BEVTrack 方案。具体来说，给定多视图视频，我们首先使用虚拟 BEV 变换模块来获取每个视图的 BEV。然后，我们提出了一个统一的 BEV 对齐模块来融合各自生成的 BEV，其中我们通过考虑空间一致性和时间连续性来专门设计自监督损失。在推理过程中，我们设计了相机-主体协同配准和跟踪策略，利用多视点相机和多个目标之间的相互依赖性，实现期望的BEV跟踪。我们还建立了新的训练和评估基准，实验结果验证了问题的合理性和方法的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647835",
        "title": "Iterative Differential Entropy Minimization (IDEM) Method for Fine Rigid Pairwise 3D Point Cloud Registration: A Focus on the Metric",
        "link": "https://doi.org/10.1109/tpami.2025.3647835",
        "published": "2025",
        "author": "Emmanuele Barberi, Felice Sfravara, Filippo Cucinotta",
        "summary": "Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于精细刚性成对 3D 点云配准的迭代微分熵最小化 (IDEM) 方法：关注度量",
        "abstract_cn": "点云配准是计算机视觉的中心主题，对齐算法不断改进以提高鲁棒性。常用的方法评估点云之间的欧几里德距离并最小化目标函数，例如均方根误差 (RMSE)。然而，当点云预先对齐良好时，这些方法最为有效，并且密度差异、噪声、孔洞和有限重叠等问题可能会影响结果。传统方法，例如迭代最近点 (ICP)，需要选择一个点云作为固定点，因为欧几里德距离缺乏交换性。当只有一个点云出现问题时，可以进行调整，但在实际场景中，两个点云都可能受到影响，通常需要进行预处理。作者介绍了一种新颖的基于微分熵的度量，旨在用作精细刚性成对 3D 点云配准优化框架内的目标函数，称为迭代微分熵最小化 (IDEM)。该指标不依赖于固定点云的选择，并且在转换过程中，揭示了与最佳对齐相对应的清晰最小值。进行了多个案例研究，并将结果与​​使用 RMSE、Chamfer 距离和 Hausdorff 距离获得的结果进行了比较。即使在密度差异、噪声、孔洞和部分重叠的情况下，所提出的度量也被证明是有效的，其中 RMSE 并不总是产生最佳对齐。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647952",
        "title": "HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving",
        "link": "https://doi.org/10.1109/tpami.2025.3647952",
        "published": "2025",
        "author": "Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao",
        "summary": "In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "HUGSIM：实时、逼真的自动驾驶闭环模拟器",
        "abstract_cn": "在过去的几十年里，自动驾驶算法在感知、规划和控制方面取得了重大进展。然而，评估单个组件并不能完全反映整个系统的性能，这凸显了需要更全面的评估方法。这推动了 HUGSIM 的开发，这是一种用于评估自动驾驶算法的闭环、逼真、实时模拟器。我们通过 3D 高斯分布将捕获的 2D RGB 图像提升到 3D 空间，提高闭环场景的渲染质量并构建闭环环境来实现这一目标。在渲染方面，我们解决了闭环场景中新颖视图合成的挑战，包括视点外推和 360 度车辆渲染。除了新颖的视图合成之外，HUGSIM 还进一步实现了完整的闭环模拟循环，根据控制命令动态更新自我和参与者的状态和观察结果。此外，HUGSIM还提供了涵盖KITTI-360、Waymo、nuScenes和PandaSet的70多个序列以及400多个不同场景的综合基准，为现有自动驾驶算法提供了公平、现实的评估平台。 HUGSIM 不仅可以作为直观的评估基准，还可以释放在逼真的闭环设置中微调自动驾驶算法的潜力。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648017",
        "title": "Adaptive and Robust DBSCAN With Multi-Agent Reinforcement Learning",
        "link": "https://doi.org/10.1109/tpami.2025.3648017",
        "published": "2025",
        "author": "Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Xizhao Wang",
        "summary": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</b>daptive and <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">R</b>obust <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DBSCAN</b> with Multi-agent Reinforcement Learning cluster framework, namely<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">AR-DBSCAN</b>. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">information uncertainty</i> determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the Normalized Mutual Information (NMI) and the Adjusted Rand Index (ARI) metrics, respectively, but also is capable of robustly finding dominant parameters.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "具有多代理强化学习的自适应且鲁棒的 DBSCAN",
        "abstract_cn": "基于密度的噪声应用空间聚类（DBSCAN）是一种著名的基于密度的聚类算法，由于其在识别任意形状的聚类和处理噪声数据方面的有效性而获得了广泛的流行和使用。然而，当面对不同密度尺度的数据集（现实应用中的常见场景）时，它在产生令人满意的聚类结果方面遇到了挑战。在本文中，我们提出了一种新颖的 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</b>自适应和 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">R</b>robust <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DBSCAN</b> 具有多智能体强化学习集群框架，即<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">AR-DBSCAN</b>。首先，我们将初始数据集建模为两级编码树，并根据编码树中确定的信息不确定性将数据顶点分类为不同的密度分区。然后将每个分区分配给一个代理，以在无需手动帮助的情况下找到最佳聚类参数。该分配是密度自适应的，使 AR-DBSCAN 能够通过针对不同分区使用不同的代理来有效处理数据集中的不同密度分布。其次，设计了多智能体深度强化学习引导的自动参数搜索过程。通过感知聚类环境来调整参数搜索方向的过程被建模为马尔可夫决策过程。使用弱监督奖励训练策略网络，每个代理通过与集群交互来自适应地学习最佳集群参数。第三，提出了一种适应数据规模的递归搜索机制，能够对大参数空间进行高效和受控的探索。对九个人工数据集和一个真实世界的数据集进行了广泛的实验。离线和在线任务的结果表明，AR-DBSCAN不仅在归一化互信息（NMI）和调整兰德指数（ARI）指标上的聚类精度分别提高了144.1%和175.3%，而且能够稳健地找到主导参数。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647762",
        "title": "Towards Understanding Generalization and Stability Gaps Between Centralized and Decentralized Federated Learning",
        "link": "https://doi.org/10.1109/tpami.2025.3647762",
        "published": "2025",
        "author": "Yan Sun, Li Shen, Dacheng Tao",
        "summary": "As two mainstream frameworks in federated learning (FL), both centralized and decentralized approaches have shown great application value in practical scenarios. However, existing studies do not provide sufficient evidence and clear guidance for analysis of which performs better in the FL community. Although decentralized methods have been proven to approach the comparable convergence of centralized with less communication, their test performance always falls short of expectations in empirical studies. To comprehensively and fairly compare their efficiency gaps in FL, in this paper, we explore their stability and generalization efficiency. Specifically, we prove that on the general smooth non-convex objectives, 1) centralized FL (CFL) always generalizes better than decentralized FL (DFL); 2) CFL achieves the best performance via adopting partial participation instead of full participation; and, 3) there is a necessary requirement for the topology in DFL to avoid performance collapse as the training scale increases. We also conduct extensive experiments on several common setups in FL to validate that our theoretical analysis is consistent with experimental phenomena and contextually valid in several general and practical scenarios.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "理解集中式和分散式联邦学习之间的泛化性和稳定性差距",
        "abstract_cn": "作为联邦学习（FL）的两种主流框架，集中式和分散式方法在实际场景中都表现出了巨大的应用价值。然而，现有的研究没有提供足够的证据和明确的指导来分析哪些在 FL 社区中表现更好。尽管去中心化方法已被证明可以在较少通信的情况下达到与中心化方法相当的收敛性，但它们的测试性能始终低于实证研究的预期。为了全面、公平地比较它们在 FL 中的效率差距，在本文中，我们探讨了它们的稳定性和泛化效率。具体来说，我们证明在一般平滑非凸目标上，1）中心化 FL（CFL）总是比去中心化 FL（DFL）概括得更好； 2）CFL通过采用部分参与而不是全部参与来获得最佳性能； 3）DFL 中的拓扑结构有必要的要求，以避免随着训练规模的增加而导致性能崩溃。我们还对 FL 中的几种常见设置进行了广泛的实验，以验证我们的理论分析与实验现象一致，并且在几种一般和实际场景中有效。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3647862",
        "title": "Forget Me Not: Fighting Local Overfitting With Knowledge Fusion and Distillation",
        "link": "https://doi.org/10.1109/tpami.2025.3647862",
        "published": "2025",
        "author": "Uri Stern, Eli Corn, Daphna Weinshall",
        "summary": "Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">forgetting rate</i> of deep models on validation data, capturing what we term <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">local overfitting:</i> a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Knowledge Fusion</i> followed by <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Knowledge Distillation</i> – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "勿忘我：通过知识融合和蒸馏对抗局部过度拟合",
        "abstract_cn": "深度神经网络中的过度拟合发生频率低于预期。这是一个令人费解的观察结果，因为理论预测更大的模型容量最终会导致过度拟合——但在实践中很少看到这种情况。但是，如果过度拟合确实发生，不是在全局范围内，而是在数据空间的特定子区域中，该怎么办？在这项工作中，我们引入了一种新颖的分数，用于测量验证数据上深度模型的<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">遗忘率</i>，捕捉我们所说的<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">局部过度拟合：仅限于输入空间的某些区域的性能下降。我们证明，即使没有传统的过度拟合，局部过度拟合也会发生，并且与双下降现象密切相关。基于这些见解，我们引入了一种两阶段方法，利用单个模型的训练历史来恢复和保留遗忘的知识：首先，通过将检查点聚合到一个集合中，然后将其提炼成原始大小的单个模型，从而在不增加推理成本的情况下提高性能。跨多个数据集、现代架构和训练制度的广泛实验验证了我们方法的有效性。值得注意的是，在存在标签噪声的情况下，我们的方法 - <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">知识融合</i> 其次是 <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">知识蒸馏</i> – 优于原始模型和独立训练的集成，实现罕见的双赢场景：降低训练和推理复杂性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648667",
        "title": "DiFaReli++: Diffusion Face Relighting with Consistent Cast Shadows",
        "link": "https://doi.org/10.1109/tpami.2025.3648667",
        "published": "2025",
        "author": "Puntawat Ponglertnapakorn, Nontawat Tritrong, Supasorn Suwajanakorn",
        "summary": "We introduce a novel approach to single-view face relighting in the wild, addressing challenges such as global illumination and cast shadows. A common scheme in recent methods involves intrinsically decomposing an input image into 3D shape, albedo, and lighting, then recomposing it with the target lighting. However, estimating these components is errorprone and requires many training examples with ground-truth lighting to generalize well. Our work bypasses the need for accurate intrinsic estimation and can be trained solely on 2D images without any light stage data, relit pairs, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We propose a novel conditioning technique that simplifies modeling the complex interaction between light and geometry. It uses a rendered shading reference along with a shadow map, inferred using a simple and effective technique, to spatially modulate the DDIM. Moreover, we propose a single-shot relighting framework that requires just one network pass, given pre-processed data, and even outperforms the teacher model across all metrics. Our method realistically relights in-the-wild images with temporally consistent cast shadows under varying lighting conditions. We achieve state-of-the-art performance on the standard benchmark Multi-PIE and rank highest in user studies.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "DiFaReli++：具有一致投射阴影的扩散面重新照明",
        "abstract_cn": "我们引入了一种在野外进行单视图面部重新照明的新颖方法，解决了全局照明和投射阴影等挑战。最近方法中的常见方案涉及将输入图像本质上分解为 3D 形状、反照率和照明，然后使用目标照明对其进行重新组合。然而，估计这些组件很容易出错，并且需要许多具有地面实况照明的训练示例才能很好地概括。我们的工作绕过了精确内在估计的需要，并且可以仅在 2D 图像上进行训练，而无需任何灯光阶段数据、重照明对、多视图图像或照明地面实况。我们的关键思想是利用条件扩散隐式模型 (DDIM) 来解码解缠结的光编码以及与从现成估计器推断出的 3D 形状和面部身份相关的其他编码。我们提出了一种新颖的调节技术，可以简化光与几何形状之间复杂相互作用的建模。它使用渲染的着色参考和阴影贴图（使用简单有效的技术推断）来空间调制 DDIM。此外，我们提出了一种单次重新照明框架，在给定预处理数据的情况下，只需要一次网络传递，甚至在所有指标上都优于教师模型。我们的方法可以在不同的照明条件下真实地重新照亮野外图像，并具有时间一致的投射阴影。我们在标准基准 Multi-PIE 上实现了最先进的性能，并在用户研究中排名最高。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648431",
        "title": "HGNN Shield: Defending Hypergraph Neural Networks Against High-Order Structure Attack",
        "link": "https://doi.org/10.1109/tpami.2025.3648431",
        "published": "2025",
        "author": "Yifan Feng, Yifan Zhang, Shaoyi Du, Shihui Ying, Jun-Hai Yong, Yue Gao",
        "summary": "Hypergraph Neural Networks (HGNNs) are crucial in modeling complex high-order correlations in diverse domains, utilizing hyperedges that connect multiple vertices. However, their susceptibility to structural attacks and irrational connections can disrupt message propagation and degrade performance. To address these issues, we introduce the HGNN Shield, a defense framework incorporating two key modules: Hyperedge-Dependent Estimation (HDE) and High-Order Shield (HOS). The HDE module prioritizes vertex dependencies within hyperedges and adapts traditional connectivity measures to hypergraphs, facilitating precise structural modifications. This adaptation allows for a nuanced assessment of vertex relationships within hyperedges, contributing theoretically by extending classical graph-based connection dependency measures to hypergraphs. Following HDE, the HOS module, positioned before convolutional layers, consists of three submodules: Hyperpath Cut, Hyperpath Link, and Hyperpath Refine. These components collectively detect, disconnect, and refine adversarial connections, ensuring robust message propagation. The theoretical contribution of the HOS module lies in maintaining hyperpath integrity and learning trajectory under adversarial conditions, providing a certifiable defense mechanism against high-order structural attacks. Experiments on six hypergraph datasets indicate that HGNN Shield significantly enhances robustness and maintains data integrity against targeted attacks, outperforming existing methods (an average performance improvement of 9.33% over other methods). Our framework not only improves HGNN reliability but also advances security in hypergraph-based applications.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "HGNN Shield：防御超图神经网络免受高阶结构攻击",
        "abstract_cn": "超图神经网络（HGNN）对于利用连接多个顶点的超边对不同领域中复杂的高阶相关性进行建模至关重要。然而，它们对结构性攻击和不合理连接的敏感性可能会破坏消息传播并降低性能。为了解决这些问题，我们引入了 HGNN Shield，这是一种防御框架，包含两个关键模块：超边缘相关估计 (HDE) 和高阶屏蔽 (HOS)。 HDE 模块优先考虑超边内的顶点依赖性，并使传统的连接措施适应超图，从而促进精确的结构修改。这种适应允许对超边内的顶点关系进行细致入微的评估，通过将经典的基于图的连接依赖度量扩展到超图来在理论上做出贡献。 HDE 之后，HOS 模块位于卷积层之前，由三个子模块组成：Hyperpath Cut、Hyperpath Link 和 Hyperpath Refine。这些组件共同检测、断开和优化对抗性连接，确保稳健的消息传播。 HOS模块的理论贡献在于在对抗条件下维持超路径完整性和学习轨迹，提供针对高阶结构攻击的可认证防御机制。在六个超图数据集上的实验表明，HGNN Shield 显着增强了鲁棒性，并保持了针对针对性攻击的数据完整性，优于现有方法（比其他方法平均性能提高了 9.33%）。我们的框架不仅提高了 HGNN 的可靠性，还提高了基于超图的应用程序的安全性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648453",
        "title": "Learning Deep Tree-Based Retriever for Efficient Recommendation: Theory and Method",
        "link": "https://doi.org/10.1109/tpami.2025.3648453",
        "published": "2025",
        "author": "Ze Liu, Jin Zhang, Defu Lian, Chao Feng, Jie Wang, Enhong Chen",
        "summary": "With the advancement of deep learning, deep recommendation models have achieved remarkable improvements in recommendation accuracy. However, due to the large number of candidate items in practice and the high cost of preference computation, these methods still suffer from low recommendation efficiency. The recently proposed tree-based deep recommendation models alleviate the problem by directly learning tree structure and representations under the guidance of recommendation objectives. To guarantee the effectiveness of beam search for recommendation accuracy, these models strive to ensure that the tree adheres to the max-heap assumption, where a parent node's preference should be the maximum among its children's preferences. However, they employ a one- versus-all strategy, framing the training task as a series of independent binary classification objectives for each node, which limits their ability to fully satisfy the max-heap assumption. To this end, we propose a Deep Tree-based Retriever (DTR for short) for efficient recommendation. DTR frames the training task as a softmax-based multi-class classification over tree nodes at the same level, enabling explicit horizontal competition and more discriminative top-k selection among them, which mimics the beam search behavior during training. To mitigate the suboptimality induced by the labeling of non-leaf nodes, we propose a rectification method for the loss function, which further aligns with the max-heap assumption in expectation. As the number of tree nodes grows exponentially with the levels, we employ sampled softmax to approximate optimization and thereby enhance efficiency. Furthermore, we propose a tree-based sampling method to reduce the bias inherent in sampled softmax. Theoretical results reveal DTR's generalization capability, and both the rectification method and tree-based sampling contribute to improved generalization. The experiments are conducted on four real-world datasets, validating the effectiveness of the proposed method.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "学习基于深度树的检索器以实现高效推荐：理论与方法",
        "abstract_cn": "随着深度学习的进步，深度推荐模型在推荐精度方面取得了显着的提升。然而，由于实践中候选项目数量巨大以及偏好计算成本较高，这些方法仍然存在推荐效率较低的问题。最近提出的基于树的深度推荐模型通过在推荐目标的指导下直接学习树结构和表示来缓解这个问题。为了保证波束搜索在推荐准确性方面的有效性，这些模型努力确保树遵守最大堆假设，其中父节点的偏好应该是其子节点偏好中的最大值。然而，他们采用一对多策略，将训练任务构建为每个节点的一系列独立的二元分类目标，这限制了他们完全满足最大堆假设的能力。为此，我们提出了一种基于深度树的检索器（简称DTR）来进行高效推荐。 DTR 将训练任务构建为同一级别树节点上基于 softmax 的多类分类，从而实现显式水平竞争和其中更具辨别力的 top-k 选择，从而模仿训练期间的波束搜索行为。为了减轻非叶节点标记引起的次优性，我们提出了一种损失函数的校正方法，该方法进一步与期望中的最大堆假设保持一致。由于树节点的数量随着级别呈指数增长，我们使用采样的softmax来近似优化，从而提高效率。此外，我们提出了一种基于树的采样方法来减少采样的 softmax 中固有的偏差。理论结果揭示了DTR的泛化能力，校正方法和基于树的采样都有助于提高泛化能力。实验在四个真实数据集上进行，验证了所提出方法的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648863",
        "title": "SAM-I2V++: Efficiently Upgrading SAM for Promptable Video Segmentation",
        "link": "https://doi.org/10.1109/tpami.2025.3648863",
        "published": "2025",
        "author": "Haiyang Mei, Pengyu Zhang, Mike Zheng Shou",
        "summary": "Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM's static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2's performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "SAM-I2V++：高效升级 SAM 以实现快速视频分割",
        "abstract_cn": "Segment Anything Model (SAM) 等基础模型显着改进了计算机视觉中的快速图像分割。然而，将这些功能扩展到视频带来了巨大的挑战，特别是在确保动态场景中精确且时间一致的掩模传播方面。 SAM 2试图通过从头开始训练海量图像和视频数据的模型来学习复杂的时空关联来解决这个问题，但导致巨大的训练成本阻碍了研究和实际部署。在本文中，我们介绍了 SAM-I2V++，这是一种训练高效的图像到视频升级方法，用于培养可提示的视频分割（PVS）模型。我们的方法战略性地升级预训练的 SAM 以支持 PVS，从而显着降低训练复杂性和资源需求。为了实现这一目标，我们引入了三项关键创新：(i) 基于 SAM 静态图像编码器构建的图像到视频特征提取升级器，以实现时空视频感知；(ii) 记忆选择性关联器，通过相似性驱动选择检索最相关的过去帧，并使用多尺度增强交叉注意力将所选记忆特征与当前帧相关联；(iii) 利用对象记忆的记忆即提示机制，确保动态场景中掩模传播的时间一致。综合实验表明，我们的方法实现了 SAM 2 93% 的性能，而仅使用 0.2% 的训练成本。我们的工作为 PVS 提供了一条资源高效的途径，降低了 PVS 模型设计进一步研究的障碍，并实现了该领域更广泛的应用和进步。项目页面：https://github.com/showlab/SAM-I2V。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3648837",
        "title": "Language Embedded 3D Gaussians for Open-Vocabulary Scene Querying",
        "link": "https://doi.org/10.1109/tpami.2025.3648837",
        "published": "2025",
        "author": "Miao Wang, Jin-Chuan Shi, Shao-Hua Guan, Hao-Bin Duan",
        "summary": "Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于开放词汇场景查询的语言嵌入式 3D 高斯",
        "abstract_cn": "3D 空间中的开放词汇查询具有挑战性，但对于对象定位和分割等场景理解任务至关重要。通过将语言特征融入 3D 空间，语言嵌入场景表示取得了进展。然而，它们的功效很大程度上取决于训练和渲染方面资源密集型的神经网络。尽管最近的 3D Gaussians 提供了高效且高质量的新颖视图合成，但直接在其中嵌入语言特征会导致内存使用量过高并降低性能。在这项工作中，我们介绍了 Language Embedded 3D Gaussians，这是一种用于开放词汇查询任务的新颖场景表示。我们没有在 3D 高斯上嵌入高维原始语义特征，而是提出了一种专用的量化方案，可以大大减轻内存需求，以及一种新颖的嵌入过程，可以实现更平滑但高精度的查询，从而解决多视图特征不一致和基于点的表示中的高频归纳偏差。我们的综合实验表明，我们的表示在当前语言嵌入表示中实现了最佳视觉质量和语言查询准确性，同时在单个桌面 GPU 上保持实时渲染帧速率。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3646464",
        "title": "Data-Driven Bidirectional Spatial-Adaptive Network for Weakly Supervised Object Detection in Remote Sensing Images",
        "link": "https://doi.org/10.1109/tpami.2025.3646464",
        "published": "2025",
        "author": "Zebin Wu, Shangdong Zheng, Yang Xu, Le Wang, Zhihui Wei, Gang Hua",
        "summary": "Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于遥感图像中弱监督目标检测的数据驱动双向空间自适应网络",
        "abstract_cn": "弱监督对象检测（WSOD）仅使用图像级分类注释来学习检测器。在没有精确的实例级标签的情况下，大多数遥感图像（RSI）中的WSOD方法选择得分最高的提案作为最终的检测结果，这面临着两个主要挑战：（1）小规模或罕见姿态的实例很容易被忽略； （2）通过得分最高的区域优化网络不可避免地会忽略许多有价值的候选提案。为了缓解上述挑战，我们提出了一种数据驱动的双向空间自适应网络（BSANet）。它包含一个前向-反向空间丢失（FRSD）模块，以减少极端尺度和姿势以及拥挤场景引起的实例模糊性，并更好地挖掘整个实例。从注意力学习的角度来看，所提出的 FRSD 在概念上类似于数据驱动的硬注意力机制，自适应采样和重建空间相关区域以挖掘更多潜在特征响应。同时，我们的FRSD有效缓解了非参数硬注意力学习方式无法适应不同数据集的固有问题。此外，我们构建了一个软注意力分支来同时对软像素级和硬区域级注意力信息进行建模，以探索软注意力学习和硬注意力学习之间的互补优势。我们在具有挑战性的 NWPU VHR-10.v2 和 DIOR 数据集上评估我们的 BSANet。实验结果表明，我们的方法设定了新的最先进方法。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649078",
        "title": "Bi-C\n                    <sup>2</sup>\n                    R: Bidirectional Continual Compatible Representation for Re-Indexing Free Lifelong Person Re-Identification",
        "link": "https://doi.org/10.1109/tpami.2025.3649078",
        "published": "2025",
        "author": "Zhenyu Cui, Jiahuan Zhou, Yuxin Peng",
        "summary": "Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as \"re-indexing\". However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average strategy is designed to adaptively fill the diverse knowledge gaps between different data domains. Finally, we verify our proposed Bi-C2R method through theoretical analysis and extensive experiments on multiple benchmarks, which demonstrate that the proposed method can achieve leading performance on both the introduced RFL-ReID task and the traditional L-ReID task.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "Bi-C <sup>2</sup> R：用于重新索引免费终身人员重新识别的双向连续兼容表示",
        "abstract_cn": "终身人员重新识别 (L-ReID) 利用顺序收集的数据来持续训练和更新 ReID 模型，重点关注所有数据的整体性能。它的主要挑战是在新数据训练时避免旧知识的灾难性遗忘问题。现有的 L-ReID 方法通常会在每次更新后重新提取所有历史图库图像的新特征以进行推理，称为“重新索引”。然而，由于数据隐私问题和大规模图库图像的高重新索引成本，历史图库数据通常无法直接保存。结果，不可避免地导致更新后的模型提取的查询特征与更新前的模型提取的图库特征之间的检索不兼容，极大地损害了重识别性能。为了解决上述问题，本文重点研究了一项名为“重新索引免费终身人员重新识别（RFL-ReID）”的新任务，该任务需要在不重新索引历史图库图像的情况下执行终身人员重新识别。因此，RFL-ReID比L-ReID更具挑战性，需要在不同的流数据中不断学习和平衡新旧知识，并使新旧模型输出的特征相互兼容。为此，我们提出了双向连续兼容表示（Bi-C2R）框架，不断更新旧模型提取的图库特征，以兼容的方式执行高效的 L-ReID。具体来说，首先设计了一个双向兼容传输网络来桥接新旧知识之间的关系，并在更新后不断将旧图库特征更新到新特征空间。其次，设计了双向兼容蒸馏模块和双向防遗忘蒸馏模型，平衡双特征空间中新旧知识的兼容性。最后，设计了特征级指数移动平均策略来自适应地填补不同数据域之间的多样化知识差距。最后，我们通过理论分析和多个基准上的大量实验验证了我们提出的 Bi-C2R 方法，这表明所提出的方法在引入的 RFL-ReID 任务和传统的 L-ReID 任务上都能取得领先的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649001",
        "title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation",
        "link": "https://doi.org/10.1109/tpami.2025.3649001",
        "published": "2025",
        "author": "Ziyang Gong, Zhixiang Wei, Di Wang, Xiaoxing Hu, Xianzheng Ma, Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Xue Yang, Naoto Yokoya, Jing Zhang, Bo Du, Junchi Yan, Liangpei Zhang",
        "summary": "Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "CrossEarth：域可推广遥感语义分割的地理空间视觉基础模型",
        "abstract_cn": "由于遥感 (RS) 图像中存在巨大的域差距，其特点是位置、波长和传感器类型等可变性，遥感域泛化 (RSDG) 已成为一个关键且有价值的研究前沿，专注于开发能够跨不同场景有效泛化的模型。然而，该领域的研究仍未得到充分探索：（1）当前的跨领域方法主要集中在领域适应（DA），它将模型适应预定义的领域而不是看不见的领域； (2)很少有研究针对RSDG问题，尤其是语义分割任务。现有的相关模型是针对特定的未知领域开发的，在其他未见过的场景下存在拟合不足的问题； (3) 现有的RS基础模型倾向于优先考虑域内性能而不是跨域泛化。为此，我们引入了第一个 RSDG 语义分割视觉基础模型 CrossEarth。 CrossEarth 通过专门设计的数据级地球式注入管道和模型级多任务训练管道展示了强大的跨域泛化能力。此外，针对语义分割任务，我们还制定了RSDG基准，包含跨不同地区、频段、平台和气候的32个语义分割场景，为未来RSDG模型的通用性提供综合评估。对该集合的大量实验证明了 CrossEarth 相对于现有最先进方法的优越性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649050",
        "title": "Spike Camera Optical Flow Estimation Based on Continuous Spike Streams",
        "link": "https://doi.org/10.1109/tpami.2025.3649050",
        "published": "2025",
        "author": "Rui Zhao, Ruiqin Xiong, Dongkai Wang, Shiyu Xuan, Jian Zhang, Xiaopeng Fan, Tiejun Huang",
        "summary": "Spike camera is an emerging bio-inspired vision sensor with ultra-high temporal resolution. It records scenes by accumulating photons and outputting binary spike streams. Optical flow estimation aims to estimate pixel-level correspondences between different moments, describing motion information along time, which is a key task of spike camera. High-quality optical flow is important since motion information is a foundation for analyzing spikes. However, extracting stable light-intensity information from spikes is difficult due to the randomness of binary spikes. Besides, the continuity of spikes can offer contextual information for optical flow. In this paper, we propose a network Spike2Flow++ to estimate optical flow for spike camera. In Spike2Flow++, we propose a differential of spike firing time (DSFT) to represent information in binary spikes. Moreover, we propose a dual DSFT representation and a dual correlation construction to extract stable light-intensity information for reliable correlations. To use the continuity of spikes as motion contextual information, we propose a joint correlation decoding (JCD) that jointly estimates a series of flow fields. To adaptively fuse different motions in JCD, we propose a global motion bank aggregation to construct an information bank for all motions and adaptively extract contexts from the bank for each iteration during recurrent decoding of each motion. To train and evaluate our network, we construct a real scene with spikes and flow++ (RSSF++) based on real-world scenes. Experiments demonstrate that our Spike2Flow++ achieves state-of-the-art performance on RSSF++, photo-realistic high-speed motion (PHM), and real-captured data.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "基于连续尖峰流的尖峰相机光流估计",
        "abstract_cn": "Spike 相机是一种新兴的仿生视觉传感器，具有超高时间分辨率。它通过积累光子并输出二进制尖峰流来记录场景。光流估计旨在估计不同时刻之间的像素级对应关系，描述随时间的运动信息，这是尖峰相机的关键任务。高质量光流非常重要，因为运动信息是分析尖峰的基础。然而，由于二进制尖峰的随机性，从尖峰中提取稳定的光强度信息很困难。此外，尖峰的连续性可以为光流提供上下文信息。在本文中，我们提出了一种网络 Spike2Flow++ 来估计尖峰相机的光流。在 Spike2Flow++ 中，我们提出了尖峰触发时间微分 (DSFT) 来表示二进制尖峰中的信息。此外，我们提出了双 DSFT 表示和双相关结构来提取稳定的光强度信息以获得可靠的相关性。为了使用尖峰的连续性作为运动上下文信息，我们提出了一种联合相关解码（JCD）来联合估计一系列流场。为了自适应地融合 JCD 中的不同运动，我们提出了一种全局运动库聚合，为所有运动构建一个信息库，并在每个运动的循环解码期间从该信息库中自适应地为每次迭代提取上下文。为了训练和评估我们的网络，我们根据真实场景构建了一个具有尖峰和流++ (RSSF++) 的真实场景。实验表明，我们的 Spike2Flow++ 在 RSSF++、逼真的高速运动 (PHM) 和真实捕获的数据上实现了最先进的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649111",
        "title": "Continuous Review and Timely Correction: Enhancing the Resistance to Noisy Labels Via Self-Not-True and Class-Wise Distillation",
        "link": "https://doi.org/10.1109/tpami.2025.3649111",
        "published": "2025",
        "author": "Long Lan, Jingyi Wang, Xinghao Wu, Bo Han, Xinwang Liu",
        "summary": "Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn't entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network's guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD's robustness in tackling complex scenarios characterized by label noise.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "持续审查和及时纠正：通过自我不真实和类明智的蒸馏增强对噪音标签的抵抗力",
        "abstract_cn": "深度神经网络拥有卓越的学习能力和表达能力，但这使得它们很容易出现过度拟合，特别是当它们遇到错误标记的数据时。当网络首先学习正确标记的数据并随后记住错误标记的实例时，就会出现一种称为记忆效应的显着现象。虽然提前停止可以减轻过度拟合，但它并不能完全防止网络在初始训练阶段适应不正确的标签，这可能会导致从准确数据中丢失有价值的见解。此外，早期停止无法纠正因输入标签错误而造成的错误，这凸显了改进策略的必要性。在本文中，我们引入了一种不断回顾和及时纠正所学知识的创新机制。我们的方法允许网络反复重新访问和强化正确的信息，同时及时解决因错误标记的数据而产生的任何不准确问题。我们提出了一种称为自非真实蒸馏（SNTD）的新方法。该技术采用自蒸馏，其中来自先前训练迭代的网络充当老师，指导当前网络审查并巩固其对准确标签的理解。至关重要的是，SNTD 在此过程中掩盖了逻辑中的真实类别标签，专注于非真实类别以纠正可能获得的任何错误知识。我们还认识到不同的数据类别遵循不同的学习轨迹。单个教师网络可能很难同时有效地指导所有班级的学习，这需要为每个特定班级选择不同的教师网络。此外，教师网络指导的影响在整个培训过程中也有所不同。为了应对这些挑战，我们提出了 SNTD+，它集成了按类蒸馏策略和动态权重调整机制。总之，这些增强功能显着增强了 SNTD 在处理以标签噪声为特征的复杂场景时的稳健性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649294",
        "title": "On the Transferability and Discriminability of Representation Learning in Unsupervised Domain Adaptation",
        "link": "https://doi.org/10.1109/tpami.2025.3649294",
        "published": "2026",
        "author": "Wenwen Qiang, Ziyin Gu, Lingyu Si, Jiangmeng Li, Changwen Zheng, Fuchun Sun, Hui Xiong",
        "summary": "In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "论无监督领域适应中表征学习的可迁移性和可辨别性",
        "abstract_cn": "在本文中，我们解决了无监督域适应（UDA）中仅依赖分布对齐和源域经验风险最小化的局限性。我们的信息论分析表明，这种标准的基于对抗性的框架忽略了目标域特征的可辨别性，导致性能不佳。为了弥合这一理论与实践之间的差距，我们将“良好的表示学习”定义为保证可迁移性和可辨别性，并证明了针对目标域可辨别性的额外损失项是必要的。基于这些见解，我们提出了一种新颖的基于对抗性的 UDA 框架，该框架明确地将领域对齐目标与可辨别性增强约束相结合。我们的方法被实例化为具有全局和局部一致性的域不变表示学习（RLGLC），利用 Wasserstein 距离的非对称松弛 Wasserstein（AR-WWD）来解决类不平衡和语义维度加权问题，并采用局部一致性机制来保留细粒度的目标域判别信息。跨多个基准数据集的大量实验表明，RLGLC 始终超越最先进的方法，证实了我们理论观点的价值，并强调了在基于对抗性的 UDA 中强制执行可转移性和可辨别性的必要性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649177",
        "title": "A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots",
        "link": "https://doi.org/10.1109/tpami.2025.3649177",
        "published": "2025",
        "author": "Mingqi Yuan, Tao Yu, Wenqi Ge, Xiuyong Yao, Dapeng Li, Huijiang Wang, Jiayu Chen, Bo Li, Wei Zhang, Wenjun Zeng, Hua Chen, Xin Jin",
        "summary": "Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and regularly updated collection of BFM papers and projects to facilitate further research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "行为基础模型综述：下一代仿人机器人全身控制系统",
        "abstract_cn": "人形机器人作为复杂运动控制、人机交互和通用物理智能的多功能平台而受到广泛关注。然而，由于复杂的动力学、欠驱动和多样化的任务要求，在人形机器人中实现高效的全身控制（WBC）仍然是一个基本挑战。虽然基于学习的控制器已显示出完成复杂任务的希望，但它们对新场景的劳动密集型且成本高昂的再培训的依赖限制了现实世界的适用性。为了解决这些限制，行为（al）基础模型（BFM）已经成为一种新范式，它利用大规模预训练来学习可重用的原始技能和广泛的行为先验，从而实现零射击或快速适应各种下游任务。在本文中，我们全面概述了人形 WBC 的 BFM，追踪了它们在不同预训练流程中的发展。此外，我们还讨论了现实世界的应用、当前的局限性、紧迫的挑战和未来的机遇，将 BFM 定位为实现可扩展和通用类人智能的关键方法。最后，我们提供了精选且定期更新的 BFM 论文和项目集，以促进进一步的研究，可在 https://github.com/yuanmingqi/awesome-bfm-papers 上获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3649521",
        "title": "Fast Multi-View Discrete Clustering Via Spectral Embedding Fusion",
        "link": "https://doi.org/10.1109/tpami.2025.3649521",
        "published": "2026",
        "author": "Ben Yang, Xuetao Zhang, Zhiyuan Xue, Feiping Nie, Badong Chen",
        "summary": "Multi-view spectral clustering (MVSC) has garnered growing interest across various real-world applications, owing to its flexibility in managing diverse data space structures. Nevertheless, the fusion of multiple $n\\times n$ similarity matrices and the separate post- discretization process hinder the utilization of MVSC in large-scale tasks, where $n$ denotes the number of samples. Moreover, noise in different similarity matrices, along with the two-stage mismatch caused by the post- discretization, results in a reduction in clustering effectiveness. To overcome these challenges, we establish a novel fast multi-view discrete clustering (FMVDC) model via spectral embedding fusion, which integrates spectral embedding matrices ($n\\times c$, $c\\ll n$) to directly obtain discrete sample categories, where $c$ indicates the number of clusters, bypassing the need for both similarity matrix fusion and post- discretization. To further enhance clustering efficiency, we employ an anchor-based spectral embedding strategy to decrease the computational complexity of spectral analysis from cubic to linear. Since gradient descent methods are incapable of discrete models, we propose a fast optimization strategy based on the coordinate descent method to solve the FMVDC model efficiently. Extensive studies demonstrate that FMVDC significantly improves clustering performance compared to existing state-of-the-art methods, particularly in large-scale clustering tasks.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过光谱嵌入融合进行快速多视图离散聚类",
        "abstract_cn": "多视图谱聚类（MVSC）由于其在管理不同数据空间结构方面的灵活性，在各种实际应用中引起了越来越多的兴趣。然而，多个$n\\times n$相似度矩阵的融合和单独的后离散化过程阻碍了MVSC在大规模任务中的利用，其中$n$表示样本数量。此外，不同相似度矩阵中的噪声，以及后离散化引起的两阶段不匹配，导致聚类效果降低。为了克服这些挑战，我们通过谱嵌入融合建立了一种新颖的快速多视图离散聚类（FMVDC）模型，该模型集成谱嵌入矩阵（$n\\times c$，$c\\ll n$）来直接获得离散样本类别，其中$c$表示聚类的数量，绕过了相似性矩阵融合和后离散化的需要。为了进一步提高聚类效率，我们采用基于锚的光谱嵌入策略来降低光谱分析的计算复杂度，从三次到线性。由于梯度下降法无法求解离散模型，因此我们提出了一种基于坐标下降法的快速优化策略来有效求解 FMVDC 模型。大量研究表明，与现有最先进的方法相比，FMVDC 显着提高了聚类性能，特别是在大规模聚类任务中。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650165",
        "title": "GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation",
        "link": "https://doi.org/10.1109/tpami.2025.3650165",
        "published": "2026",
        "author": "Zihui Zhang, Weisheng Dai, Bing Wang, Bo Li, Bo Yang",
        "summary": "We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "GrowSP++：用于无监督 3D 语义分割的增长超点和基元",
        "abstract_cn": "我们研究原始点云的 3D 语义分割问题。与主要依赖大量人类注释来训练神经网络的现有方法不同，我们提出了 GrowSP++，这是一种无监督方法，可以成功识别 3D 场景中每个点的复杂语义类，而不需要任何类型的人类标签。我们的方法由三个主要部分组成：1）包含 2D-3D 特征蒸馏的特征提取器，2）具有逐渐增长的超点的超点构造函数，3）具有附加增长策略的语义原始构造函数。我们方法的关键是超点构造函数以及超点和语义基元的渐进增长策略，驱动特征提取器逐步学习属于同一语义类的 3D 点的相似特征。我们在五个具有挑战性的室内和室外数据集上广泛评估了我们的方法，在所有无监督基线上展示了最先进的性能。我们希望我们的工作能够激发更先进的无监督 3D 语义学习方法。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650712",
        "title": "Low-Rank Tensor Learning by Generalized Nonconvex Regularization",
        "link": "https://doi.org/10.1109/tpami.2025.3650712",
        "published": "2026",
        "author": "Sijia Xia, Michael K. Ng, Xiongjun Zhang",
        "summary": "In this paper, we study the problem of low-rank tensor learning, where only a few of training samples are observed and the underlying tensor has a low-rank structure. The existing methods are based on the sum of nuclear norms of unfolding matrices of a tensor, which may be suboptimal. In order to explore the low-rankness of the underlying tensor effectively, we propose a nonconvex model based on transformed tensor nuclear norm for low-rank tensor learning. Specifically, a family of nonconvex functions are employed onto the singular values of all frontal slices of a tensor in the transformed domain to characterize the low-rankness of the underlying tensor. An error bound between the stationary point of the nonconvex model and the underlying tensor is established under restricted strong convexity on the loss function (such as least squares loss and logistic regression) and suitable regularity conditions on the nonconvex penalty function. By reformulating the nonconvex function into the difference of two convex functions, a proximal majorization-minimization (PMM) algorithm is designed to solve the resulting model. Then the global convergence and convergence rate of PMM are established under very mild conditions. Numerical experiments are conducted on tensor completion and binary classification to demonstrate the effectiveness of the proposed method over other state-of-the-art methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过广义非凸正则化进行低秩张量学习",
        "abstract_cn": "在本文中，我们研究了低秩张量学习问题，其中仅观察到少量训练样本，并且底层张量具有低秩结构。现有方法基于张量展开矩阵的核范数之和，这可能不是最优的。为了有效地探索底层张量的低秩性，我们提出了一种基于变换张量核范数的非凸模型，用于低秩张量学习。具体来说，将一系列非凸函数应用于变换域中张量的所有正面切片的奇异值，以表征基础张量的低秩性。在损失函数（例如最小二乘损失和逻辑回归）上的受限强凸性和非凸罚函数上的适当正则条件下，建立非凸模型的驻点与底层张量之间的误差界限。通过将非凸函数重新表示为两个凸函数的差，设计了近端极大化最小化（PMM）算法来求解所得模型。然后在非常温和的条件下建立PMM的全局收敛性和收敛速率。在张量完成和二元分类上进行了数值实验，以证明所提出的方法相对于其他最先进方法的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650545",
        "title": "MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs",
        "link": "https://doi.org/10.1109/tpami.2025.3650545",
        "published": "2026",
        "author": "Jianjian Cao, Chong Yu, Peng Ye, Tao Chen",
        "summary": "Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://github.com/double125/MADTP-plus.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "MADTP++：弥合令牌和权重修剪之间的差距，以加速 VLT",
        "abstract_cn": "视觉语言转换器（VLT）取得了巨大的成功，但由于大量的输入标记和广泛的模型参数，其计算成本构成了挑战。现有的VLT压缩方法主要依赖于基于单模态的令牌剪枝或粗粒度权重剪枝技术。然而，这些方法面临着重大障碍，例如忽略不同模态的关键对齐，缺乏动态压缩每一层进行令牌剪枝的灵活性，由于粗粒度权重剪枝而表现出不可避免的性能下降，以及难以同时压缩输入令牌和模型参数。为了解决这些限制，我们提出了 MADTP++，这是一种将定制令牌和权重修剪过程集成到统一框架中的新颖方法，在参数计数和计算成本方面实现了卓越的压缩。具体来说，对于标记修剪过程，我们引入了多模态对齐指导（MAG）模块和动态标记修剪（DTP）模块来对齐不同模态的语义特征，并根据不同的输入实例指导动态消除冗余标记。对于权重修剪过程，我们提出了一个硬件感知权重修剪（HWP）模块，该模块利用跨不同硬件设置的稀疏张量核心来实现 VLT 内的细粒度参数修剪。为了进一步统一令牌和权重剪枝，我们还提出了一种协作优化训练策略，该策略在剪枝之前自动将所需的 GFLOP 和参数减少分配给每个分支，并采用知识蒸馏约束来促进两个剪枝维度的联合优化。对各种 VLT 模型和数据集进行的大量实验表明，MADTP++ 可以显着减少模型参数和计算成本，同时保持具有竞争力的性能。我们已在 https://github.com/double125/MADTP-plus 上提供了代码。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3650761",
        "title": "Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models",
        "link": "https://doi.org/10.1109/tpami.2026.3650761",
        "published": "2026",
        "author": "YiFan Zhang, Qingsong Wen, Chaoyou Fu, Kun Wang, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin",
        "summary": "Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a 'less is more' pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "超越 LLaVA-HD：深入研究高分辨率多模态大语言模型",
        "abstract_cn": "高分辨率清晰地看到是多模态大语言模型 (MLLM) 的基础，它已被证明对于视觉感知和推理至关重要。现有的作品通常采用直接的分辨率升级方法，其中图像由全局和局部分支组成，后者是切片图像块，但大小调整为与前者相同的分辨率。这意味着更高的分辨率需要更多的局部补丁，从而导致高昂的计算费用，同时，局部图像标记的主导地位可能会削弱全局上下文。在本文中，我们深入研究了这些问题并提出了一个新的框架以及详细的优化策略。具体来说，我们根据对不同适配器擅长不同任务的观察，使用混合适配器从全局视图中提取上下文信息。对于局部补丁，引入可学习的查询嵌入来减少图像标记，与用户问题最相关的重要标记将由基于相似性的选择器进一步选择。我们的实证结果证明了“少即是多”的模式，利用更少但信息更丰富的本地图像标记可以提高性能。此外，一个重大挑战在于训练策略，因为全局挖掘块和局部压缩块的同时端到端训练并不能产生最佳结果。因此，我们提倡交替培训方式，确保全球和本地方面的平衡学习。最后，我们还引入了一个对图像细节要求较高的具有挑战性的数据集，增强了局部压缩层的训练。所提出的方法被称为具有复杂任务、本地图像压缩和全局专家混合 (SliME) 的 MLLM，仅用 200 万个训练数据就在各种基准测试中实现了领先的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650695",
        "title": "Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm",
        "link": "https://doi.org/10.1109/tpami.2025.3650695",
        "published": "2026",
        "author": "Changshun Wu, Weicheng He, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem",
        "summary": "Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "重新审视实时对象检测中的分布外检测：从基准陷阱到新的缓解范例",
        "abstract_cn": "分布外 (OoD) 输入对深度学习模型提出了持续的挑战，通常会引发对非目标对象的过度自信的预测。虽然之前的工作主要集中在完善评分函数和调整测试时间阈值，但此类算法改进仅提供增量收益。我们认为，需要重新思考整个开发生命周期，以有效减轻这些风险。这项工作解决了对象检测中 OoD 检测的两个被忽视的维度。首先，我们揭示了广泛使用的评估基准中的根本缺陷：与其设计意图相反，OoD 测试集中多达 13% 的对象实际上属于分布内类，反之亦然。这些质量问题严重扭曲了现有方法报告的性能，并导致其高误报率。其次，我们引入了一种新颖的训练时缓解范例，该范例独立于外部 OoD 检测器运行。我们不是仅仅依赖事后评分，而是使用精心合成的 OoD 数据集来微调检测器，该数据集在语义上类似于分布中的对象。该过程通过抑制 OoD 对象的客观性来形成防御决策边界，从而使 BDD-100K 上的 YOLO 模型的幻觉误差减少 91%。我们的方法概括了 YOLO、Faster R-CNN 和 RT-DETR 等检测范例，并支持小样本适应。总之，这些贡献提供了一种原则性且有效的方法来减少物体探测器中 OoD 引起的幻觉。代码和数据可在以下网址获取：https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3650770",
        "title": "Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination",
        "link": "https://doi.org/10.1109/tpami.2026.3650770",
        "published": "2026",
        "author": "Sida Peng, Jiarui Guo, Xi Chen, Yuan Liu, Dongchen Yang, Hujun Bao, Xiaowei Zhou",
        "summary": "This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "学习未知光照下逆渲染的扩散先验",
        "abstract_cn": "本文旨在从未知静态照明条件下拍摄的姿势图像中恢复物体材料。最近的方法通过基于可微分物理的渲染来优化材料参数来解决此任务。然而，由于物体几何形状、材质和环境光照之间的耦合，逆渲染过程中存在固有的模糊性，导致先前的方法无法获得准确的结果。为了克服这个不适定问题，我们的关键思想是使用生成模型先学习材料，以规范优化过程。我们观察到，一般渲染方程可以分为漫反射和镜面反射着色项，从而将材质先验公式化为反照率和镜面反射的扩散模型。由于这种设计，我们的模型可以使用现有的丰富的 3D 对象数据进行训练，并且自然地充当从 RGB 图像恢复材质表示时解决歧义的多功能工具。此外，我们开发了一种从粗到精的训练策略，利用估计的材料来指导扩散模型以满足多视图一致约束，从而获得更稳定和准确的结果。对现实世界和合成数据集的广泛实验表明，我们的方法在材料回收方面实现了最先进的性能。代码可在 https://zju3dv.github.io/IntrinsicAnything/ 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650500",
        "title": "Efficient Exploration for Multi-Agent Diversity With Agent Identity",
        "link": "https://doi.org/10.1109/tpami.2025.3650500",
        "published": "2026",
        "author": "Tianxu Li, Kun Zhu",
        "summary": "Multi-Agent Reinforcement Learning (MARL) has proven to be effective in learning cooperative policies, where agents learn decentralized policies, sharing the same network parameters, through centralized training. However, this parameter sharing can lead to similar behaviors among agents, hindering effective exploration. Existing multi-agent diversity methods that rely on the variational inference methods to differentiate agents may suffer from significant overfitting, which in turn hinders the exploration of new trajectories. To encourage multi-agent diversity and efficient exploration, we propose Active Exploration with Agent-Identity (AEAI), a novel exploration method, which maximizes the entropy over trajectories of different agents to promote sufficient exploration. Moreover, we derive a novel lower bound for the mutual information objective based on the successor features to align the directions of trajectories and agent identities in order to learn agent identity-conditioned policies. We combine these two items and integrate our method with existing MARL methods. We evaluate our proposed AEAI on challenging multi-agent tasks across various MARL benchmarks. Experimental results show that our method consistently outperforms existing state-of-the-art methods, highlighting its effectiveness in fostering diversity and improving exploration.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "具有智能体身份的多智能体多样性的有效探索",
        "abstract_cn": "多智能体强化学习（MARL）已被证明在学习合作策略方面是有效的，其中智能体通过集中训练学习去中心化策略，共享相同的网络参数。然而，这种参数共享可能会导致代理之间出现相似的行为，从而阻碍有效的探索。现有的依赖变分推理方法来区分智能体的多智能体多样性方法可能会出现严重的过度拟合，从而阻碍了新轨迹的探索。为了鼓励多智能体多样性和高效探索，我们提出了智能体身份主动探索（AEAI），这是一种新颖的探索方法，它最大化不同智能体轨迹的熵以促进充分的探索。此外，我们基于后继特征推导了互信息目标的新下界，以调整轨迹和代理身份的方向，以便学习代理身份条件策略。我们将这两项结合起来，并将我们的方法与现有的 MARL 方法集成。我们评估了我们提出的 AEAI 在各种 MARL 基准上具有挑战性的多智能体任务。实验结果表明，我们的方法始终优于现有的最先进方法，凸显了其在促进多样性和改进探索方面的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650478",
        "title": "Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction",
        "link": "https://doi.org/10.1109/tpami.2025.3650478",
        "published": "2026",
        "author": "Bohan Li, Jiajun Deng, Yasheng Sun, Xiaofeng Wang, Xin Jin, Wenjun Zeng",
        "summary": "Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Hi</i>erarchical context alignment paradigm for a more accurate <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">SOP</i> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(Hi-SOP)</i>. Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI & NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于语义占用预测的分层上下文对齐与解缠几何和时间建模",
        "abstract_cn": "基于相机的 3D 语义占用预测 (SOP) 对于从有限的 2D 图像观察中理解复杂的 3D 场景至关重要。现有的 SOP 方法通常会聚合上下文特征来协助占用表示学习，从而缓解遮挡或模糊等问题。然而，这些解决方案经常面临错位问题，其中不同帧中同一位置的对应特征在聚合过程中可能具有不同的语义，这导致上下文融合结果不可靠和表示学习过程不稳定。为了解决这个问题，我们引入了一种新的<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">分层上下文对齐范例，以实现更准确的<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">SOP</i> <斜体 xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(Hi-SOP)</i>。 Hi-SOP 首先解开几何和时间上下文以进行单独对齐，然后组合这两个分支以增强 SOP 的可靠性。将视觉输入解析为局部-全局对齐层次结构包括：（I）解开的几何和时间单独对齐，在每个对齐中分别利用深度置信度和相机姿势作为相关特征匹配的先验； (II)基于语义一致性的变换后的几何体和时间体的全局对齐和组合。我们的方法在 SemanticKITTI 和 NuScenes-Occupancy 数据集上的语义场景完成以及 NuScenes 数据集上的 LiDAR 语义分割方面优于 SOTA。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650546",
        "title": "Lifelong Learning of Large Language Model based Agents: A Roadmap",
        "link": "https://doi.org/10.1109/tpami.2025.3650546",
        "published": "2026",
        "author": "Junhao Zheng, Chengming Shi, Xidi Cai, Qiuke Li, Duzhen Zhang, Chenxing Li, Dong Yu, Qianli Ma",
        "summary": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/qianlimalab/</uri> awesome-lifelong-llm-agent.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "基于大型语言模型的智能体的终身学习：路线图",
        "abstract_cn": "终身学习，也称为持续学习或增量学习，是通过使系统能够不断适应动态环境来推进通用人工智能 (AGI) 的重要组成部分。虽然大型语言模型 (LLM) 在自然语言处理方面表现出了令人印象深刻的能力，但现有的 LLM 代理通常是为静态系统设计的，缺乏随着时间的推移而适应新挑战的能力。这项调查首次系统地总结了将终身学习纳入基于法学硕士的代理人的潜在技术。我们将这些代理的核心组件分为三个模块：用于多模式输入集成的感知模块、用于存储和检索不断发展的知识的记忆模块以及用于与动态环境进行基础交互的动作模块。我们强调这些支柱如何共同实现持续适应、减轻灾难性遗忘并提高长期绩效。这项调查为致​​力于开发法学硕士代理人终身学习能力的研究人员和从业者提供了路线图，提供了对新兴趋势、评估指标和应用场景的见解。相关文献和资源可在 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/qianlimalab/</uri> Awesome-lifelong-llm-agent 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3650796",
        "title": "IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History",
        "link": "https://doi.org/10.1109/tpami.2026.3650796",
        "published": "2026",
        "author": "Yi Xu, Weiran Shen, Jun Xu, Xiao Zhang, Ji-Rong Wen",
        "summary": "Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">novice expert</i> to <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">experienced expert</i>. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">inverse batched contextual bandit</i> (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming problem by utilizing the behavioral evolution history of the batched contextual bandit with inaccessible rewards, and it can be extended to fairness-aware expert limitation. We demonstrate that IBCB is a unified framework for both deterministic and randomized bandit policies. The experimental results indicate that IBCB outperforms several existing imitation learning algorithms on synthetic and real-world data and significantly reduces running time. Additionally, empirical analyses reveal that IBCB exhibits better imitation ability for fairness-aware experts, out-of-distribution generalization and is highly effective in learning the bandit policy from the interaction history of novice experts. The code is publicly available.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "IBCB：行为进化史的高效逆批上下文强盗",
        "abstract_cn": "传统的模仿学习侧重于对专家的行为机制进行建模，这需要某个固定专家产生的大量交互历史。然而，在许多流式应用程序中，例如流式推荐系统，在线决策者通常在决策过程中进行在线学习，这意味着在线决策者生成的交互历史包括他们从<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">新手专家</i>到<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">新手专家</i>的行为演变xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">经验丰富的专家</i>。这对只能利用经验丰富的专家的数据的现有模仿学习方法提出了新的挑战。为了解决这个问题，本文提出了一种<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">逆批处理上下文强盗</i> (IBCB)框架，该框架可以有效地估计环境奖励参数并根据专家的行为进化历史学习策略。具体来说，IBCB利用具有不可获得奖励的批量上下文强盗的行为进化历史，将反问题转化为简单的二次规划问题，并且可以扩展到公平意识专家限制。我们证明 IBCB 是确定性和随机老虎机策略的统一框架。实验结果表明，IBCB 在合成和真实数据上优于几种现有的模仿学习算法，并显着减少了运行时间。此外，实证分析表明，IBCB 对具有公平意识的专家表现出更好的模仿能力，具有分布外泛化能力，并且在从新手专家的交互历史中学习强盗策略方面非常有效。该代码是公开的。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3650769",
        "title": "LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting",
        "link": "https://doi.org/10.1109/tpami.2026.3650769",
        "published": "2026",
        "author": "Yuchen Su, Zhineng Chen, Yongkun Du, Zuxuan Wu, Hongtao Xie, Yu-Gang Jiang",
        "summary": "End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _{1}$</tex-math></inline-formula>-norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/ychensu/LRANet-PP</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "LRANet++：用于准确高效文本识别的低秩近似网络",
        "abstract_cn": "端到端文本识别旨在在统一框架内联合优化文本检测和识别。尽管取得了重大进展，但为任意形状的文本设计准确且高效的端到端文本识别器仍然具有挑战性。我们认为主要瓶颈是缺乏可靠且高效的文本检测方法。为了解决这个问题，我们提出了一种新颖的参数化文本形状表示，其基于用于精确检测的低秩近似和用于快速推理的三重分配检测头。具体来说，与当前与数据无关的形状表示方法不同，我们利用标记文本边界之间的形状相关性来构建鲁棒的低秩子空间。通过最小化 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\ell _{1}$</tex-math></inline-formula>-norm 目标，我们提取正交向量，从噪声中捕获内在文本形状注释，通过仅几个基向量的线性组合实现精确重建。接下来，三重分配方案将训练复杂性与推理速度解耦。它利用深度稀疏分支来指导超轻量级推理分支，而密集分支则提供丰富的并行监督。基于这些进步，我们将增强型检测模块与轻量级识别分支集成，形成端到端文本识别框架，称为 LRANet++，能够准确有效地识别任意形状的文本。在具有挑战性的基准上进行的大量实验证明了 LRANet++ 与最先进的方法相比的优越性。代码位于：<uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/ychensu/LRANet-PP</uri>。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3650590",
        "title": "Next Bit Prediction: A Unified Lossless and Lossy Point Cloud Geometry Compression Framework",
        "link": "https://doi.org/10.1109/tpami.2025.3650590",
        "published": "2026",
        "author": "Bojun Liu, Yangzhi Ma, Li Li, Dong Liu, Zhu Li, Houqiang Li",
        "summary": "We propose Next Bit Prediction (NBP), a unified framework that simultaneously addresses lossless compression and lossy reconstruction of 3D point cloud geometry through a next-bit probability estimation paradigm. Our key insight is that both lossless compression and lossy reconstruction fundamentally rely on accurate probability estimation of geometric symbols, though targeting different metrics. Lossless compression minimizes bitrate via precise symbol distribution prediction, while lossy reconstruction enhances reconstruction fidelity through probability-guided geometry refinement. Recognizing that point clouds become sparser with increasing bit depth, NBP introduces two key technical innovations. For more significant bits, where the point density is higher, we develop a multi-stage Occupancy Probability Estimation (OPE) mechanism to estimate the probability distribution of occupancy status across multiple iteration stages, with each stage supporting either lossless or lossy mode. For less significant bits that focus on point placement, a Disentangled Probability Estimation (DPE) module is proposed to handle density information and binary residuals, simultaneously enabling lossless compression and facilitating probability-driven coordinate refinement for high-quality lossy reconstruction. Extensive experiments demonstrate the advantages of NBP, including low complexity, progressive coding, and superior coding efficiency, achieving state-of-the-art results both quantitatively and qualitatively.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "下一位预测：统一的无损和有损点云几何压缩框架",
        "abstract_cn": "我们提出了下一位预测（NBP），这是一个统一的框架，通过下一位概率估计范例同时解决 3D 点云几何的无损压缩和有损重建。我们的主要见解是，无损压缩和有损重建从根本上都依赖于几何符号的准确概率估计，尽管针对不同的指标。无损压缩通过精确的符号分布预测来最小化比特率，而有损重建则通过概率引导的几何细化来增强重建保真度。 NBP 认识到点云随着位深度的增加而变得越来越稀疏，因此引入了两项关键的技术创新。对于点密度较高的更重要的位，我们开发了一种多阶段占用概率估计（OPE）机制来估计跨多个迭代阶段的占用状态的概率分布，每个阶段支持无损或有损模式。对于关注点放置的不太重要的位，提出了解缠结概率估计（DPE）模块来处理密度信息和二进制残差，同时实现无损压缩并促进概率驱动的坐标细化以实现高质量有损重建。大量的实验证明了NBP的优势，包括低复杂度、渐进式编码和卓越的编码效率，在定量和定性上都取得了最先进的结果。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652616",
        "title": "A Hierarchical Prior Mining Approach for Non-local Multi-view Stereo",
        "link": "https://doi.org/10.1109/tpami.2026.3652616",
        "published": "2026",
        "author": "Jiaqi Yang, Yanan He, Chunlin Ren, Qingshan Xu, Siwen Quan, Xiyu Zhang, Yanning Zhang",
        "summary": "As a fundamental problem in computer vision, multi-view stereo (MVS) aims at recovering the 3D geometry of the target from a set of 2D images. However, the reconstructed quality is significantly impacted by the presence of low-textured areas. In this paper, we propose a Hierarchical Prior Mining (HPM) framework for non-local multi-view stereo. Different from most existing works dedicated to focusing on local information and only using a single prior, HPM captures non-local structural cues and leverages multi-source priors for geometry recovery. Based on the framework, we first propose HPM-MVS, which obtains precise initial hypotheses through non-local operations, simultaneously constructing a better planar prior model in an HPM framework to further facilitate hypothesis generation. In addition, we futher propose HPM-MVS++, which excavates the structured region information of images and spatial geometric relationships of hypotheses as prior knowledge. Then, it incorporates them into probabilistic graphical models, ultimately deducing two novel multi-view matching costs. This significantly enhances the robustness to challenging situations and improves the completeness of the reconstruction. Experimental results on the ETH3D and Tanks & Temples have verified the superior performance and strong generalization capability of our approach. The code is available at https://github.com/CLinvx.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "非局部多视立体分层先验挖掘方法",
        "abstract_cn": "作为计算机视觉中的一个基本问题，多视图立体（MVS）旨在从一组 2D 图像中恢复目标的 3D 几何形状。然而，重建质量会受到低纹理区域的显着影响。在本文中，我们提出了一种用于非局部多视图立体的分层先验挖掘（HPM）框架。与大多数专注于局部信息且仅使用单个先验的现有工作不同，HPM 捕获非局部结构线索并利用多源先验进行几何恢复。基于该框架，我们首先提出HPM-MVS，它通过非局部操作获得精确的初始假设，同时在HPM框架中构建更好的平面先验模型，以进一步促进假设生成。此外，我们进一步提出HPM-MVS++，挖掘图像的结构化区域信息和假设的空间几何关系作为先验知识。然后，它将它们合并到概率图形模型中，最终推导出两种新颖的多视图匹配成本。这显着增强了对挑战性情况的鲁棒性并提高了重建的完整性。在 ETH3D 和 Tanks & Temples 上的实验结果验证了我们的方法的优越性能和强大的泛化能力。该代码可在 https://github.com/CLinvx 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652193",
        "title": "Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning",
        "link": "https://doi.org/10.1109/tpami.2026.3652193",
        "published": "2026",
        "author": "Jifeng Hu, Yanchao Sun, Sili Huang, Siyuan Guo, Hechang Chen, Li Shen, Lichao Sun, Yi Chang, Dacheng Tao",
        "summary": "Recent works have shown the potential of diffusion models in computer vision and natural language processing. Apart from the classical supervised learning fields, diffusion models have also shown strong competitiveness in reinforcement learning (RL) by formulating decision-making as sequential generation. However, incorporating temporal information of sequential data and utilizing it to guide diffusion models to perform better generation is still an open challenge. In this paper, we take one step forward to investigate controllable generation with temporal conditions that are refined from temporal information. We observe the importance of temporal conditions in sequential generation in sufficient scenarios and provide a comprehensive discussion and comparison of different temporal conditions. Based on the observations, we propose an effective temporally-conditional diffusion model coined Temporally-Composable Diffuser (TCD), which extracts temporal information from interaction sequences and explicitly guides generation with temporal conditions. Specifically, we separate the sequences into three parts according to time expansion and identify historical, immediate, and prospective conditions accordingly. Each condition preserves non-overlapping temporal information of sequences, enabling more controllable generation when we jointly use them to guide the diffuser. Finally, we conduct extensive experiments and analysis to reveal the favorable applicability of TCD in offline RL tasks, where our method reaches or matches the best performance compared with prior SOTA baselines. Codes are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/JF-Hu/Temporally-Composable-Diffuser</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于离线强化学习的具有时间条件指导的指令扩散器",
        "abstract_cn": "最近的工作显示了扩散模型在计算机视觉和自然语言处理中的潜力。除了经典的监督学习领域外，扩散模型通过将决策制定为顺序生成，在强化学习（RL）领域也表现出了强大的竞争力。然而，整合序列数据的时间信息并利用它来指导扩散模型更好地生成仍然是一个开放的挑战。在本文中，我们向前迈出了一步，研究了从时间信息中提炼出的时间条件下的可控生成。我们在足够的场景中观察了时间条件在顺序生成中的重要性，并对不同时间条件进行了全面的讨论和比较。基于观察结果，我们提出了一种有效的时间条件扩散模型，称为时间可组合扩散器（TCD），它从交互序列中提取时间信息，并明确指导时间条件的生成。具体来说，我们根据时间扩展将序列分为三个部分，并相应地识别历史条件、当前条件和预期条件。每个条件都保留序列的不重叠时间信息，当我们联合使用它们来引导扩散器时，可以实现更可控的生成。最后，我们进行了大量的实验和分析，以揭示 TCD 在离线 RL 任务中的良好适用性，与之前的 SOTA 基线相比，我们的方法达到或匹配了最佳性能。代码可在 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/JF-Hu/Temporally-Composable-Diffuser</uri> 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651530",
        "title": "Human Motion Prediction via Continual Prior Compensation",
        "link": "https://doi.org/10.1109/tpami.2026.3651530",
        "published": "2026",
        "author": "Jianwei Tang, Jian-Fang Hu, Tianming Liang, Xiaotong Lin, Jiangxin Sun, Wei-Shi Zheng, Jianhuang Lai",
        "summary": "Human Motion Prediction (HMP) aims to predict future human poses at different moments according to observed past motion sequences. Previous approaches mainly treated the prediction of different temporal moments as a single prediction task and learned the predictions of varied moments simultaneously, which would encounter a main limitation: the learning of short term predictions (referring to \"near-future\" prediction) could be hindered by the predictions of long-term (referring to \"far future\" prediction) motions. In this paper, we develop a novel temporal continual learning framework called Continual Prior Compensation (CPC) to progressively train HMP models, in which we divide the prediction task of motions corresponding to varied temporal moments into several subtasks and train the model in a multi-stage manner. To mitigate the prior information forgetting in the progressive training, we further introduce a learnable random variable Prior Compensation Factor (PCF) to explicitly measure the prior knowledge loss. We theoretically show that the PCF can be efficiently learned together with the model parameters by minimizing a reasonable upper bound of the objective function. The proposed CPC is further enhanced to estimate the prior information loss for each subtask and a new framework called Continual Prior Compensation++ (CPC++) with Fine-Grained Prior Compensation Factor (FGPCF) is finally developed. Our CPC and CPC++ frameworks are quite flexible and can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on three HMP benchmark datasets using multiple SOTA HMPbackbones (PGBIG, siMLPe, MotionMixer, and LTD) demonstrate the effectiveness and flexibility of our frameworks.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过连续先验补偿进行人体运动预测",
        "abstract_cn": "人体运动预测（HMP）旨在根据观察到的过去运动序列来预测未来不同时刻的人体姿势。以前的方法主要将不同时间时刻的预测视为单个预测任务，并同时学习不同时刻的预测，这会遇到一个主要限制：短期预测（指“近未来”预测）的学习可能会受到长期（指“远未来”预测）运动的预测的阻碍。在本文中，我们开发了一种称为连续先验补偿（CPC）的新型时间连续学习框架来逐步训练 HMP 模型，其中我们将不同时间时刻对应的运动预测任务划分为多个子任务，并以多阶段的方式训练模型。为了减轻渐进训练中的先验信息遗忘，我们进一步引入了可学习的随机变量先验补偿因子（PCF）来明确测量先验知识损失。我们从理论上证明，通过最小化目标函数的合理上限，可以有效地学习 PCF 和模型参数。所提出的 CPC 进一步增强，以估计每个子任务的先验信息损失，并最终开发了一个名为连续先验补偿 ++（CPC++）和细粒度先验补偿因子（FGPCF）的新框架。我们的CPC和CPC++框架非常灵活，可以轻松与不同的HMP骨干模型集成并适应各种数据集和应用程序。使用多个 SOTA HMPbackbones（PGBIG、siMLPe、MotionMixer 和 LTD）对三个 HMP 基准数据集进行的广泛实验证明了我们框架的有效性和灵活性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652297",
        "title": "Robust Distributed Cooperative Classification with Learned Compressed-Feature Diffusion",
        "link": "https://doi.org/10.1109/tpami.2026.3652297",
        "published": "2026",
        "author": "Xiling Yao, Jie Chen, Jingdong Chen",
        "summary": "Cooperative inference in distributed sensor networks is challenged by limited communication bandwidth and the risk of node failures. This paper introduces Compressed Feature Diffusion for Decentralized Classification (CFD-DC), a novel framework that addresses these challenges. Each node performs local inference using its own features and compressed feature representations received from other nodes. Our approach relies on two key components: first, a trainable feature compressor at each node that learns compact representations, reducing communication while preserving critical discriminative information; second, an adaptive node weighting mechanism that dynamically adjusts the influence of local and remote features, providing robustness to unreliable or failed nodes. Experiments on multi-view image classification and a simulated multi-node underwater acoustic target classification task demonstrate the effectiveness of the framework. The results show competitive performance compared to centralized and state-of-the-art multi-view methods, reduced communication costs, and superior robustness in scenarios with node failures.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "具有学习压缩特征扩散的鲁棒分布式协作分类",
        "abstract_cn": "分布式传感器网络中的协作推理受到有限通信带宽和节点故障风险的挑战。本文介绍了用于分散分类的压缩特征扩散 (CFD-DC)，这是一种解决这些挑战的新颖框架。每个节点使用自己的特征和从其他节点接收到的压缩特征表示来执行本地推理。我们的方法依赖于两个关键组件：首先，每个节点都有一个可训练的特征压缩器，可以学习紧凑的表示，减少通信，同时保留关键的判别信息；其次，自适应节点权重机制，动态调整本地和远程特征的影响，为不可靠或故障节点提供鲁棒性。多视图图像分类和模拟多节点水声目标分类任务的实验证明了该框架的有效性。结果显示，与集中式和最先进的多视图方法相比，具有竞争性的性能，降低了通信成本，并且在节点故障的情况下具有卓越的鲁棒性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653457",
        "title": "MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition",
        "link": "https://doi.org/10.1109/tpami.2026.3653457",
        "published": "2026",
        "author": "Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, Jianhua Tao",
        "summary": "Multimodal emotion recognition plays a vital role in enhancing user experience in human-computer interaction. Over the past few decades, researchers have developed a range of algorithms and made remarkable progress. While each approach demonstrates certain advantages, inconsistent choices in feature extraction methods, evaluation protocols, and experimental settings have hindered fair comparisons among them. These inconsistencies significantly impede the advancement of the field. To address this issue, we introduce MERBench, a unified evaluation benchmark for multimodal emotion recognition. Our goal is to assess the contributions of several key techniques commonly used in prior studies, such as feature selection, multimodal fusion, robustness analysis, fine-tuning, and pre-training. We believe this work offers clear and comprehensive guidance for future research. Based on the evaluation results of MERBench, we further point out some promising research directions. In addition, we present a new emotion dataset, MER2023, specifically designed for the Chinese language environment. This dataset serves as a benchmark for research in multi-label learning, noise robustness, and semi-supervised learning. and encourage researchers to evaluate their algorithms under the same experimental setup as MERBench for fair comparisons.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "MERBench：多模态情感识别统一评估基准",
        "abstract_cn": "多模态情感识别对于增强人机交互中的用户体验起着至关重要的作用。在过去的几十年里，研究人员开发了一系列算法并取得了显着的进展。虽然每种方法都表现出一定的优势，但特征提取方法、评估协议和实验设置的不一致选择阻碍了它们之间的公平比较。这些不一致严重阻碍了该领域的进步。为了解决这个问题，我们引入了 MERBench，一个多模态情感识别的统一评估基准。我们的目标是评估先前研究中常用的几种关键技术的贡献，例如特征选择、多模态融合、鲁棒性分析、微调和预训练。我们相信这项工作为未来的研究提供了清晰而全面的指导。基于MERBench的评估结果，我们进一步指出了一些有前景的研究方向。此外，我们还提出了一个新的情感数据集 MER2023，专为中文语言环境设计。该数据集可作为多标签学习、噪声鲁棒性和半监督学习研究的基准。并鼓励研究人员在与 MERBench 相同的实验设置下评估他们的算法，以进行公平比较。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651700",
        "title": "CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey",
        "link": "https://doi.org/10.1109/tpami.2026.3651700",
        "published": "2026",
        "author": "Jindong Li, Yongguang Li, Yali Fu, Jiahong Liu, Yixin Liu, Menglin Yang, Irwin King",
        "summary": "As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP's growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maintained at:<uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "CLIP 支持的领域泛化和领域适应：综合调查",
        "abstract_cn": "随着机器学习的发展，领域泛化（DG）和领域适应（DA）对于提高跨不同环境的模型鲁棒性变得至关重要。对比语言-图像预训练 (CLIP) 在这些任务中发挥着核心作用，提供强大的零样本功能，使模型能够在看不见的领域中有效运行。然而，尽管 CLIP 的影响力越来越大，但还没有全面的调查系统地审查其在 DG 和 DA 中的应用，这凸显了进行此项审查的必要性。这项调查提供了对 CLIP 驱动的 DG 和 DA 的统一和深入的概述。在审查方法之前，我们建立精确而完整的场景定义，涵盖源可访问性（SA 与 SF）、源编号（SS 与 MS）和标签关系（CS、PS、OS、OPS），形成一个连贯的分类法来构建所有后续分析。对于 DG，我们将方法分类为增强任务对齐的即时优化技术和利用 CLIP 作为可转移特征提取骨干的架构。对于DA，我们研究了依赖于标记源数据的源可用方法和主要在目标域样本上运行的无源方法，强调能够跨异构设置进行适应的知识转移机制。我们进一步为 DG 和 DA 提供综合趋势分析，揭示总体模式、方法原则和情景相关行为。然后，我们讨论现实部署场景、LLM 知识集成、多模式融合、可解释性和灾难性遗忘等关键挑战，并概述开发可扩展且值得信赖的基于 CLIP 的 DG 和 DA 系统的未来方向。通过综合现有研究并强调关键差距，这项调查为研究人员和从业者提供了可行的见解，激发了利用 CLIP 提高现实场景中领域稳健性的新策略。持续更新的相关工作列表保存在：<uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation</uri>。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652225",
        "title": "HGNNv2: Stable Hypergraph Neural Networks",
        "link": "https://doi.org/10.1109/tpami.2026.3652225",
        "published": "2026",
        "author": "Yue Gao, Jielong Yan, Yifan Feng, Xiangmin Han, Shihui Ying, Zongze Wu, Han Hu",
        "summary": "Hypergraph neural networks (HGNNs) are widely used models for analyzing higher-order relational data. HGNNs suffer from the rapid performance degradation with increasing layers. Hypergraph dynamic system (HDS) is a potential way to deal with this challenge. However, hypergraph dynamic system is confined to a time-continuous isotropic model, lacking positional information in the structural space of the hypergraph. In contrast, anisotropic diffusion can capture structural space differences among vertices, providing a more precise representation of the information propagation process in hypergraph structures than isotropic diffusion. In this paper, we introduce HGNNv2, a stable hypergraph neural network, which is built as a hypergraph dynamic system with partial differential equation (PDE). This model incorporates a position-aware anisotropic diffusion term and an external control term. We further present the vertex-rooted subtree method to determine anisotropic diffusion intensity. HGNNv2 has properties that vertices occupying equivalent positions in the structural space share equivalent structural labels and positional features. Experiments on 6 hypergraph datasets and 3 graph datasets reveal that HGNNv2 outperforms all 12 compared methods. HGNNv2 is capable of achieving stable final representations and task accuracy even under noisy conditions. HGNNv2 achieves stable performance with fewer layers than hypergraph dynamic systems employing isotropic diffusion. We provide feature visualizations to illustrate the evolution of representations.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "HGNNv2：稳定的超图神经网络",
        "abstract_cn": "超图神经网络（HGNN）是广泛用于分析高阶关系数据的模型。随着层数的增加，HGNN 的性能会迅速下降。超图动态系统（HDS）是应对这一挑战的潜在方法。然而，超图动态系统仅限于时间连续各向同性模型，缺乏超图结构空间中的位置信息。相比之下，各向异性扩散可以捕获顶点之间的结构空间差异，从而比各向同性扩散更精确地表示超图结构中的信息传播过程。在本文中，我们介绍了 HGNNv2，一种稳定的超图神经网络，它被构建为具有偏微分方程（PDE）的超图动态系统。该模型包含位置感知各向异性扩散项和外部控制项。我们进一步提出了顶点根子树方法来确定各向异性扩散强度。 HGNNv2 具有以下性质：在结构空间中占据等效位置的顶点共享等效的结构标签和位置特征。对 6 个超图数据集和 3 个图数据集的实验表明 HGNNv2 优于所有 12 种比较方法。即使在噪声条件下，HGNNv2 也能够实现稳定的最终表示和任务准确性。与采用各向同性扩散的超图动态系统相比，HGNNv2 用更少的层数实现了稳定的性能。我们提供特征可视化来说明表示的演变。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3650864",
        "title": "Parse, Align and Aggregate: Graph-driven Compositional Reasoning for Video Question Answering",
        "link": "https://doi.org/10.1109/tpami.2026.3650864",
        "published": "2026",
        "author": "Jiangtong Li, Zhaohe Liao, Fengshun Xiao, Tianjiao Li, Qiang Zhang, Haohua Zhao, Li Niu, Guang Chen, Liqing Zhang, Changjun Jiang",
        "summary": "Video Question-Answering (VideoQA) enables machines to interpret and respond to complex video content, advanc ing human-computer interaction. However, existing multimodal large language models (MLLMs) often provide incomplete or opaque explanations and existing benchmarks mainly focus on the correction of final answers, limiting insight into their reasoning processes and hindering both transparency and verifiability. To address this gap, we propose the Question Parsing, Video Alignment and Answer Aggregation framework (QPVA3), which leverages a compositional graph to drive visual and logical reasoning in VideoQA. Specifically, QPVA3 consists of three core components, the planner, executor, and reasoner to generate the compositional graph and conduct graph-driven reasoning. For the original question, the planner parses it into the compositional graph, capturing the underlying reasoning logic and structuring it into a series of interconnected questions. For each question in compositional graph, the executor aligns the video by selecting relevant video clips and generates answers, ensuring accurate, context-specific responses. For each question with its first-order descents, the reasoner aggregates answers by integrating rea soning logic with visual evidence, resolving conflicts to produce a coherent and accurate response. Moreover, to assess the performance of existing MLLMs in the reasoning processes of VideoQA, we introduce novel compositional consistency metrics and construct a VideoQA benchmark (QPVA3Bench) with 3,492 question-video tuples, each annotated with detailed composi tional graphs and fine-grained answers. We evaluate the QPVA3 framework on QPVA3Bench and 5 other VideoQA benchmarks. Experimental results demonstrate that our framework improves both consistency and accuracy compared to baselines, leading to a more transparent and verifiable VideoQA system. This approach has the potential to advance the field, as supported by our comprehensive evaluation and benchmarking efforts. Code and dataset are available at https://github.com/QPVA3/QPVA3-PAMI.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "解析、对齐和聚合：视频问答的图驱动组合推理",
        "abstract_cn": "视频问答 (VideoQA) 使机器能够解释和响应复杂的视频内容，从而促进人机交互。然而，现有的多模态大语言模型（MLLM）通常提供不完整或不透明的解释，并且现有基准主要侧重于最终答案的修正，限制了对其推理过程的洞察，并阻碍了透明度和可验证性。为了解决这一差距，我们提出了问题解析、视频对齐和答案聚合框架（QPVA3），该框架利用组合图来驱动 VideoQA 中的视觉和逻辑推理。具体来说，QPVA3由规划器、执行器和推理器三个核心组件组成，用于生成组合图并进行图驱动推理。对于原始问题，规划器将其解析为组合图，捕获底层推理逻辑并将其结构化为一系列相互关联的问题。对于组合图中的每个问题，执行器通过选择相关视频剪辑来对齐视频并生成答案，确保准确、针对特定上下文的响应。对于具有一阶下降的每个问题，推理器通过将推理逻辑与视觉证据相结合来聚合答案，解决冲突以产生连贯且准确的响应。此外，为了评估现有 MLLM 在 VideoQA 推理过程中的性能，我们引入了新颖的组合一致性指标，并构建了一个包含 3,492 个问题视频元组的 VideoQA 基准（QPVA3Bench），每个元组都注释有详细的组合图和细粒度的答案。我们在 QPVA3Bench 和其他 5 个 VideoQA 基准测试上评估了 QPVA3 框架。实验结果表明，与基线相比，我们的框架提高了一致性和准确性，从而形成更加透明和可验证的 VideoQA 系统。在我们全面评估和基准测试工作的支持下，这种方法有潜力推动该领域的发展。代码和数据集可在 https://github.com/QPVA3/QPVA3-PAMI 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651260",
        "title": "Neuron Abandoning Attention Flow: Visual Explanation of Dynamics Inside CNN Models",
        "link": "https://doi.org/10.1109/tpami.2026.3651260",
        "published": "2026",
        "author": "Yi Liao, Yongsheng Gao, Weichuan Zhang",
        "summary": "In this paper, we present a Neuron Abandoning Attention Flow (NAFlow) method to address the unsolved problem of visually explaining the attention evolution dynamics inside CNNs when making their classification decisions. A novel cascading neuron abandoning back- propagation algorithm is designed to precisely exclude the abandoned neurons on all intermediate layers inside a CNN model for the first time. Firstly, a Neuron Abandoning Back-Propagation module is proposed to generate Back-Propagation Feature Maps (BPFM) by using inverse function of the intermediate layers of CNN models, on which the neurons not used for decision-making are removed. Meanwhile, the cascading NA-BP modules calculate the tensors of importance coefficients which are linearly combined with the tensors of BPFMs to form the NAFlow. Secondly, to be able to visualize attention flow for similarity metric-based CNN models, a new channel contribution weights module is proposed to calculate the importance coefficients via Jacobian Matrix. Extensive evaluations demonstrate the effectiveness of the proposed NAFlow across eleven widely-used CNN models for various tasks of general image classification, contrastive learning classification, few-shot image classification, and image retrieval.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "神经元放弃注意力流：CNN 模型内部动力学的视觉解释",
        "abstract_cn": "在本文中，我们提出了一种神经元放弃注意力流（NAFlow）方法，以解决在做出分类决策时直观地解释 CNN 内部注意力演化动态的未解决问题。一种新颖的级联神经元放弃反向传播算法旨在首次精确排除CNN模型内所有中间层上的废弃神经元。首先，提出了一种神经元放弃反向传播模块，利用CNN模型中间层的逆函数生成反向传播特征图（BPFM），去除不用于决策的神经元。同时，级联的NA-BP模块计算重要性系数张量，这些张量与BPFM的张量线性组合以形成NAFlow。其次，为了能够可视化基于相似性度量的 CNN 模型的注意力流，提出了一种新的通道贡献权重模块来通过雅可比矩阵计算重要性系数。广泛的评估证明了所提出的 NAFlow 在 11 个广泛使用的 CNN 模型中对于一般图像分类、对比学习分类、少样本图像分类和图像检索等各种任务的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651319",
        "title": "Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models",
        "link": "https://doi.org/10.1109/tpami.2026.3651319",
        "published": "2026",
        "author": "Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink",
        "summary": "Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/donghao51/Awesome-Multimodal-Adaptation</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "多模态适应和泛化的进展：从传统方法到基础模型",
        "abstract_cn": "领域适应和泛化对于现实世界的应用至关重要，例如自动驾驶和医学成像，其中模型必须在具有不同数据分布的环境中可靠运行。然而，这些任务具有挑战性，因为模型需要克服由照明、天气、传感器配置等变化引起的各种域间隙。由于不同模式中存在独特的挑战，以不同模式同时解决领域差距（称为多模式领域适应和泛化）甚至更具挑战性。在过去的几年里，这些领域取得了重大进展，应用范围从动作识别到语义分割等等。最近，大规模预训练多模态基础模型（例如 CLIP）的出现激发了众多研究，这些研究利用这些模型来增强下游适应和泛化。这项调查总结了多模式适应和泛化方面的最新进展，特别是这些领域如何从传统方法演变为基础模型。具体来说，本次调查涵盖（1）多模态域适应，（2）多模态测试时间适应，（3）多模态域泛化，（4）借助多模态基础模型的域适应和泛化，以及（5）多模态基础模型的适应。对于每个主题，我们都会正式定义问题并对现有方法进行彻底审查。此外，我们分析相关数据集和应用程序，突出开放的挑战和潜在的未来研究方向。我们还维护一个活跃的存储库，其中包含最新文献并支持这些领域的研究活动，网址为 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/donghao51/Awesome-Multimodal-Adaptation</uri>。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652302",
        "title": "Model Lineage Analysis: Determination and Closeness Measurement",
        "link": "https://doi.org/10.1109/tpami.2026.3652302",
        "published": "2026",
        "author": "Chen Tang, Lan Zhang, Qi Zhao, Xirong Zhuang, Jiewei Lai, Xiang-yang Li",
        "summary": "As model modification techniques are increasingly employed to obtain high-performing machine learning models at reduced costs, identifying lineage relationships-i.e., whether one model is derived from another-has garnered significant research interest. However, existing approaches are largely empirical, lack theoretical grounding, and are often ineffective against high-impact modifications. Furthermore, none have addressed the measurement of lineage closeness, which quantifies the degree of modification between models. In this paper, we reformulate the model lineage determination problem as a question of whether two models' parameters reside within the same local optimum of the loss landscape. Based on this formulation, we propose a simple yet effective method for lineage determination. We further explore the impact of various modification techniques on models' decision boundaries using visualization techniques, and observe that changes in decision boundaries serve as an accurate metric for lineage closeness. Leveraging this insight, we propose a task-agnostic and modification-type-agnostic method to measure lineage closeness by computing the mean adversarial distance from data points to decision boundaries and the matching rate of data point predictions. To reduce computational overhead, we design an efficient sampling strategy for data point selection. Extensive experiments demonstrate that our approach achieves 100% accuracy in model lineage determination and provides precise, quantitative measurements of lineage closeness across a wide range of modification scenarios.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "模型谱系分析：确定和紧密度测量",
        "abstract_cn": "随着模型修改技术越来越多地用于以降低的成本获得高性能的机器学习模型，识别谱系关系（即一个模型是否源自另一个模型）引起了人们的广泛研究兴趣。然而，现有的方法主要是经验性的，缺乏理论基础，并且往往对高影响力的修改无效。此外，没有人涉及血统紧密度的测量，它量化了模型之间的修改程度。在本文中，我们将模型谱系确定问题重新表述为两个模型参数是否位于损失景观的同一局部最优值内的问题。基于这个公式，我们提出了一种简单而有效的谱系确定方法。我们使用可视化技术进一步探索各种修改技术对模型决策边界的影响，并观察决策边界的变化可以作为谱系紧密度的准确度量。利用这种见解，我们提出了一种与任务无关和修改类型无关的方法，通过计算从数据点到决策边界的平均对抗距离以及数据点预测的匹配率来测量谱系紧密度。为了减少计算开销，我们设计了一种有效的数据点选择采样策略。大量实验表明，我们的方法在模型谱系确定方面实现了 100% 的准确性，并在各种修改场景中提供了精确、定量的谱系紧密度测量。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651728",
        "title": "Unleashing the Power of Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation",
        "link": "https://doi.org/10.1109/tpami.2026.3651728",
        "published": "2026",
        "author": "Duo Peng, Zhengbo Zhang, Ping Hu, Qiuhong Ke, De Wen Soh, Mohammed Bennamoun, Jun Liu",
        "summary": "Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "释放文本到图像扩散模型的力量，用于类别无关的姿势估计",
        "abstract_cn": "与类别无关的姿势估计（CAPE）旨在在几次镜头设置中检测未见过的对象类别的关键点，其中标记数据的稀缺对泛化提出了重大挑战。在这项工作中，我们提出了即时姿势匹配（PPM），这是一种新颖的框架，可以释放现成的 CAPE 文本到图像扩散模型的强大功能。 PPM 通过文本到图像扩散模型从少量示例中学习伪提示。这些学习到的伪提示捕获关键点的语义信息，然后可以使用这些信息从图像中定位相同类型的关键点。为了提供代表性初始化的提示，我们引入了一种与类别无关的预训练策略来捕获跨类别和关键点共享的前景。为了支持可靠的提示预训练，我们提出了前景感知区域聚合（FARA）模块来提供强大且一致的监督信号。基于前景先验，进一步提出了前景引导注意细化（FGAR）模块，以加强交叉注意响应以实现准确的关键点定位。为了提高效率，即时集成推理 (PEI) 方案支持联合关键点预测。与之前高度依赖基类别注释数据的方法不同，我们的 PPM 框架可以在无基类别的设置中运行，同时保持强大的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652081",
        "title": "Incremental Online Learning of Randomized Neural Network with Forward Regularization",
        "link": "https://doi.org/10.1109/tpami.2026.3652081",
        "published": "2026",
        "author": "Junda Wang, Minghui Hu, Ning Li, Abdulaziz Al-Ali, Ponnuthurai Nagaratnam Suganthan",
        "summary": "Online learning of deep neural networks faces challenges such as delayed non-incremental updating, increasing consumption, retrospective retraining, and catastrophic forgetting. To alleviate these drawbacks and achieve progressive immediate decision-making, we propose a novel Incremental Online Learning (IOL) framework of Randomized Neural Networks (Randomized NN), facilitating continuous improvements and analytics to Randomized NN performance in online scenarios. Within the framework, we further formulate IOL with ridge regularization (-R) and IOL with forward regularization (-F), both avoiding retrospective retraining and catastrophic forgetting. Moreover, the incremental algorithms for -R/-F on non-stationary batch stream are derived, featuring recursive weight updates and variable learning rates. Compared to -R, we recommend -F which improves learning performance using future unlabeled observations while further reducing online regrets to offline global experts. Additionally, we conduct a detailed analysis and theoretically derive relative cumulative regret bounds of the Randomized NN learners for -R/-F under adversarial assumptions via a novel methodology and present several corollaries, from which we observed the superiority in online learning acceleration and declined regret bounds of employing -F in IOL. Finally, our proposed methods were rigorously examined across diverse tasks, from simulation, regression, and classification tasks, to long-term time-series forecasting (LTSF) and continual learning (CL) fields, which distinctly validated the efficacy of the IOL frameworks and the advantages of forward regularization.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "具有前向正则化的随机神经网络增量在线学习",
        "abstract_cn": "深度神经网络的在线学习面临延迟非增量更新、增加消耗、回顾性再训练和灾难性遗忘等挑战。为了缓解这些缺点并实现渐进式即时决策，我们提出了一种新颖的随机神经网络（Randomized NN）增量在线学习（IOL）框架，促进在线场景中随机 NN 性能的持续改进和分析。在该框架内，我们进一步制定了具有岭正则化（-R）的IOL和具有前向正则化（-F）的IOL，两者都避免了回顾性再训练和灾难性遗忘。此外，还推导了非平稳批流上的-R/-F增量算法，具有递归权重更新和可变学习率的特点。与-R相比，我们推荐-F，它可以利用未来未标记的观察来提高学习性能，同时进一步减少对线下全球专家的在线遗憾。此外，我们通过一种新颖的方法进行了详细的分析，并从理论上推导了对抗性假设下 -R/-F 的随机 NN 学习器的相对累积遗憾界限，并提出了几个推论，从中我们观察到了在 IOL 中使用 -F 的在线学习加速和下降遗憾界限的优越性。最后，我们提出的方法在不同的任务中进行了严格的检验，从模拟、回归和分类任务，到长期时间序列预测（LTSF）和持续学习（CL）领域，这明显验证了 IOL 框架的有效性和前向正则化的优势。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651447",
        "title": "Learning Physics-Informed Noise Models from Dark Frames for Low-Light Raw Image Denoising",
        "link": "https://doi.org/10.1109/tpami.2026.3651447",
        "published": "2026",
        "author": "Hansen Feng, Lizhi Wang, Yiqi Huang, Yuzhi Wang, Lin Zhu, Hua Huang",
        "summary": "Recently, the mainstream practice for training low-light raw image denoising methods has shifted towards employing synthetic data. Noise modeling, which focuses on characterizing the noise distribution of real-world sensors, profoundly influences the effectiveness and practicality of synthetic data. Currently, physics-based noise modeling struggles to characterize the entire real noise distribution, while learning-based noise modeling impractically depends on paired real data. In this paper, we propose a novel strategy: learning the noise model from dark frames instead of paired real data, to break down the data dependency. Based on this strategy, we introduce an efficient physics-informed noise neural proxy (PNNP) to approximate the real-world sensor noise model. Specifically, we integrate physical priors into neural proxies and introduce three efficient techniques: physics-guided noise decoupling (PND), physics-aware proxy model (PPM), and differentiable distribution loss (DDL). PND decouples the dark frame into different components and handles different levels of noise flexibly, which reduces the complexity of noise modeling. PPM incorporates physical priors to constrain the synthetic noise, which promotes the accuracy of noise modeling. DDL provides explicit and reliable supervision for noise distribution, which promotes the precision of noise modeling. PNNP exhibits powerful potential in characterizing the real noise distribution. Extensive experiments on public datasets demonstrate superior performance in practical low-light raw image denoising. The source code will be publicly available at the project homepage.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "从暗帧中学习基于物理的噪声模型以实现低光原始图像去噪",
        "abstract_cn": "最近，训练低光原始图像去噪方法的主流实践已经转向使用合成数据。噪声建模专注于表征现实世界传感器的噪声分布，深刻影响合成数据的有效性和实用性。目前，基于物理的噪声建模很难表征整个真实噪声分布，而基于学习的噪声建模则不切实际地依赖于成对的真实数据。在本文中，我们提出了一种新颖的策略：从暗帧而不是配对的真实数据中学习噪声模型，以打破数据依赖性。基于这一策略，我们引入了一种高效的物理信息噪声神经代理（PNNP）来近似真实世界的传感器噪声模型。具体来说，我们将物理先验集成到神经代理中，并引入三种有效的技术：物理引导噪声解耦（PND）、物理感知代理模型（PPM）和可微分分布损失（DDL）。 PND将暗帧解耦成不同的分量，灵活处理不同级别的噪声，降低了噪声建模的复杂度。 PPM 结合了物理先验来约束合成噪声，从而提高了噪声建模的准确性。 DDL为噪声分布提供了明确且可靠的监控，从而提高了噪声建模的精度。 PNNP 在表征真实噪声分布方面表现出强大的潜力。对公共数据集的大量实验证明了其在实际低光原始图像去噪方面的卓越性能。源代码将在项目主页上公开。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652557",
        "title": "ConsistentID: Portrait Generation With Multimodal Fine-Grained Identity Preserving",
        "link": "https://doi.org/10.1109/tpami.2026.3652557",
        "published": "2026",
        "author": "Jiehui Huang, Xiao Dong, Wenhui Song, Zheng Chong, Zhenchao Tang, Jun Zhou, Yuhao Cheng, Long Chen, Hanhui Li, Yiqiang Yan, Shengcai Liao, Xiaodan Liang",
        "summary": "Diffusion-based technologies have made significant strides, particularly in personalized and customized facial generation. However, existing methods struggle to achieve high-fidelity and detailed identity (ID) consistency. This is mainly due to two challenges: insufficient fine-grained control over specific facial areas and the absence of a comprehensive strategy for ID preservation that accounts for both intricate facial details and the overall facial structure. To address these limitations, we introduce ConsistentID, an innovative method crafted for diverse identity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image. ConsistentID comprises two core components: a multimodal facial prompt generator and an ID-preservation network. The facial prompt generator combines localized facial features, facial feature descriptions, and overall facial descriptions to enhance the precision of facial detail reconstruction. The ID-preservation network, optimized with a facial attention localization strategy, ensures consistent identity preservation across facial regions. Together, these components leverage fine-grained multimodal identity information to improve identity preservation accuracy significantly. To drive ConsistentID's training, we propose a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets. Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset. In addition, although ConsistentID introduces more multimodal ID information, it still maintains rapid inference speed during the generation process. Our codes and pre-trained checkpoints are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/JackAILab/ConsistentID</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "ConsolidatedID：具有多模式细粒度身份保护的肖像生成",
        "abstract_cn": "基于扩散的技术已经取得了重大进展，特别是在个性化和定制的面部生成方面。然而，现有方法很难实现高保真度和详细的身份 (ID) 一致性。这主要是由于两个挑战：对特定面部区域的细粒度控制不足，以及缺乏既考虑复杂的面部细节又考虑整体面部结构的全面的身份保存策略。为了解决这些限制，我们引入了 ConcientID，这是一种创新方法，专门用于在细粒度多模式面部提示下生成多样化的身份保留肖像，仅利用单个参考图像。 ConsolidatedID 包含两个核心组件：多模式面部提示生成器和 ID 保存网络。面部提示生成器结合面部局部特征、面部特征描述和面部整体描述，提高面部细节重建的精度。通过面部注意力定位策略进行优化的身份保存网络可确保跨面部区域的一致身份保存。这些组件共同利用细粒度的多模式身份信息来显着提高身份保存的准确性。为了推动 ConcientID 的训练，我们提出了一个细粒度的肖像数据集 FGID，其中包含超过 500,000 张面部图像，比现有的公共面部数据集提供了更大的多样性和全面性。实验结果证实，我们的 ConcientID 在个性化面部生成方面实现了卓越的精度和多样性，超越了 MyStyle 数据集中的现有方法。此外，ConstantID虽然引入了更多的多模态ID信息，但在生成过程中仍然保持快速的推理速度。我们的代码和预训练检查点可在 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/JackAILab/ConcientID</uri> 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652456",
        "title": "ARMOR: Shielding Unlearnable Examples against Data Augmentation",
        "link": "https://doi.org/10.1109/tpami.2026.3652456",
        "published": "2026",
        "author": "Xueluan Gong, Yuji Wang, Yanjiao Chen, Haocheng Dong, Yiming Li, Mengyuan Sun, Shuaike Li, Qian Wang",
        "summary": "Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs). To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs. Recently, unlearnable examples [18] are proposed to minimize the training loss such that the model learns almost nothing. However, raw data are often pre-processed before being used for training, which may restore the private information of protected data. In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned. We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation. To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation. In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class. We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process. Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR. Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation. ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines. We also show that ARMOR is robust to adversarial training. We will open-source our codes upon publication.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "ARMOR：屏蔽不可学习的示例以防止数据增强",
        "abstract_cn": "在线发布的私人数据可能会被未经授权的各方收集以训练深度神经网络 (DNN)。为了保护隐私，可以在原始样本中添加防御性噪声，以降低 DNN 的可学习性。最近，提出了不可学习的示例[18]来最小化训练损失，使得模型几乎什么也学不到。然而，原始数据在用于训练之前通常会进行预处理，这可能会恢复受保护数据的私有信息。在本文中，我们揭示了数据增强引起的数据隐私侵犯，数据增强是一种常用的数据预处理技术，用于提高模型泛化能力，就我们而言，这是此类技术中的第一个。我们证明，数据增强可以将在不可学习的示例上训练的模型的准确率从 21.3% 显着提高到 66.1%。为了解决这个问题，我们提出了一个名为 ARMOR 的防御框架，以保护数据隐私免受数据增强的潜在破坏。为了克服无法访问模型训练过程的困难，我们设计了一种非本地模块辅助代理模型，可以更好地捕获数据增强的效果。此外，我们设计了一种替代增强选择策略，可最大化增强样本和非增强样本之间的分布对齐，从而为每个类别选择最佳增强策略。我们还使用动态步长调整算法来增强防御性噪声生成过程。在 4 个数据集和 5 种数据增强方法上进行了大量的实验来验证 ARMOR 的性能。与 6 种最先进的防御方法的比较表明，ARMOR 可以在数据增强下保留受保护的私有数据的不可学习性。 ARMOR 使在增强保护样本上训练的模型的测试准确度比基线降低了 60%。我们还表明 ARMOR 对于对抗性训练具有鲁棒性。我们将在发布后开源我们的代码。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652609",
        "title": "Causal Inference Via Style Bias Deconfounding for Domain Generalization",
        "link": "https://doi.org/10.1109/tpami.2026.3652609",
        "published": "2026",
        "author": "Jiaxi Li, Di Lin, Hao Chen, Hongying Liu, Liang Wan, Wei Feng",
        "summary": "Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in real-world visual applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework that explicitly addresses style as a confounding factor to enhance domain generalization in image modalities. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a backdoor causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过风格偏差去混杂进行因果推理以实现领域泛化",
        "abstract_cn": "深度神经网络 (DNN) 经常难以处理分布外的数据，从而限制了它们在现实世界视觉应用中的可靠性。为了解决这个问题，开发了领域泛化方法来从单个或多个训练领域学习领域不变的特征，从而能够泛化到看不见的测试领域。然而，现有的方法通常忽略了训练集中风格频率的影响。这种疏忽使模型容易捕捉由风格混杂因素引起的虚假视觉相关性，而不是学习真正的因果表示，从而破坏推理的可靠性。在这项工作中，我们介绍了风格解混因果学习（SDCL），这是一种基于因果推理的新颖框架，它明确地将风格作为一个混杂因素来解决，以增强图像模态的领域泛化。我们的方法首先构建针对领域泛化问题的结构因果模型（SCM），并应用后门调整策略来考虑风格影响。在此基础上，我们设计了一个风格引导专家模块（SGEM），在训练期间自适应地聚类风格分布，捕获全局混杂风格。此外，后门因果学习模块（BDCL）在特征提取过程中进行因果干预，确保将全局混杂风格公平地整合到样本预测中，有效减少风格偏差。 SDCL 框架具有高度通用性，可以与最先进的数据增强技术无缝集成。跨不同自然和医学图像识别任务的广泛实验验证了其功效，在多域和更具挑战性的单域泛化场景中展示了卓越的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652316",
        "title": "OIF-PCR++: Point Cloud Registration via Progressive Distillation of Conditional Positional Encoding",
        "link": "https://doi.org/10.1109/tpami.2026.3652316",
        "published": "2026",
        "author": "Fan Yang, Zhi Chen, Nanjun Yuan, Lin Guo, Wenbing Tao",
        "summary": "Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well to complex real-world scenarios. The code will be released upon acceptance to support the research community.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "OIF-PCR++：通过条件位置编码的渐进蒸馏进行点云配准",
        "abstract_cn": "Transformer 架构在各种视觉任务（包括点云配准）中显示出巨大的潜力。位置编码作为顺序感知模块，在 Transformer 框架中起着至关重要的作用。在本文中，我们提出了 OIF-PCR++，一种用于点云配准的条件位置编码（CPE）方法。核心 CPE 模块在不同阶段利用长度和矢量编码，以要注册的点云之间的相对姿态状态为条件。因此，它通过结合几何线索逐步减轻特征模糊性。在所提出的 CPE 的基础上，我们引入了一种迭代位置编码优化流程，包括两个阶段：1）我们通过可微的最佳传输层找到一个对应关系，并使用它将长度信息编码到点云特征中，这通过增强空间一致性来减轻不同参考帧带来的挑战。 2）我们应用渐进的方向对齐策略来实现配对点云之间的粗略对齐，然后借助这种对齐逐渐合并方向信息，进一步增强特征的独特性并减少特征的模糊性。通过这种迭代优化过程，长度和方向信息被有效地集成，以实现一致且独特的位置编码，从而能够学习有区别的点云特征。此外，我们提出了一种内部传播机制，可以和谐地集成一致的几何信息以进行位置编码。所提出的位置编码非常高效，仅略微增加计算开销，同时显着提高特征可区分性。大量的实验表明，与室内、室外、对象级和多路基准的最先进方法相比，我们的方法实现了卓越的性能，同时也可以很好地推广到复杂的现实场景。该代码将在接受后发布，以支持研究社区。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651246",
        "title": "How to Break It Down for Building It Up? Theory-Guided Graph Decomposition Learning for Spatiotemporal Traffic Prediction",
        "link": "https://doi.org/10.1109/tpami.2026.3651246",
        "published": "2026",
        "author": "Jiahao Ji, Jingyuan Wang, Yu Mou, Cheng Long, Junjie Wu",
        "summary": "Traffic state prediction based on spatiotemporal data has become a prominent focus in data-driven AI research. While significant progress has been made, most mainstream approaches assume uniform spatial and temporal correlations across conditions and use shared parameters for all scenarios. This simplification overlooks the complexity and heterogeneity inherent in human mobility patterns, often leading to suboptimal predictions. Recently, methods adopting the \"decompose, then predict\" (DTP) paradigm have gained traction. These methods break down data into smaller, manageable subcomponents, each predicted using dedicated parameters. Although effective in practice, DTP methods face unresolved theoretical questions: What type of decomposition truly makes subcomponents more manageable than the original data? To address this, we present an information theory-based analysis that derives sufficient conditions for a decomposition algorithm to reduce data-induced prediction errors. These conditions suggest that an effective algorithm should ensure decomposed components are as independent as possible, a principle we term the Component Independence Principle. Guided by this principle, we introduce the Theory-guided Graph Decomposition Learning (TGDL) framework, which decomposes graph-based multivariate time series data into approximately independent subgraph components that are easier to predict than the original data. Moreover, TGDL is a portable framework that can be integrated into any graph-based traffic prediction model to improve its predictive performance. Extensive experiments on four public datasets demonstrate the effectiveness of our approach. With a solid theoretical foundation, our TGDL enhances the performance of diverse traffic prediction models, yielding an average improvement of 19.37% across experiments. Our code is available at https://github.com/bigscity/TGDL.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "如何分解它以建立它？用于时空交通预测的理论引导图分解学习",
        "abstract_cn": "基于时空数据的交通状态预测已成为数据驱动的人工智能研究的一个突出焦点。虽然已经取得了重大进展，但大多数主流方法都假设不同条件下的统一空间和时间相关性，并为所有场景使用共享参数。这种简化忽略了人类流动模式固有的复杂性和异质性，常常导致次优的预测。最近，采用“分解然后预测”（DTP）范式的方法受到关注。这些方法将数据分解为更小的、可管理的子组件，每个子组件都使用专用参数进行预测。尽管在实践中有效，DTP 方法面临着未解决的理论问题：哪种类型的分解真正使子组件比原始数据更易于管理？为了解决这个问题，我们提出了一种基于信息论的分析，该分析为分解算法得出充分的条件，以减少数据引起的预测误差。这些条件表明，有效的算法应确保分解的组件尽可能独立，我们将这一原则称为组件独立原则。在这一原则的指导下，我们引入了理论引导的图分解学习（TGDL）框架，该框架将基于图的多元时间序列数据分解为比原始数据更容易预测的近似独立的子图组件。此外，TGDL 是一个便携式框架，可以集成到任何基于图的流量预测模型中，以提高其预测性能。对四个公共数据集的广泛实验证明了我们方法的有效性。凭借坚实的理论基础，我们的 TGDL 增强了各种流量预测模型的性能，在实验中平均提高了 19.37%。我们的代码可在 https://github.com/bigscity/TGDL 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3651754",
        "title": "Accelerated Optimization of Large Mixture-of-Experts Models by Density-Aware Multi-Stage Learning",
        "link": "https://doi.org/10.1109/tpami.2026.3651754",
        "published": "2026",
        "author": "Jianxing Yu, Haowei Jiang, Huaijie Zhu, Wenqing Chen, Yanghui Rao, Qinliang Su, Jian Yin",
        "summary": "This article aims to speed up the training of large neural networks with the Mixture-of-Experts (MoE) structure. Training MoE often needs a lot of computing resources due to its large scale. Traditional acceleration methods either degrade prediction performance or rely on dedicated hardware with additional resources, but the resources are usually limited in real applications. One solution is to resort to new optimization strategies, such as learning from easy to hard by multiple stages. However, existing strategies are designed mainly for networks with a serial structure, but MoE has multiple expert networks working in parallel. They employ an identical learning plan for all experts, ignoring that each expert's learning domain and speed differ, resulting in some experts being over-learned while others being under-learned. This mismatch will make it hard for experts to train together, harming training efficiency. To address this problem, we propose a new training acceleration framework. It can customize an effective learning plan for each expert by considering their training progress, avoiding blindly searching in a huge parameter space. In detail, we first design a multi-stage planner that starts with optimizing a subpart of the network and then scales it up to retrain until it expands to an entire network. It uses the density function to assess the knowledge gained by the expert in each stage, giving priority to the experts who learn faster to increase the training scale, so as to boost convergence. Afterward, we exploit the growth operator to add the expert training scale of the next stage. In each stage, the network would converge to some locally optimal values. That can provide a better initialization to train the next stage more easily, since the time and data required for training from scratch are greatly reduced. To alleviate the gradient vanishing problem caused by network growth, we develop a scheduler to dynamically adjust the learning rate. Extensive experiments are conducted to validate the effectiveness of our method. The results show that we can obtain more than 25% training acceleration on average.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过密度感知多阶段学习加速大型混合专家模型的优化",
        "abstract_cn": "本文旨在利用混合专家（MoE）结构加速大型神经网络的训练。由于规模庞大，训练 MoE 通常需要大量的计算资源。传统的加速方法要么降低预测性能，要么依赖具有额外资源的专用硬件，但在实际应用中资源通常是有限的。一种解决方案是采取新的优化策略，例如从易到难分多个阶段学习。然而，现有的策略主要是针对串行结构的网络设计的，但MoE有多个并行工作的专家网络。他们对所有专家采用相同的学习计划，忽略了每个专家的学习领域和速度的不同，导致一些专家学习过度，而另一些专家学习不足。这种不匹配会导致专家很难一起训练，从而损害训练效率。为了解决这个问题，我们提出了一个新的训练加速框架。它可以根据每个专家的训练进度，为每个专家定制有效的学习计划，避免在巨大的参数空间中盲目搜索。具体来说，我们首先设计一个多阶段规划器，从优化网络的子部分开始，然后将其扩展以重新训练，直到扩展到整个网络。它利用密度函数来评估专家在每个阶段所获得的知识，优先考虑学习速度更快的专家来增加训练规模，从而促进收敛。然后，我们利用增长算子来添加下一阶段的专家训练规模。在每个阶段，网络都会收敛到一些局部最优值。这可以提供更好的初始化，以便更轻松地训练下一​​阶段，因为从头开始训练所需的时间和数据都大大减少了。为了缓解网络增长引起的梯度消失问题，我们开发了一个调度器来动态调整学习率。进行了大量的实验来验证我们方法的有效性。结果表明，我们平均可以获得超过25%的训练加速。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652831",
        "title": "Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation",
        "link": "https://doi.org/10.1109/tpami.2026.3652831",
        "published": "2026",
        "author": "Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Jose J. Guerrero, Giovanni Maria Farinella, Antonino Furnari",
        "summary": "Short-Term object-interaction Anticipation (STA) consists in detecting the location of the next-active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user's goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1) We propose STAformer and STAformer++, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2) We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to $+23\\%$ on Ego4D and $+31\\%$ on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "集成可供性和注意力模型以实现短期对象交互预期",
        "abstract_cn": "短期对象交互预期（STA）包括检测下一个活动对象的位置、交互的名词和动词类别，以及通过观察自我中心视频进行接触的时间。这种能力对于可穿戴助手了解用户目标并提供及时帮助或实现人机交互至关重要。在这项工作中，我们提出了一种提高 STA 预测性能的方法。我们的贡献有两个方面：1）我们提出了 STAformer 和 STAformer++，这​​是两种基于注意力的新颖架构，集成了帧引导时间池、双图像视频注意力和多尺度特征融合，以支持图像输入视频对的 STA 预测； 2）我们引入了两个新颖的模块，通过对可供性进行建模来对人类行为进行 STA 预测。首先，我们集成了一个环境可供性模型，该模型充当给定物理场景中可能发生的交互的持久记忆。我们探索如何通过简单的后期融合来整合环境可供性，并采用一种自适应学习如何最好地将可供性与端到端预测融合的方法。其次，我们通过观察手和物体轨迹来预测交互热点，从而增加了热点周围 STA 预测的可信度。我们的结果显示整体 Top-5 mAP 显着改善，Ego4D 上的增益高达 $+23\\%$，一组新颖的策划 EPIC-Kitchens STA 标签的增益高达 $+31\\%$。我们在 Ego4D 和 EPIC-Kitchens 上发布了代码、注释和预提取的可供性，以鼓励该领域的未来研究。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652303",
        "title": "NuwaDynamics+: A Causality-Aware Generative Framework for Spatio-Temporal Representation Learning",
        "link": "https://doi.org/10.1109/tpami.2026.3652303",
        "published": "2026",
        "author": "Kun Wang, Yifan Duan, Hao Wu, Jian Zhao, Kai Wang, Zhengyang Zhou, Yuxuan Liang, Xu Wang, Yang Wang, Yu Zheng, Xuelong Li",
        "summary": "Spatio-temporal (ST) prediction is crucial in earth sciences, including meteorological forecasting and urban computing, to name just a few. Access to ample high-quality data, combined with deep models adept at inference, is essential for attaining significant outcomes. Yet, data scarcity and the substantial costs of sensor deployment result in notable data imbalances. Overly specialized models that lack causal linkages further undermine the generalizability of inference techniques. To address these challenges, we first introduce a causal framework for ST predictions, named NuwaDynamics, aimed at pinpointing causal regions in data and providing models with the capability for causal reasoning in a dual-phase process. Initially, we employ upstream self-supervision to identify causally significant patches, equipping the model with generalizable insights and performing targeted interventions on non-essential patches to approximate potential testing distributions. This stage is known as the discovery phase. Progressing from discovery, we apply the insights to downstream tasks tailored to specific ST goals, enhancing the model's recognition of a wider potential data distribution and augmenting its causal perceptual abilities (referred to as the Update phase). Additionally, we address environmental controllability and high computational complexity by implementing channel multiplication and conditional generation methods. This process, termed NuwaDynamics+, can further be interpreted as the front-door adjustment technique in the causality domain. Through comprehensive experiments across ten real-world or simulated ST benchmarks, we demonstrate that integrating the NuwaDynamics+ concept substantially improves various model performance. NuwaDynamics+ concept also significantly enhances the versatility across various dynamic ST tasks, such as extreme weather forecasting and long-temporal-step super-resolution predictions.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "NuwaDynamics+：用于时空表示学习的因果感知生成框架",
        "abstract_cn": "时空（ST）预测在地球科学中至关重要，包括气象预报和城市计算等。获取充足的高质量数据，结合擅长推理的深度模型，对于取得重大成果至关重要。然而，数据稀缺和传感器部署的巨大成本导致了显着的数据不平衡。缺乏因果联系的过于专业化的模型进一步削弱了推理技术的普遍性。为了应对这些挑战，我们首先引入了一个名为 NuwaDynamics 的 ST 预测因果框架，旨在精确定位数据中的因果区域，并为模型提供双阶段过程中因果推理的能力。最初，我们采用上游自我监督来识别具有因果关系的重要补丁，为模型配备可概括的见解，并对非必要补丁进行有针对性的干预，以近似潜在的测试分布。这个阶段被称为发现阶段。从发现开始，我们将见解应用于针对特定 ST 目标定制的下游任务，增强模型对更广泛的潜在数据分布的识别并增强其因果感知能力（称为更新阶段）。此外，我们通过实施通道乘法和条件生成方法来解决环境可控性和高计算复杂性问题。这个过程被称为 NuwaDynamics+，可以进一步解释为因果关系领域的前门调整技术。通过十个真实世界或模拟 ST 基准的综合实验，我们证明集成 NuwaDynamics+ 概念可显着提高各种模型性能。 NuwaDynamics+ 概念还显着增强了各种动态 ST 任务的多功能性，例如极端天气预报和长时步超分辨率预测。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3652860",
        "title": "Seeing through Satellite Images at Street Views",
        "link": "https://doi.org/10.1109/tpami.2026.3652860",
        "published": "2026",
        "author": "Ming Qian, Bin Tan, Qiuyu Wang, Xianwei Zheng, Hanjiang Xiong, Gui-Song Xia, Yujun Shen, Nan Xue",
        "summary": "This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://qianmingduowan.github.io/sat2density-pp/</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过卫星图像查看街景",
        "abstract_cn": "本文研究了卫星街景合成的任务，其目的是在给定卫星图像和指定的相机位置或轨迹的情况下渲染逼真的街景全景图像和视频。我们的方法涉及从卫星和街道视点捕获的配对图像中学习卫星图像条件神经辐射场，由于卫星和街景图像之间的稀疏视图性质以及极大的视点变化，这成为一个具有挑战性的学习问题。我们基于特定任务的观察来应对挑战，即街景特定元素（包括天空和照明效果）仅在街景全景图中可见，并提出了一种新颖的方法 Sat2Density++，通过在神经网络中对这些街景特定元素进行建模来实现照片级真实街景全景渲染的目标。在实验中，我们的方法在城市和郊区场景数据集上进行了评估，证明 Sat2Density++ 能够渲染逼真的街景全景图，这些全景图在多个视图中保持一致，并且忠实于卫星图像。项目页面位于 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://qianmingduowan.github.io/sat2密度-pp/</uri>。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2025.3616163",
        "title": "CFSM: A Novel Causal Feature Selection Module for Two-Dimensional Out-of-Distribution Generalization",
        "link": "https://doi.org/10.1109/tpami.2025.3616163",
        "published": "2026-02-01",
        "author": "Lin Zhu, Weihan Yin, Yiyao Yang, Yifei Wu, Qinying Gu, Xinbing Wang, Chenghu Zhou, Nanyang Ye",
        "summary": "In real-world scenarios, training and test data are often collected in diverse settings, leading to domain shifts arising from evolving environments and selection bias. While causality-inspired methods have shown promising results in tackling the out-of-distribution (OOD) generalization issue, prior methods treat the discovered differences across domains as confounding variables. While effective in handling domain differences (i.e., unseen environmental features in test data), they may fail when confronted with intricate spurious correlations in real-world datasets. In this study, we first analyze this limitation to inadequate modeling of causal intervention and derive the OOD generalization bound to explain the challenges it introduces. To address this problem, we propose a modified causal intervention approach to mitigate various types of confounders. Motivated by the mathematical formulation of our modified causal intervention, we introduce the Causal Feature Selection Module (CFSM) to suppress model weights on both domain-differences features and spurious correlation features. Integrated within the Base Feature Extraction Module, In-Sample Module, and Cross-Sample Module (B-I-C architecture), CFSM collectively neutralizes the confounding effects arising from both domain discrepancies and correlation distinctions, thereby achieving causal feature selection. Under mild assumptions, we prove that the proposed CFSM method can achieve strictly lower OOD errors. Further experiments conducted on various benchmark datasets demonstrate the effectiveness of the proposed method. Compared to previous deconfounding methods, our method not only mitigates the effect of domain-differences features but also the hard-to-identify spurious correlation features, achieving significant improvements in two-dimensional OOD generalization.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "CFSM：一种用于二维分布外泛化的新型因果特征选择模块",
        "abstract_cn": "在现实场景中，训练和测试数据通常是在不同的环境中收集的，导致不断变化的环境和选择偏差导致领域发生变化。虽然受因果关系启发的方法在解决分布外 (OOD) 泛化问题方面显示出了良好的结果，但先前的方法将发现的跨领域差异视为混杂变量。虽然可以有效处理域差异（即测试数据中看不见的环境特征），但在面对现实数据集中复杂的虚假相关性时，它们可能会失败。在本研究中，我们首先分析了因果干预建模不足的局限性，并得出 OOD 概括来解释它带来的挑战。为了解决这个问题，我们提出了一种改进的因果干预方法来减轻各种类型的混杂因素。在我们修改后的因果干预的数学公式的推动下，我们引入了因果特征选择模块（CFSM）来抑制域差异特征和虚假相关特征上的模型权重。 CFSM 集成在基本特征提取模块、样本内模块和跨样本模块（B-I-C 架构）中，共同抵消了域差异和相关性差异产生的混杂效应，从而实现因果特征选择。在温和的假设下，我们证明所提出的 CFSM 方法可以严格实现较低的 OOD 错误。在各种基准数据集上进行的进一步实验证明了所提出方法的有效性。与之前的去混杂方法相比，我们的方法不仅减轻了域差异特征的影响，而且还减轻了难以识别的虚假相关特征，在二维 OOD 泛化方面取得了显着的改进。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653796",
        "title": "Revisiting 360 Depth Estimation With PanoGabor: A New Fusion Perspective",
        "link": "https://doi.org/10.1109/tpami.2026.3653796",
        "published": "2026",
        "author": "Zhijie Shen, Chunyu Lin, Lang Nie, Kang Liao, Weisi Lin, Yao Zhao",
        "summary": "Depth estimation from a monocular 360 image is important to the perception of the entire 3D environment. However, the inherent distortion and large field of view (FoV) in 360 images pose great challenges for this task. To this end, existing mainstream solutions typically introduce additional perspective-based 360 representations (<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i>, Cubemap) to achieve effective feature extraction. Nevertheless, regardless of the introduced representations, they eventually need to be unified into the equirectangular projection (ERP) format for the subsequent depth estimation, which inevitably reintroduces additional distortions. In this work, we propose an oriented-distortion-aware Gabor Fusion framework (PGFuse) to address the above challenges. First, we introduce Gabor filters that analyze texture in the frequency domain, extending the receptive fields and enhancing depth cues. To address the reintroduced distortions, we design a latitude-aware distortion representation to generate customized, distortion-aware Gabor filters (PanoGabor filters). Furthermore, we design a channel- wise and spatial- wise unidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor filters to unify other representations into the ERP format, delivering effective and distortion-aware features. Considering the orientation sensitivity of the Gabor transform, we further introduce a spherical gradient constraint to stabilize this sensitivity. Experimental results on three popular indoor 360 benchmarks demonstrate the superiority of the proposed PGFuse to existing state-of-the-art solutions. Code and models will be available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/zhijieshen-bjtu/PGFuse</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "使用 PanoGabor 重新审视 360 度深度估计：新的融合视角",
        "abstract_cn": "单目 360 度图像的深度估计对于整个 3D 环境的感知非常重要。然而，360 度图像固有的畸变和大视场（FoV）给这项任务带来了巨大的挑战。为此，现有主流解决方案通常引入额外的基于透视的 360 度表示（<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">例如</i>，Cubemap）以实现有效的特征提取。然而，无论引入何种表示形式，它们最终都需要统一为等距柱状投影（ERP）格式以进行后续的深度估计，这不可避免地会重新引入额外的失真。在这项工作中，我们提出了一种定向扭曲感知的 Gabor Fusion 框架（PGFuse）来解决上述挑战。首先，我们介绍 Gabor 滤波器，它可以分析频域中的纹理，扩展感受野并增强深度线索。为了解决重新引入的失真问题，我们设计了一种纬度感知失真表示来生成定制的失真感知 Gabor 滤波器（PanoGabor 滤波器）。此外，我们设计了一个通道和空间单向融合模块（CS-UFM），它集成了所提出的 PanoGabor 滤波器，将其他表示形式统一到 ERP 格式中，提供有效和失真感知的特征。考虑到Gabor变换的方向敏感性，我们进一步引入球形梯度约束来稳定这种敏感性。三个流行的室内 360 度基准测试的实验结果证明了所提出的 PGFuse 相对于现有最先进解决方案的优越性。代码和模型可在 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/zhijieshen-bjtu/PGFuse</uri> 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653765",
        "title": "An Efficient Multi-Estimation-Based Parameter Centroid Decision Via Linear Regression Approach",
        "link": "https://doi.org/10.1109/tpami.2026.3653765",
        "published": "2026",
        "author": "Yeongyu Choi, Fabien Moutarde, Ju H. Park, Ho-Youl Jung",
        "summary": "We propose a novel post-processing approach for the local optimization of Locally Optimized RANdom SAmple Consensus (LO-RANSAC), called the Multi-Estimation-based Parameter Centroid (MEPC) decision. It is observed that the optimal thresholds for hypothesis generation and evaluation differ in local optimization with the inner RANSAC. Instead of binary labeling for inliers and outliers, a new ternary labeling for inliers, midliers, and outliers is introduced, using two thresholds. Our experimental results show that the highest-scoring model measured by the ternary method is closer to the real model than that measured by the existing binary method. However, it should be noted that the highest score still does not correspond to the best model due to inaccurate evaluation by data noise. We introduce a new linear model centroid decision method to compensate for the highest-scoring model distorted by noise. In this process, an efficient method for measuring the similarity between two hypotheses is introduced, and candidates close to the real model are found by comparing their similarity with the highest-scoring model. Our approach determines a representative model of the multiple candidate hypotheses, which is defined as the geometric centroid of hyperplanes. We test on various datasets for homography, fundamental, and essential matrices, demonstrating that applying MEPC to existing RANSAC algorithms achieves more accurate and stable model estimation. Moreover, additional experiments on vanishing point detection show the potential of our approach for various model estimation applications.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过线性回归方法进行高效的基于多重估计的参数质心决策",
        "abstract_cn": "我们提出了一种新颖的后处理方法，用于局部优化的随机抽样共识（LO-RANSAC）的局部优化，称为基于多重估计的参数质心（MEPC）决策。据观察，假设生成和评估的最佳阈值在局部优化中与内部 RANSAC 不同。引入了使用两个阈值的新的内部值、中间值和异常值的三元标记，而不是内部值和异常值的二元标记。我们的实验结果表明，三元方法测量的最高得分模型比现有二元方法测量的模型更接近真实模型。但需要注意的是，由于数据噪声的评估不准确，最高分数仍然不对应于最佳模型。我们引入了一种新的线性模型质心决策方法来补偿因噪声而扭曲的最高得分模型。在此过程中，引入了一种测量两个假设之间相似性的有效方法，并通过将其与最高得分模型的相似性进行比较来找到接近真实模型的候选模型。我们的方法确定了多个候选假设的代表性模型，该模型被定义为超平面的几何质心。我们在各种数据集上测试了单应矩阵、基本矩阵和本质矩阵，证明将 MEPC 应用于现有的 RANSAC 算法可以实现更准确和稳定的模型估计。此外，关于消失点检测的额外实验显示了我们的方法在各种模型估计应用中的潜力。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653573",
        "title": "AtomThink: Multimodal Slow Thinking With Atomic Step Reasoning",
        "link": "https://doi.org/10.1109/tpami.2026.3653573",
        "published": "2026",
        "author": "Kun Xiang, Zhili Liu, Terry Jingchen Zhang, Yinya Huang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Hanhui Li, Yihan Zeng, Yu-Jie Yuan, Jianhua Han, Lanqing Hong, Hang Xu, Xiaodan Liang",
        "summary": "In this paper, we address the challenging task of multimodal reasoning by incorporating the notion of \"slow thinking\" into multimodal large language models (MLLMs). Our core idea is that models can learn to adaptively use different levels of reasoning to tackle questions of varying complexity. We propose a novel paradigm of Self-structured Chain of Thought (SCoT), which consists of minimal semantic atomic steps. Unlike existing methods that rely on structured templates or free-form paradigms, our method not only generates flexible CoT structures for various complex tasks but also mitigates the phenomenon of overthinking for easier tasks. To introduce structured reasoning into visual cognition, we design a novel AtomThink framework with four key modules: (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning (SFT) process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single-step utilization rate. Extensive experiments demonstrate that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 × and boosts inference efficiency by 85.3%. Our code is publicly available at https://github.com/Kun-Xiang/AtomThink.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "AtomThink：具有原子步骤推理的多模式慢速思维",
        "abstract_cn": "在本文中，我们通过将“慢思维”的概念纳入多模态大语言模型（MLLM）来解决多模态推理的挑战性任务。我们的核心思想是模型可以学习自适应地使用不同级别的推理来解决不同复杂性的问题。我们提出了一种新的自结构化思想链（SCoT）范式，它由最少的语义原子步骤组成。与依赖结构化模板或自由形式范式的现有方法不同，我们的方法不仅为各种复杂任务生成灵活的 CoT 结构，而且还减轻了对简单任务过度思考的现象。为了将结构化推理引入视觉认知，我们设计了一个新颖的 AtomThink 框架，该框架具有四个关键模块：（i）生成高质量多模态推理路径的数据引擎； (ii) 使用序列化推理数据的监督微调（SFT）过程； (iii) 策略引导的多轮推理方法； (iv) 原子能力度量来评估单步利用率。大量实验表明，所提出的 AtomThink 显着提高了基线 MLLM 的性能，在 MathVista 和 MathVerse 上实现了超过 10% 的平均准确度增益。与最先进的结构化 CoT 方法相比，我们的方法不仅实现了更高的精度，而且将数据利用率提高了 5 倍，并将推理效率提高了 85.3%。我们的代码可在 https://github.com/Kun-Xiang/AtomThink 上公开获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653482",
        "title": "Goal-guided Prompting with Adaptive Modality Selection for Efficient Assembly Activity Anticipation in Egocentric Videos",
        "link": "https://doi.org/10.1109/tpami.2026.3653482",
        "published": "2026",
        "author": "Tianshan Liu, Bing-Kun Bao",
        "summary": "With the functions of egocentric observation and multimodal perception equipped in augmented reality (AR) devices, the next generation of smart assistants has the potential to reduce human labor and enhance execution efficiency in assembly tasks. Among diverse assembly activity understanding tasks, anticipating the near future activities is crucial yet challenging, which can assist humans or agents to actively plan and engage in interactions with the environment. However, the existing egocentric activity anticipation methods still struggle to achieve a decent trade-off between accuracy and computational efficiency, hindering them to be deployed in practical applications. To address this dilemma, in this paper, we propose a goal-guided prompting framework with adaptive modality selection (GP-AMS), for assembly activity anticipation in egocentric videos. For bridging the semantic gap between the historical observations and unobserved future activities, we inject the inferred high-level goal clues into the constructed prompts, which are further utilized to guide a pre-trained vision-language (V-L) model to compensate relevant semantics of unseen future. Moreover, a mask-and-predict strategy is adopted with two imposed constraints, i.e., casual masking and probabilistic token-dropping, to mine the intrinsic associations between the assembly activities within a specific procedure. For maintaining the benefits of exploiting multimodal information while avoiding extensively increasing the computational burdens, an adaptive modality selection strategy is designed to train a policy network, which learns to dynamically decide which modalities should be sampled for processing by the anticipation model on a per observation time-step basis. By allocating major computation to the selected indicative modalities on-the-fly, the efficiency of the overall model can be improved, thus paving the way for feasibility on real-world devices. Extensive experimental results on two public data sets validate that the proposed method yields not only consistent improvements in anticipation accuracy, but also significant savings in computation budgets.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过自适应模态选择进行目标引导提示，以实现以自我为中心的视频中的高效装配活动预期",
        "abstract_cn": "借助增强现实（AR）设备中配备的以自我为中心的观察和多模态感知功能，下一代智能助手有潜力减少人力并提高装配任务的执行效率。在各种装配活动理解任务中，预测不久的将来的活动至关重要但具有挑战性，它可以帮助人类或智能体积极计划并参与与环境的交互。然而，现有的以自我为中心的活动预测方法仍然难以在准确性和计算效率之间实现良好的权衡，阻碍了它们在实际应用中的部署。为了解决这个困境，在本文中，我们提出了一种具有自适应模态选择的目标引导提示框架（GP-AMS），用于自我中心视频中的装配活动预期。为了弥合历史观察和未观察到的未来活动之间的语义差距，我们将推断的高级目标线索注入到构建的提示中，进一步利用这些线索来指导预先训练的视觉语言（V-L）模型，以补偿未预见的未来的相关语义。此外，采用掩码和预测策略，并施加两个约束，即随意掩码和概率令牌丢弃，以挖掘特定程序内组装活动之间的内在关联。为了保持利用多模态信息的好处，同时避免大量增加计算负担，设计了一种自适应模态选择策略来训练策略网络，该网络学习动态地决定应在每个观察时间步长的基础上对预期模型进行采样以供处理的模态。通过将主要计算动态分配给选定的指示模态，可以提高整体模型的效率，从而为现实设备上的可行性铺平道路。对两个公共数据集的大量实验结果验证了所提出的方法不仅可以持续提高预测精度，而且还可以显着节省计算预算。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653768",
        "title": "Learn to Enhance Sparse Spike Streams",
        "link": "https://doi.org/10.1109/tpami.2026.3653768",
        "published": "2026",
        "author": "Liwen Hu, Yijia Guo, Mianzhi Liu, Yiming Fan, Rui Ma, Shengbo Chen, Lei Ma, Tiejun Huang",
        "summary": "High-speed vision tasks have long been a challenge in computer vision. Recently, the spike camera has shown great potential in these tasks due to its high temporal resolution. Unlike traditional cameras, it emits asynchronous spike signals to capture visual information. However, under low-light conditions, spike signals becomehighly sparse, and the sparse spike streamseverely hinders theeffectiveness of existing spike-based methods in high-speed scenarios. To address this challenge,we introduce SS2DS, the first deep learning framework that enhances sparse spike streams into dense spike streams. SS2DS first estimates the spike firing frequency within sparse streams. Subsequently, the spike firing frequency is enhanced by a neural network. Finally, SS2DS decodes the enhanced spike stream from the enhanced spike firing frequency sequence. SS2DS can adjust the temporal distribution of sparse spike streams and improve the performance degradation of existing methods in low-light and high-speed scenarios. In order to evaluate sparse spikestream enhancement,we construct both synthetic and real sparse spike stream datasets. The real dataset iscollected in dynamic scenarios using the third-generation spike camera.By comparing the reconstruction results, enhanced spike streams achieve an average improvement of +0.78 MA, -18.42 BRISQUE, and -1.42 NIQE over sparse spike streams. Moreover, the enhanced spike streams also benefit other spike-based vision tasks, such as 3D reconstruction (+1.325 dB PSNR, +0.005 SSIM, and -0.01 LPIPS) and super-resolution (+0.63 MA, -13.67 BRISQUE, and -1.28 NIQE). Code and datasets will be released after publication.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "学习增强稀疏尖峰流",
        "abstract_cn": "高速视觉任务长期以来一直是计算机视觉领域的一个挑战。最近，尖峰相机由于其高时间分辨率而在这些任务中显示出巨大的潜力。与传统相机不同，它发出异步尖峰信号来捕获视觉信息。然而，在弱光条件下，尖峰信号变得高度稀疏，稀疏的尖峰流严重阻碍了现有基于尖峰的方法在高速场景下的有效性。为了应对这一挑战，我们引入了 SS2DS，这是第一个将稀疏尖峰流增强为密集尖峰流的深度学习框架。 SS2DS 首先估计稀疏流内的尖峰发射频率。随后，神经网络增强了尖峰发射频率。最后，SS2DS 从增强型尖峰放电频率序列中解码增强型尖峰流。 SS2DS可以调整稀疏尖峰流的时间分布，改善现有方法在弱光和高速场景下的性能下降。为了评估稀疏尖峰流增强，我们构建了合成的和真实的稀疏尖峰流数据集。使用第三代尖峰相机在动态场景下采集真实数据集。通过比较重建结果，增强尖峰流比稀疏尖峰流平均提高了+0.78 MA、-18.42 BRISQUE 和-1.42 NIQE。此外，增强的尖峰流还有益于其他基于尖峰的视觉任务，例如 3D 重建（+1.325 dB PSNR、+0.005 SSIM 和 -0.01 LPIPS）和超分辨率（+0.63 MA、-13.67 BRISQUE 和 -1.28 NIQE）。代码和数据集将在发布后发布。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653866",
        "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving",
        "link": "https://doi.org/10.1109/tpami.2026.3653866",
        "published": "2026",
        "author": "Haochen Liu, Tianyu Li, Haohan Yang, Li Chen, Caojun Wang, Ke Guo, Haochen Tian, Hongchen Li, Hongyang Li, Chen Lv",
        "summary": "End-to-end autonomous driving has emerged as a promising paradigm for directly mapping sensor inputs to planning maneuvers using learning-based modular integrations. However, existing imitation learning (IL)-based models suffer from generalization to hard cases, and a lack of corrective feedback loop under post-deployment. While reinforcement learning (RL) offers a potential solution to tackle hard cases with optimality, it is often hindered by overfitting to specific driving cases, resulting in catastrophic forgetting of generalizable knowledge and sample inefficiency. To overcome these challenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE), a novel learning pipeline that constantly refines hard domain while keeping generalizable driving policy for model-agnostic end-to-end driving systems. Through reinforcement fine-tuning and policy expansion that facilitates continuous improvement, R2SE features three key components: 1) Generalist Pretraining with hard-case allocation trains a generalist imitation learning (IL) driving system while dynamically identifying failure-prone cases for targeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes residual corrections using reinforcement learning (RL) to improve performance in hard case domain while preserving global driving knowledge; 3) Self-aware Adapter Expansion dynamically integrates specialist policies back into the generalist model, enhancing continuous performance improvement. Experimental results in closed-loop simulation and real-world datasets demonstrate improvements in generalization, safety, and long-horizon policy robustness over state-of-the-art E2E systems, highlighting the effectiveness of reinforce refinement for scalable autonomous driving.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "加强精细化和自我感知扩展，实现端到端自动驾驶",
        "abstract_cn": "端到端自动驾驶已成为一种有前途的范例，可使用基于学习的模块化集成将传感器输入直接映射到规划操作。然而，现有的基于模仿学习（IL）的模型存在对困难情况的泛化，以及部署后缺乏纠正反馈循环的问题。虽然强化学习 (RL) 提供了一种潜在的解决方案来以最优的方式处理困难情况，但它常常会因对特定驾驶情况的过度拟合而受到阻碍，从而导致通用知识的灾难性遗忘和样本效率低下。为了克服这些挑战，我们提出了具有自我意识扩展的强化细化（R2SE），这是一种新颖的学习管道，可以不断细化硬域，同时为与模型无关的端到端驾驶系统保持通用的驾驶策略。通过强化微调和策略扩展促进持续改进，R2SE 具有三个关键组成部分：1）具有硬案例分配的通用预训练训练通用模仿学习（IL）驱动系统，同时动态识别容易失败的案例以进行有针对性的改进； 2) Residual Reinforced Specialist Fine-tuning 使用强化学习（RL）优化残差校正，以提高硬案例领域的性能，同时保留全局驾驶知识； 3) 自我感知适配器扩展将专业策略动态集成回通才模型中，从而增强持续的性能改进。闭环仿真和现实数据集的实验结果表明，与最先进的 E2E 系统相比，在泛化性、安全性和长期政策鲁棒性方面都有所提高，凸显了强化细化可扩展自动驾驶的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654274",
        "title": "Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective",
        "link": "https://doi.org/10.1109/tpami.2026.3654274",
        "published": "2026",
        "author": "Nan Zhong, Mian Zou, Yiran Xu, Zhenxing Qian, Xinpeng Zhang, Baoyuan Wu, Kede Ma",
        "summary": "The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata-specifically exchangeable image file format (EXIF) tags-to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (e.g., camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (e.g., focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "自监督人工智能生成的图像检测：相机元数据视角",
        "abstract_cn": "人工智能生成图像的激增给多媒体取证带来了不断升级的挑战，但许多现有检测器依赖于对特定生成模型内部结构的假设，限制了它们的跨模型适用性。我们引入了一种自我监督的方法来检测人工智能生成的图像，该方法利用相机元数据（特别是可交换图像文件格式（EXIF）标签）来学习数字摄影的固有特征。我们的借口任务通过对分类 EXIF 标签（例如，相机型号和场景类型）和成对排序有序和连续 EXIF 标签（例如，焦距和光圈值）进行分类，仅在相机拍摄的照片上训练特征提取器。使用这些 EXIF 诱导的特征，我们首先通过使用高斯混合模型对摄影图像的分布进行建模并将低似然样本标记为 AI 生成来执行一类检测。然后，我们扩展到二进制检测，将学习的提取器视为相同架构的分类器的强正则化器，对空间置乱补丁的高频残差进行操作。跨各种生成模型的大量实验表明，我们的 EXIF 诱导检测器极大地推进了现有技术的发展，为野外样本提供了强大的泛化能力，并对常见的良性图像扰动具有鲁棒性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654092",
        "title": "SLeak: Multi-Target Privacy Stealing Attack against Split Learning",
        "link": "https://doi.org/10.1109/tpami.2026.3654092",
        "published": "2026",
        "author": "Xiaoyang Xu, Wenzhe Yi, Juan Wang, Hongxin Hu, Mengda Yang, Ziang Li, Yong Zhuang, Yaxin Liu, Mang Ye",
        "summary": "Split Learning (SL) is a distributed learning framework that has gained popularity for its privacy-preserving nature and low computational demands. However, recent studies have the potential that a server adversary to carry out inference attacks, compromising the privacy of victim clients. Nevertheless, upon re-evaluating prior studies, we found that existing methods rely on overly strong assumptions to enhance their performance, resulting in a significant decline in effectiveness under more realistic scenarios. In this work, we provide new insights into the inherent vulnerabilities of SL. Specifically, we discover that both the smashed data and the server model contain the client's representation preference, which the server adversary can exploit to build a substitute client that approximates the target client's unique feature extraction behavior. With a well-trained substitute client, the server can perfectly steal the target client's functionality, training data, and labels. Building on this observation, we introduce Split Leakage (SLeak), a new threat that targets multiple privacy stealing objectives against SL. Notably, SLeak does not depend on strong privacy priors and only requires partial same-domain auxiliary public data to conduct the attacks. Experimental results on diverse datasets and target models show that SLeak surpasses the state-of-the-art method across multiple metrics. Moreover, ablation studies further confirm its robustness and applicability under various scenarios and assumptions.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "SLeak：针对分裂学习的多目标隐私窃取攻击",
        "abstract_cn": "分割学习（SL）是一种分布式学习框架，因其隐私保护性质和低计算需求而受到欢迎。然而，最近的研究表明服务器对手有可能进行推理攻击，从而损害受害者客户端的隐私。然而，在重新评估之前的研究后，我们发现现有方法依赖过于强烈的假设来提高其性能，导致在更现实的场景下有效性显着下降。在这项工作中，我们对 SL 的固有漏洞提供了新的见解。具体来说，我们发现粉碎的数据和服务器模型都包含客户端的表示偏好，服务器对手可以利用它来构建近似目标客户端独特特征提取行为的替代客户端。通过训练有素的替代客户端，服务器可以完美窃取目标客户端的功能、训练数据和标签。基于这一观察，我们引入了 Split Leakage (SLeak)，这是一种针对 SL 的多个隐私窃取目标的新威胁。值得注意的是，SLeak 不依赖于强隐私先验，只需要部分同域辅助公共数据即可进行攻击。在不同数据集和目标模型上的实验结果表明，SLeak 在多个指标上都超越了最先进的方法。此外，消融研究进一步证实了其在各种场景和假设下的稳健性和适用性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653901",
        "title": "VRP-UDF: Towards Unbiased Learning of Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors",
        "link": "https://doi.org/10.1109/tpami.2026.3653901",
        "published": "2026",
        "author": "Wenyuan Zhang, Chunsheng Wang, Kanle Shi, Yu-Shen Liu, Zhizhong Han",
        "summary": "Unsigned distance functions (UDFs) have been a vital representation for open surfaces. With different differentiable renderers, current methods are able to train neural networks to infer a UDF by minimizing the rendering errors with the UDF to the multi-view ground truth. However, these differentiable renderers are mainly handcrafted, which makes them either biased on ray-surface intersections, or sensitive to unsigned distance outliers, or not scalable to large scenes. To resolve these issues, we present a novel differentiable renderer to infer UDFs more accurately. Instead of using handcrafted equations, our differentiable renderer is a neural network which is pre-trained in a data-driven manner. It learns how to render unsigned distances into depth images, leading to a prior knowledge, dubbed volume rendering priors. To infer a UDF for an unseen scene from multiple RGB images, we generalize the learned volume rendering priors to map inferred unsigned distances in alpha blending for RGB image rendering. To reduce the bias of sampling in UDF inference, we utilize an auxiliary point sampling prior as an indicator of ray-surface intersection, and propose novel schemes towards more accurate and uniform sampling near the zero-level sets. We also propose a new strategy that leverages our pretrained volume rendering prior to serve as a general surface refiner, which can be integrated with various Gaussian reconstruction methods to optimize the Gaussian distributions and refine geometric details. Our results show that the learned volume rendering prior is unbiased, robust, scalable, 3D aware, and more importantly, easy to learn. Further experiments show that the volume rendering prior is also a general strategy to enhance other neural implicit representations such as signed distance function and occupancy. We evaluate our method on both widely used benchmarks and real scenes, and report superior performance over the state-of-the-art methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "VRP-UDF：利用体积渲染先验从多视图图像中无偏学习无符号距离函数",
        "abstract_cn": "无符号距离函数 (UDF) 是开放曲面的重要表示形式。使用不同的可微渲染器，当前的方法能够训练神经网络通过最小化 UDF 到多视图地面实况的渲染错误来推断 UDF。然而，这些可微渲染器主要是手工制作的，这使得它们要么对光线表面相交有偏差，要么对无符号距离异常值敏感，要么无法扩展到大场景。为了解决这些问题，我们提出了一种新颖的可微渲染器来更准确地推断 UDF。我们的可微渲染器不是使用手工方程，而是一个以数据驱动方式预先训练的神经网络。它学习如何将无符号距离渲染为深度图像，从而获得先验知识，称为体积渲染先验。为了从多个 RGB 图像推断出未见过的场景的 UDF，我们将学习到的体积渲染先验推广到在 RGB 图像渲染的 alpha 混合中映射推断的无符号距离。为了减少 UDF 推理中采样的偏差，我们利用辅助点采样先验作为射线表面相交的指标，并提出了在零水平集附近更准确和均匀采样的新方案。我们还提出了一种新策略，利用我们预先训练的体渲染作为通用表面细化器，它可以与各种高斯重建方法集成，以优化高斯分布并细化几何细节。我们的结果表明，学习到的体渲染先验是无偏的、稳健的、可扩展的、3D 感知的，更重要的是，易于学习。进一步的实验表明，体积渲染先验也是增强其他神经隐式表示（例如符号距离函数和占用率）的通用策略。我们在广泛使用的基准和真实场景上评估我们的方法，并报告其优于最先进方法的性能。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654243",
        "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation",
        "link": "https://doi.org/10.1109/tpami.2026.3654243",
        "published": "2026",
        "author": "Hao Wang, Keyan Hu, Xin Guo, Haifeng Li, Chao Tao",
        "summary": "Remote sensing semantic segmentation must address both what the ground objects are within an image and where they are located. Consequently, segmentation models must ensure not only the semantic correctness of large-scale patches (low-frequency information) but also the precise localization of boundaries between patches (high-frequency information related to boundary components). However, most existing approaches rely heavily on discriminative learning, which excels at capturing low-frequency features, while overlooking its inherent limitations in learning high-frequency features for semantic segmentation. Recent studies have revealed that diffusion generative models excel at generating high-frequency details. Our theoretical analysis confirms that the diffusion denoising process significantly enhances the model's ability to learn high-frequency features; however, we also observe that these models exhibit insufficient semantic inference for low-frequency features when guided solely by the original image. Therefore, we integrate the strengths of both discriminative and generative learning, proposing the Integration of Discriminative and diffusion-based Generative learning for Boundary Refinement (IDGBR) framework. The framework first generates a coarse segmentation map using a discriminative backbone model. This map and the original image are fed into a conditioning guidance network to jointly learn a guidance representation subsequently leveraged by an iterative denoising diffusion process refining the coarse segmentation. Extensive experiments across five remote sensing semantic segmentation datasets (binary and multi-class segmentation) confirm our framework's capability of consistent boundary refinement for coarse results from diverse discriminative architectures. The source code is available at https://github.com/KeyanHu-git/IDGBR.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "区分性和基于扩散的生成学习整合的礼物：边界细化遥感语义分割",
        "abstract_cn": "遥感语义分割必须解决图像中地面物体的含义及其所在位置。因此，分割模型不仅要保证大规模斑块（低频信息）的语义正确性，还要保证斑块之间边界（与边界成分相关的高频信息）的精确定位。然而，大多数现有方法严重依赖判别式学习，它擅长捕获低频特征，而忽视了其在学习高频特征以进行语义分割方面的固有局限性。最近的研究表明，扩散生成模型擅长生成高频细节。我们的理论分析证实，扩散去噪过程显着增强了模型学习高频特征的能力；然而，我们还观察到，当仅由原始图像引导时，这些模型对低频特征表现出不足的语义推断。因此，我们整合了判别学习和生成学习的优点，提出了基于判别学习和扩散生成学习的边界细化集成（IDGBR）框架。该框架首先使用判别性骨干模型生成粗分割图。该图和原始图像被输入条件引导网络，共同学习引导表示，随后通过迭代去噪扩散过程细化粗分割。跨五个遥感语义分割数据集（二元和多类分割）的广泛实验证实了我们的框架能够对来自不同判别架构的粗略结果进行一致的边界细化。源代码可在 https://github.com/KeyanHu-git/IDGBR 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653989",
        "title": "Consistency-Aware Spot-Guided Transformer for Accurate and Versatile Point Cloud Registration",
        "link": "https://doi.org/10.1109/tpami.2026.3653989",
        "published": "2026",
        "author": "Renlang Huang, Li Chai, Yufan Tang, Zhoujian Li, Jiming Chen, Liang Li",
        "summary": "Deep learning-based feature matching has showcased great superiority for point cloud registration. While coarse-to-fine matching architectures are prevalent, they typically perform sparse and geometrically inconsistent coarse matching. This forces the subsequent fine matching to rely on computationally expensive optimal transport and hypothesis-and-selection procedures to resolve inconsistencies, leading to inefficiency and poor scalability for large-scale real-time applications. In this paper, we design a consistency-aware spot-guided Transformer (CAST) to enhance the coarse matching by explicitly utilizing geometric consistency via two key sparse attention mechanisms. First, our consistency-aware self-attention selectively computes intra-point-cloud attention to a sparse subset of points with globally consistent correspondences, enabling other points to derive discriminative features through their relationships with these anchors while propagating global consistency for robust correspondence reasoning. Second, our spot-guided cross-attention restricts cross-point-cloud attention to dynamically defined \"spots\"-the union of correspondence neighborhoods of a query's neighbors in the other point cloud, which are most likely to cover the true correspondence of the query ensured by local consistency, eliminating interference from similar but irrelevant regions. Furthermore, we design a lightweight local attention-based fine matching module to precisely predict dense correspondences and estimate the transformation. Extensive experiments on both outdoor LiDAR datasets and indoor RGB-D camera datasets demonstrate that our method achieves state-of-the-art accuracy, efficiency, and robustness. Besides, our method showcases superior generalization ability on our newly constructed challenging relocalization and loop closing benchmarks in unseen domains. Our code and models are available at https://github.com/RenlangHuang/CASTv2.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "一致性感知点引导变压器可实现准确且多功能的点云配准",
        "abstract_cn": "基于深度学习的特征匹配在点云配准方面展现出了巨大的优越性。虽然粗到精匹配架构很普遍，但它们通常执行稀疏且几何不一致的粗匹配。这迫使后续的精细匹配依赖于计算成本高昂的最优传输和假设与选择程序来解决不一致问题，从而导致大规模实时应用程序效率低下且可扩展性差。在本文中，我们设计了一种一致性感知点引导变换器（CAST），通过两个关键的稀疏注意机制显式地利用几何一致性来增强粗匹配。首先，我们的一致性感知自注意力选择性地计算对具有全局一致对应关系的稀疏点子集的点云内注意力，使其他点能够通过它们与这些锚点的关系导出判别性特征，同时传播全局一致性以实现稳健的对应推理。其次，我们的点引导交叉注意力将跨点云注意力限制在动态定义的“点”上，即另一个点云中查询邻居的对应邻域的并集，这些点最有可能覆盖由局部一致性确保的查询的真实对应关系，从而消除来自相似但不相关区域的干扰。此外，我们设计了一个轻量级的基于局部注意力的精细匹配模块来精确预测密集对应并估计变换。对室外 LiDAR 数据集和室内 RGB-D 相机数据集的大量实验表明，我们的方法实现了最先进的准确性、效率和鲁棒性。此外，我们的方法在我们新构建的具有挑战性的重定位和未见过的领域的循环关闭基准上展示了卓越的泛化能力。我们的代码和模型可在 https://github.com/RenlangHuang/CASTv2 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654260",
        "title": "Deep Orientational Representation Learning for Ordinal Regression",
        "link": "https://doi.org/10.1109/tpami.2026.3654260",
        "published": "2026",
        "author": "Gengyun Jia, Xin Ma, Bing-Kun Bao",
        "summary": "Ordinal regression aims to predict ordered classes. Existing methods mainly focus on label distribution shapes and feature distance relationships, while the directional characteristics in the representation space remain underexplored. In this paper, we propose deep orientational representation learning (ORL), aiming to ensure the trajectory of features sequentially connected by ordinal categories approximates a geodesic. We treat the output layer weights as ordinal prototypes and introduce two constraints, the co-directional constraint and the counter-directional constraint. They operate by constraining the angles between pairs of vectors. The former minimizes the angle between vectors with matching start and end categories, while the latter maximizes the angle between vectors whose start categories are the same but whose end categories are on opposite sides. The two constraints optimize the representation from different ordinal directions. ORL is extended to a multi-prototype setting (MORL) to mitigate misalignment between features and oriented prototypes caused by large intra-class variations. Theoretical analysis links ORL to distribution unimodality and distance orderliness, highlighting its advantages. The effectiveness of ORL (MORL) is demonstrated on various tasks including facial age estimation, historical image dating, and aesthetic quality assessment.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "序数回归的深度方向表示学习",
        "abstract_cn": "序数回归旨在预测有序类。现有方法主要关注标签分布形状和特征距离关系，而表示空间中的方向特征仍未得到充分探索。在本文中，我们提出了深度方向表示学习（ORL），旨在确保按序数类别顺序连接的特征的轨迹近似测地线。我们将输出层权重视为序数原型，并引入两个约束：同向约束和反向约束。它们通过限制向量对之间的角度来进行操作。前者最小化起始类别和结束类别匹配的向量之间的角度，而后者则最大化起始类别相同但结束类别位于相反侧的向量之间的角度。这两个约束从不同的顺序方向优化表示。 ORL 扩展到多原型设置 (MORL)，以减轻由于类内较大变化而导致的特征和定向原型之间的错位。理论分析将ORL与分布单峰性和距离有序性联系起来，凸显了其优势。 ORL (MORL) 的有效性在各种任务中得到了证明，包括面部年龄估计、历史图像约会和审美质量评估。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654093",
        "title": "BlindU: Blind Machine Unlearning without Revealing Erasing Data",
        "link": "https://doi.org/10.1109/tpami.2026.3654093",
        "published": "2026",
        "author": "Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Shui Yu",
        "summary": "Machine unlearning enables data holders to remove the contribution of their specified samples from trained models to protect their privacy. However, it is paradoxical that most unlearning methods require the unlearning requesters to firstly upload their data to the server as a prerequisite for unlearning. These methods are infeasible in many privacy-preserving scenarios where servers are prohibited from accessing users' data, such as federated learning (FL). In this paper, we explore how to implement unlearning under the condition of not uncovering the erasing data to the server. We propose Blind Unlearning (BlindU), which carries out unlearning using compressed representations instead of original inputs. BlindU only involves the server and the unlearning user: the user locally generates privacy-preserving representations, and the server performs unlearning solely on these representations and their labels. For the FL model training, we employ the information bottleneck (IB) mechanism. The encoder of the IB-based FL model learns representations that distort maximum task-irrelevant information from inputs, allowing FL users to generate compressed representations locally. For effective unlearning using compressed representation, BlindU integrates two dedicated unlearning modules tailored explicitly for IB-based models and uses a multiple gradient descent algorithm to balance forgetting and utility retaining. While IB compression already provides protection for task-irrelevant information of inputs, to further enhance the privacy protection, we introduce a noise-free differential privacy (DP) masking method to deal with the raw erasing data before compressing. Theoretical analysis and extensive experimental results illustrate the superiority of BlindU in privacy protection and unlearning effectiveness compared with the best existing privacy-preserving unlearning benchmarks.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "BlindU：在不泄露数据的情况下盲目机器取消学习",
        "abstract_cn": "机器取消学习使数据持有者能够从训练模型中删除其指定样本的贡献，以保护他们的隐私。然而，矛盾的是，大多数取消学习方法都要求取消学习请求者首先将其数据上传到服务器，作为取消学习的先决条件。这些方法在许多禁止服务器访问用户数据的隐私保护场景中是不可行的，例如联邦学习（FL）。本文探讨的是如何在不向服务器暴露擦除数据的情况下实现取消学习。我们提出了盲解学习（BlindU），它使用压缩表示而不是原始输入来进行解学习。 BlindU 仅涉及服务器和取消学习的用户：用户在本地生成隐私保护表示，服务器仅对这些表示及其标签执行取消学习。对于 FL 模型训练，我们采用信息瓶颈（IB）机制。基于 IB 的 FL 模型的编码器学习表示，这些表示会扭曲来自输入的最大与任务无关的信息，从而允许 FL 用户在本地生成压缩表示。为了使用压缩表示进行有效的遗忘，BlindU 集成了两个专门为基于 IB 的模型量身定制的专用遗忘模块，并使用多重梯度下降算法来平衡遗忘和效用保留。虽然IB压缩已经为输入的与任务无关的信息提供了保护，但为了进一步增强隐私保护，我们引入了一种无噪声差分隐私（DP）屏蔽方法来在压缩之前处理原始擦除数据。理论分析和广泛的实验结果表明，与现有最好的隐私保护遗忘基准相比，BlindU 在隐私保护和遗忘有效性方面具有优越性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654352",
        "title": "Towards Enhanced Representation Learning for Single-Source Domain Generalization in LiDAR Semantic Segmentation",
        "link": "https://doi.org/10.1109/tpami.2026.3654352",
        "published": "2026",
        "author": "Hyeonseong Kim, Yoonsu Kang, Changgyoon Oh, Kuk-Jin Yoon",
        "summary": "With the success of the 3D deep learning models, various perception technologies for autonomous driving have been developed in the LiDAR domain. While these models perform well in the trained source domain, they struggle in unseen domains with a domain gap. In this paper, we propose a representation learning approach for domain generalization in LiDAR semantic segmentation, termed DGLSS++, which is designed to ensure robust performance in both the source domain and unseen domains despite training exclusively on the source domain. Our approach focuses on generalizing from a single source domain, addressing the domain shift caused by variations in LiDAR sensor configurations and scene distributions. To tackle both sparse-to-dense and dense-to-sparse generalization scenarios, we simulate unseen domains by generating sparsely and densely augmented domains. With the augmented domain, we introduce two constraints for generalizable representation learning: generalized masked sparsity invariant feature consistency (GMSIFC) and localized semantic correlation consistency (LSCC). GMSIFC aligns the internal sparse features of the source domain with those of the augmented domain at different sparsity, introducing a novel masking strategy to exclude voxel features associated with multiple inconsistent classes. For LSCC, class prototypes from spatially local regions are constrained to maintain similar correlations across all local regions, regardless of the scene or domain. In addition, we establish standardized training and evaluation protocols utilizing four real-world datasets and implement several baseline methods. Extensive experiments demonstrate our approach outperforms both UDA and DG baselines. The code is available at https://github.com/gzgzys9887/DGLSS.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "面向 LiDAR 语义分割中单源域泛化的增强表示学习",
        "abstract_cn": "随着3D深度学习模型的成功，各种自动驾驶感知技术在激光雷达领域得到了发展。虽然这些模型在经过训练的源领域中表现良好，但它们在存在领域差距的未知领域中表现不佳。在本文中，我们提出了一种用于 LiDAR 语义分割中域泛化的表示学习方法，称为 DGLSS++，该方法旨在确保尽管仅在源域上进行训练，但在源域和未见域中都具有鲁棒的性能。我们的方法侧重于从单一源域进行概括，解决由 LiDAR 传感器配置和场景分布变化引起的域转移。为了解决稀疏到稠密和稠密到稀疏的泛化场景，我们通过生成稀疏和稠密的增强域来模拟看不见的域。通过增强域，我们引入了可泛化表示学习的两个约束：广义掩码稀疏不变特征一致性（GMSIFC）和局部语义相关一致性（LSCC）。 GMSIFC 将源域的内部稀疏特征与不同稀疏度的增强域的内部稀疏特征对齐，引入一种新颖的掩蔽策略来排除与多个不一致类相关的体素特征。对于 LSCC，来自空间局部区域的类原型被限制为在所有局部区域之间保持相似的相关性，无论场景或域如何。此外，我们利用四个真实世界数据集建立标准化培训和评估协议，并实施多种基线方法。大量实验证明我们的方法优于 UDA 和 DG 基线。该代码可在 https://github.com/gzgzys9887/DGLSS 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654426",
        "title": "Semantic Contrast for Domain-Robust Underwater Image Quality Assessment",
        "link": "https://doi.org/10.1109/tpami.2026.3654426",
        "published": "2026",
        "author": "Jingchun Zhou, Chunjiang Liu, Qiuping Jiang, Xianping Fu, Junhui Hou, Xuelong Li",
        "summary": "Underwater image quality assessment (UIQA) is hindered by complex degradation and domain shifts across aquatic environments. Existing no-reference IQA methods rely on costly and subjective mean opinion scores (MOS), which limit their generalization to unseen domains. To overcome these challenges, we propose SCUIA, an unsupervised UIQA framework leveraging semantic contrastive learning for quality prediction without human annotations. Specifically, we introduce a vision-language contrastive learning strategy that aligns image features with textual embeddings in a unified semantic space, capturing implicit degradation-quality correlations. We further enhance quality discrimination with a hierarchical contrastive learning mechanism that combines image-specific statistical priors and semantic prompts. A triplet-based inter-group contrastive loss explicitly models relative quality relationships. To tackle cross-domain variations, we develop an unsupervised domain adaptation module that uses local statistical features to guide CLIP fine-tuning to disentangle domain-invariant quality representations from domain-specific noise. This enables zero-shot cross-domain quality prediction without labeled data. Extensive experiments on public UIQA benchmarks demonstrate significant improvements over existing methods, highlighting superior generalization and domain adaptability.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "域鲁棒水下图像质量评估的语义对比",
        "abstract_cn": "水下图像质量评估 (UIQA) 受到水生环境中复杂退化和域转移的阻碍。现有的无参考 IQA 方法依赖于昂贵的主观平均意见分数 (MOS)，这限制了它们对未知领域的推广。为了克服这些挑战，我们提出了 SCUIA，这是一种无监督的 UIQA 框架，利用语义对比学习进行质量预测，无需人工注释。具体来说，我们引入了一种视觉语言对比学习策略，该策略将图像特征与统一语义空间中的文本嵌入对齐，捕获隐式的退化质量相关性。我们通过结合图像特定统计先验和语义提示的分层对比学习机制进一步增强质量辨别。基于三元组的组间对比损失明确地模拟了相对质量关系。为了解决跨域变化，我们开发了一个无监督的域适应模块，该模块使用局部统计特征来指导 CLIP 微调，以将域不变的质量表示与域特定的噪声分开。这使得无需标记数据即可进行零样本跨域质量预测。对公共 UIQA 基准的大量实验证明了对现有方法的显着改进，突出了卓越的泛化性和领域适应性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654544",
        "title": "Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention",
        "link": "https://doi.org/10.1109/tpami.2026.3654544",
        "published": "2026",
        "author": "Junchi Yan, Fangyu Ding, Jiawei Sun, Zhaoping Hu, Yunyi Zhou, Lei Zhu",
        "summary": "Graph invariant learning (GIL) seeks invariant relations between graphs and labels under distribution shifts. Recent works try to extract an invariant subgraph to improve out-of-distribution (OOD) generalization, yet existing approaches either lack explicit control over compactness or rely on hard top-$k$ selection that shrinks the solution space and is only partially differentiable. In this paper, we provide an in-depth analysis of the drawbacks of some existing works and propose a few general principles for invariant subgraph extraction: 1) separability, as encouraged by our sparsity-driven mechanism, to filter out the irrelevant common features; 2) softness, for a broader solution space; and 3) differentiability, for a soundly end-to-end optimization pipeline. Specifically, building on optimal transport, we propose Graph Sinkhorn Attention (GSINA), a fully differentiable, cardinality-constrained attention mechanism that assigns sparse-yet-soft edge weights via Sinkhorn iterations and induces node attention. GSINA provides explicit controls for separability and softness, and uses a Gumbel reparameterization to stabilize training. It convergence behavior is also theoretically studied. Extensive empirical experimental results on both synthetic and real-world datasets validate its superiority.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过图 Sinkhorn Attention 改进图不变学习的子图提取",
        "abstract_cn": "图不变学习（GIL）寻求分布变化下图和标签之间的不变关系。最近的工作尝试提取不变子图来改进分布外（OOD）泛化，但现有方法要么缺乏对紧凑性的明确控制，要么依赖于硬顶$k$选择，从而缩小了解决方案空间并且仅是部分可微的。在本文中，我们深入分析了一些现有工作的缺点，并提出了一些不变子图提取的一般原则：1）可分离性，正如我们的稀疏驱动机制所鼓励的那样，过滤掉不相关的共同特征； 2）柔软性，提供更广阔的解决空间； 3) 可微性，用于完善的端到端优化流程。具体来说，在最佳传输的基础上，我们提出了图 Sinkhorn 注意力（GSINA），这是一种完全可微的、基数约束的注意力机制，通过 Sinkhorn 迭代分配稀疏但软的边缘权重并诱导节点注意力。 GSINA 提供了对可分离性和柔软度的显式控制，并使用 Gumbel 重新参数化来稳定训练。它的收敛行为也从理论上进行了研究。对合成数据集和真实世界数据集的广泛实证实验结果验证了其优越性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653776",
        "title": "Interpretable Subspace Clustering",
        "link": "https://doi.org/10.1109/tpami.2026.3653776",
        "published": "2026",
        "author": "Zheng Zhang, Peng Zhou, Aiting Yao, Liang Du, Xinwang Liu",
        "summary": "Subspace clustering is one of the most popular clustering methods due to its effectiveness. Although subspace clustering methods have been demonstrated to achieve promising performance, they still lack interpretability, especially when handling high-dimensional complicated data. To bridge this gap, this paper focuses on the interpretability of subspace clustering and proposes a novel interpretable subspace clustering method. Our goal is to answer two key questions about the interpretability in subspace clustering: (1) when handling an individual sample, which features should work for this sample? (2) Which cluster or subspace will the features that work put this sample into? To answer these two questions, we design two new interpretability regularized terms and plug them into the subspace clustering. In this way, we show that interpretability can be used to improve the clustering performance in turn. Extensive experiments on benchmark data sets demonstrate the effectiveness of our method in terms of clustering performance and interpretability.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "可解释的子空间聚类",
        "abstract_cn": "子空间聚类由于其有效性而成为最流行的聚类方法之一。尽管子空间聚类方法已被证明可以实现有希望的性能，但它们仍然缺乏可解释性，特别是在处理高维复杂数据时。为了弥补这一差距，本文重点关注子空间聚类的可解释性，并提出了一种新颖的可解释子空间聚类方法。我们的目标是回答有关子空间聚类可解释性的两个关键问题：（1）在处理单个样本时，哪些特征应该适用于该样本？ (2) 有效的特征会将该样本放入哪个簇或子空间？为了回答这两个问题，我们设计了两个新的可解释性正则化项并将它们插入子空间聚类中。通过这种方式，我们证明了可解释性可以用来提高聚类性能。对基准数据集的大量实验证明了我们的方法在聚类性能和可解释性方面的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654115",
        "title": "Practical Continual Forgetting for Pre-trained Vision Models",
        "link": "https://doi.org/10.1109/tpami.2026.3654115",
        "published": "2026",
        "author": "Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang",
        "summary": "For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">at any time</i> from both users and model owners, and these requests usually form a sequence. Therefore, under such a setting, selective information is expected to be <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">continuously</i> removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify three key challenges. <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(i)</b> For unwanted knowledge, efficient and effective deleting is crucial. <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(ii)</b> For remaining knowledge, the impact brought by the forgetting procedure should be minimal. <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(iii)</b> In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting. To address them, we first propose <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</b>roup <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">S</b>parse <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">LoRA</b> (GS-LoRA). Specifically, towards <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(i)</b>, we introduce modules to fine-tune the layers in Transformer blocks for each forgetting task independently, and towards <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. To further extend GS-LoRA to more practical scenarios, we incorporate</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">prototype</i> information as additional supervision and introduce a more practical approach, GS-LoRA++. For each forgotten class, we move the logits away from its original prototype. For the remaining classes, we pull the logits closer to their respective prototypes. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes. Codes have been released on <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/bjzhb666/GS-LoRA</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "预训练视觉模型的实用持续遗忘",
        "abstract_cn": "出于隐私和安全考虑，从预先训练的视觉模型中删除不需要的信息的需求如今变得越来越明显。在现实场景中，擦除请求<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">随时</i>来自用户和模型所有者，并且这些请求通常形成一个序列。因此，在这种设置下，选择性信息预计会从预训练模型中连续删除，同时保留其余信息。我们将这个问题定义为持续遗忘，并确定了三个关键挑战。 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(i)</b> 对于不需要的知识，高效、有效的删除至关重要。 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(ii)</b> 对于剩余知识，遗忘过程带来的影响应该是最小的。 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(iii)</b> 在现实场景中，遗忘过程中训练样本可能会稀缺或部分缺失。为了解决这些问题，我们首先提出<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</b>组<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">S</b>解析 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">LoRA</b> (GS-LoRA)。具体来说，针对 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(i)</b>，我们引入了模块来独立微调每个遗忘任务的 Transformer 块中的层，并针对 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">(ii)，采用简单的组稀疏正则化，能够自动选择特定的LoRA组并将其他组归零。为了进一步将 GS-LoRA 扩展到更实际的场景，我们结合了</b> <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">原型</i>信息作为额外的监督，并引入了更实用的方法 GS-LoRA++。对于每个被遗忘的类，我们将 logits 移离其原始原型。对于其余的类，我们将 logits 拉得更接近它们各自的原型。我们在人脸识别、物体检测和图像分类方面进行了广泛的实验，并证明我们的方法能够忘记特定的类别，而对其他类别的影响最小。代码已发布在 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/bjzhb666/GS-LoRA</uri> 上。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654264",
        "title": "Single-Photon Imaging in Complex Scenarios via Physics-Informed Deep Neural Networks",
        "link": "https://doi.org/10.1109/tpami.2026.3654264",
        "published": "2026",
        "author": "Siao Cai, Zhicheng Yu, Shaobing Gao, Zeyu Chen, Yiguang Liu",
        "summary": "Single-photon imaging uses single-photon-sensitive picosecond-resolution sensors to capture 3D structure and supports diverse applications, but success remains mostly limited to simple scenes. In complex scenarios, traditional methods degrade and deep learning methods lack flexibility and generalization. Here, we propose a physics-informed deep neural network (PIDNN) framework that effectively addresses both aspects, adapting to complex and variable sensing environments by embedding imaging physics into the deep neural network for unsupervised learning. Within this framework, by tailoring the number of U-Net skip connections, we impose multi-scale spatiotemporal priors that improve photon-utilization efficiency, laying the foundation for addressing the inherent low-signal-to-background ratio (SBR) problem in subsequent complex scenarios. Additionally, we introduce volume rendering into the PIDNN framework and design a dual-branch structure, further extending its applicability to multiple-depth and fog occlusion. We validated the performance of this method in various complex environments through numerical simulations and real-world experiments. The results of photon-efficient imaging with multiple returns show robust performance under low SBR and large fields of view. The method attains lower root mean-squared error than traditional methods and exhibits stronger generalization than supervised approaches. Further multiple depths and fog interference experiments confirm that its reconstruction quality surpasses existing techniques, demonstrating its flexibility and scalability. Both simulation and experimental results validate its exceptional reconstruction performance and flexibility.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过物理信息深度神经网络在复杂场景中进行单光子成像",
        "abstract_cn": "单光子成像使用单光子敏感的皮秒分辨率传感器来捕获 3D 结构并支持多种应用，但成功仍然主要局限于简单的场景。在复杂场景下，传统方法性能退化，深度学习方法缺乏灵活性和泛化性。在这里，我们提出了一种基于物理的深度神经网络（PIDNN）框架，该框架可以有效地解决这两方面的问题，通过将成像物理嵌入到深度神经网络中进行无监督学习来适应复杂多变的传感环境。在此框架内，通过定制U-Net跳跃连接的数量，我们施加多尺度时空先验，提高光子利用效率，为解决后续复杂场景中固有的低信号背景比（SBR）问题奠定基础。此外，我们将体渲染引入PIDNN框架并设计双分支结构，进一步扩展其对多深度和雾遮挡的适用性。我们通过数值模拟和真实实验验证了该方法在各种复杂环境中的性能。多次返回的光子高效成像结果显示出在低 SBR 和大视场下的稳健性能。该方法比传统方法具有更低的均方根误差，并且比监督方法具有更强的泛化能力。进一步的多重深度和雾干扰实验证实其重建质量超越了现有技术，展示了其灵活性和可扩展性。仿真和实验结果都验证了其卓越的重建性能和灵活性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654201",
        "title": "DyDiT++: Diffusion Transformers with Timestep and Spatial Dynamics for Efficient Visual Generation",
        "link": "https://doi.org/10.1109/tpami.2026.3654201",
        "published": "2026",
        "author": "Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You",
        "summary": "Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior perfor mance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the static inference paradigm, which inevitably introduces redundant computation in certain diffusion timesteps and spatial regions. To overcome this inefficiency, we propose Dynamic Diffusion Transformer (DyDiT), an architecture that dynamically adjusts its computation along both timestep and spatial dimensions. Specifically, we introduce a Timestep-wise Dynamic Width (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a Spatial-wise Dynamic Token (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerate the generation process. Building on these designs, we present an extended version, DyDiT++, with improvements in three key aspects. First, it extends the generation mechanism of DyDiT beyond diffusion to flow matching, demon strating that our method can also accelerate flow-matching based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT++. Remarkably, with <3% additional f ine-tuning iterations, our approach reduces the FLOPs of DiT XL by 51%, yielding 1.73× realistic speedup on hardware, and achieves a competitive FID score of 2.07 on ImageNet. The code is available at https://github.com/alibaba-damo-academy/DyDiT.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "DyDiT++：具有时间步长和空间动力学的扩散变压器，可实现高效的视觉生成",
        "abstract_cn": "Diffusion Transformer (DiT) 是一种新兴的视觉生成扩散模型，已表现出卓越的性能，但计算成本高昂。我们的研究表明，这些成本主要源于静态推理范式，这不可避免地在某些扩散时间步长和空间区域中引入了冗余计算。为了克服这种低效率，我们提出了动态扩散变压器（DyDiT），这是一种沿着时间步长和空间维度动态调整其计算的架构。具体来说，我们引入了一种时间步长动态宽度（TDW）方法，该方法根据生成时间步长调整模型宽度。此外，我们设计了一种空间智能动态令牌（SDT）策略，以避免在不必要的空间位置进行冗余计算。 TDW和SDT可以无缝集成到DiT中并显着加速生成过程。在这些设计的基础上，我们提出了一个扩展版本 DyDiT++，在三个关键方面进行了改进。首先，它将 DyDiT 的生成机制从扩散扩展到流匹配，这表明我们的方法还可以加速基于流匹配的生成，增强其多功能性。此外，我们增强了 DyDiT 以处理更复杂的视觉生成任务，包括视频生成和文本到图像生成，从而拓宽了其实际应用。最后，为了解决全面微调和民主化技术访问的高成本问题，我们研究了以参数有效的方式训练 DyDiT 的可行性，并引入了基于时间步长的动态 LoRA (TD-LoRA)。对各种视觉生成模型（包括 DiT、SiT、Latte 和 FLUX）的大量实验证明了 DyDiT++ 的有效性。值得注意的是，通过 <3% 的额外微调迭代，我们的方法将 DiT XL 的 FLOP 降低了 51%，在硬件上产生了 1.73 倍的实际加速，并在 ImageNet 上实现了具有竞争力的 FID 分数 2.07。代码可在 https://github.com/alibaba-damo-academy/DyDiT 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653780",
        "title": "Non-Gradient Hash Factor Learning for High-Dimensional and Incomplete Data Representation Learning",
        "link": "https://doi.org/10.1109/tpami.2026.3653780",
        "published": "2026",
        "author": "Di Wu, Shihui Li, Yi He, Xin Luo, Xinbo Gao",
        "summary": "High-dimensional and incomplete (HDI) data are ubiquitous in various Big Data-related industrial applications, such as drug innovation and recommender systems. Hash learning is the most efficient representation learning approach to extract hidden information from HDI data owing to its fast reasoning and low storage. However, an existing hash learning approach commonly employs gradient-based optimization techniques to address the discrete objective caused by the binary nature of hash factors, where the Quantization (i.e., quantizing the real values to binary codes) loss is inevitable, resulting in accuracy loss when representing HDI data. Motivated by these critical and vital issues, this paper proposes a non-gradient hash factor (NGHF) model with three-fold ideas: a) innovating a discrete differential evolution (DDE) algorithm able to simulate the continuous optimization via disabling bits of binary codes based on the projected Hamming dissimilarity, thus enabling an effective discrete optimizer, b) applying the proposed DDE algorithm to directly optimize the discrete learning objective of NGHF defined on HDI data, thereby facilitating its efficient and precise training without any Quantization loss, and c) theoretically proving the convergence of NGHF. As such, NGHF possesses high representation learning ability comparable to that of a real-valued model, making it able to achieve precise binary representation to HDI data. Extensive experimental results on nine real-world datasets demonstrate that NGHF significantly outperforms eight state-of-the-art hash learning models. Moreover, its accuracy is amazingly comparable to that of a real valued model for HDI data representation learning. Such results are inspiring for facilitating hash-learning models with both high accuracy and fast reasoning on HDI data, which is critical for industrial applications. Our source code is shared at the link: https://github.com/wudi1989/NGHF.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于高维和不完整数据表示学习的非梯度哈希因子学习",
        "abstract_cn": "高维不完整（HDI）数据在各种大数据相关的工业应用中无处不在，例如药物创新和推荐系统。哈希学习因其快速推理和低存储量而成为从 HDI 数据中提取隐藏信息的最有效的表示学习方法。然而，现有的哈希学习方法通​​常采用基于梯度的优化技术来解决由哈希因子的二进制性质引起的离散目标，其中量化（即将实际值量化为二进制代码）损失是不可避免的，导致在表示HDI数据时精度损失。受这些关键和重要问题的启发，本文提出了一种具有三重思想的非梯度哈希因子（NGHF）模型：a）创新离散差分进化（DDE）算法，能够通过基于投影汉明相异性禁用二进制代码位来模拟连续优化，从而实现有效的离散优化器，b）应用所提出的DDE算法直接优化在HDI数据上定义的NGHF的离散学习目标，从而促进其高效和精确的训练，而无需任何量化损失，c）从理论上证明NGHF的收敛性。因此，NGHF具有与实值模型相当的高表示学习能力，使其能够实现对HDI数据的精确二进制表示。对 9 个真实世界数据集的广泛实验结果表明，NGHF 显着优于 8 个最先进的哈希学习模型。此外，其准确性与 HDI 数据表示学习的实值模型的准确性令人惊讶。这些结果对于促进在 HDI 数据上实现高精度和快速推理的哈希学习模型具有启发意义，这对于工业应用至关重要。我们的源代码在链接中共享：https://github.com/wudi1989/NGHF。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654665",
        "title": "Learning-Based Multi-View Stereo: A Survey",
        "link": "https://doi.org/10.1109/tpami.2026.3654665",
        "published": "2026",
        "author": "Fangjinhua Wang, Qingtian Zhu, Di Chang, Quankai Gao, Junlin Han, Tong Zhang, Richard Hartley, Marc Pollefeys",
        "summary": "3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented/Virtual Reality (AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "基于学习的多视图立体：调查",
        "abstract_cn": "3D 重建旨在恢复场景的密集 3D 结构。它在增强/虚拟现实（AR/VR）、自动驾驶和机器人等各种应用中发挥着重要作用。多视图立体 (MVS) 算法利用从不同视点捕获的场景的多个视图，合成全面的 3D 表示，从而能够在复杂环境中进行精确重建。由于其效率和有效性，MVS 已成为基于图像的 3D 重建的关键方法。最近，随着深度学习的成功，许多基于学习的 MVS 方法被提出，与传统方法相比取得了令人印象深刻的性能。我们将这些基于学习的方法分类为：基于深度图、基于体素、基于 NeRF、基于 3D 高斯分布和大前馈方法。其中，我们特别关注基于深度图的方法，由于其简洁性、灵活性和可扩展性，这些方法是 MVS 的主要系列。在这项调查中，我们对撰写本文时的文献进行了全面的回顾。我们研究了这些基于学习的方法，总结了它们在流行基准上的表现，并讨论了该领域有前途的未来研究方向。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655110",
        "title": "Isolating Interference Factors for Robust Cloth-Changing Person Re-Identification",
        "link": "https://doi.org/10.1109/tpami.2026.3655110",
        "published": "2026",
        "author": "De Cheng, Yubo Li, Chaowei Fang, Shizhou Zhang, Nannan Wang, Xinbo Gao",
        "summary": "Cloth-Changing Person Re-Identification (CC-ReID) aims to recognize individuals across camera views despite clothing variations, a crucial task for surveillance and security systems. Existing methods typically frame it as a cross-modal alignment problem but often overlook explicit modeling of interference factors such as clothing, viewpoints, and pedestrian actions. This oversight can distort their impact, compromising the extraction of robust identity features. To address these challenges, we propose a novel framework that systematically disentangles interference factors from identity features while ensuring the robustness and discriminative power of identity representations. Our approach consists of two key components. First, a dual-stream identity feature learning framework leverages a raw image stream and a cloth-isolated stream, to extract identity representations independent of clothing textures. An adaptive cloth-irrelevant contrastive objective is introduced to mitigate identity feature variations caused by clothing differences. Second, we propose a Text-Driven Conditional Generative Adversarial Interference Disentanglement Network (T-CGAIDN), to further suppress interference factors beyond clothing textures, such as finer clothing patterns, viewpoint, background, and lighting conditions. This network incorporates a multi-granularity interference recognition branch to learn interference-related features, a conditional adversarial module for bidirectional transformation between identity and interference feature spaces, and an interference decoupling objective to eliminate interference dependencies in identity learning. Extensive experiments on public benchmarks demonstrate that our method significantly outperforms state-ofthe- art approaches, highlighting its effectiveness in CC-ReID. Our code is available at https://github.com/yblTech/IIFR-CCReID.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "隔离干扰因素，实现稳健的换布人员重新识别",
        "abstract_cn": "衣服变化的人员重新识别（CC-ReID）旨在尽管衣服变化，但仍能在摄像机视图中识别个人，这对于监控和安全系统来说是一项至关重要的任务。现有的方法通常将其视为跨模式对齐问题，但常常忽略对干扰因素（例如服装、视角和行人行为）的显式建模。这种疏忽可能会扭曲其影响，从而损害稳健身份特征的提取。为了应对这些挑战，我们提出了一种新颖的框架，系统地从身份特征中分离出干扰因素，同时确保身份表示的稳健性和辨别力。我们的方法由两个关键部分组成。首先，双流身份特征学习框架利用原始图像流和布料隔离流来提取独立于服装纹理的身份表示。引入自适应服装无关对比目标来减轻由服装差异引起的身份特征变化。其次，我们提出了一种文本驱动的条件生成对抗性干扰解缠网络（T-CGAIDN），以进一步抑制服装纹理之外的干扰因素，例如更精细的服装图案、视点、背景和照明条件。该网络包含一个多粒度干扰识别分支来学习干扰相关特征，一个用于身份和干扰特征空间之间双向转换的条件对抗模块，以及一个干扰解耦目标来消除身份学习中的干扰依赖性。对公共基准的大量实验表明，我们的方法明显优于最先进的方法，凸显了其在 CC-ReID 中的有效性。我们的代码可在 https://github.com/yblTech/IIFR-CCReID 获取。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3654392",
        "title": "Like Human Rethinking: Contour Transformer AutoRegression for Referring Remote Sensing Interpretation",
        "link": "https://doi.org/10.1109/tpami.2026.3654392",
        "published": "2026",
        "author": "Jinming Chai, Licheng Jiao, Xiaoqiang Lu, Lingling Li, Fang Liu, Long Sun, Xu Liu, Wenping Ma, Weibin Li",
        "summary": "Referring remote sensing interpretation holds significant application value in various scenarios such as ecological protection, resource exploration, and emergency management. However, referring remote sensing expression comprehension and segmentation (RRSECS) faces critical challenges, including micro-target localization drift problem caused by insufficient extraction of boundary features in existing paradigms. Moreover, when transferred to remote sensing domains, polygon-based methods encounter issues such as contour-boundary misalignment and multi-task co-optimization conflicts problems. In this paper, we propose SeeFormer, a novel contour autoregressive paradigm specifically designed for RRSECS, which accurately locates and segments micro, irregular targets in remote sensing imagery. We first introduce a brain-inspired feature refocus learning (BIFRL) module that progressively attends to effective object features via a coarse-to-fine scheme, significantly boosting small-object localization and segmentation. Next, we present a language-contour enhancer (LCE) that injects shape-aware contour priors, and a corner-based contour sampler (CBCS) to improve mask-polygon reconstruction fidelity. Finally, we develop an autoregressive dual-decoder paradigm (ARDDP) that preserves sequence consistency while alleviating multi-task optimization conflicts. Extensive experiments on RefDIOR, RRSIS-D, and OPT-RSVG datasets under varying scenarios, scales, and task paradigms demonstrate transformative performance gains: compared to the baseline PolyFormer, our proposed SeeFormer improves oIoU and mIoU by 27.58% and 39.37% for referring image segmentation and by 18.94% and 28.90% for visual grounding on the RefDIOR dataset. The code will be publicly accessible at https://github.com/IPIU-XDU/RSFM.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "像人类的反思：用于参考遥感解译的轮廓变换器自回归",
        "abstract_cn": "参考遥感解译在生态保护、资源勘探、应急管理等多种场景中具有重要的应用价值。然而，参考遥感表达理解和分割（RRSECS）面临着严峻的挑战，包括现有范式中边界特征提取不足而导致的微目标定位漂移问题。此外，当转移到遥感领域时，基于多边形的方法会遇到轮廓边界错位和多任务协同优化冲突等问题。在本文中，我们提出了 SeeFormer，这是一种专为 RRSECS 设计的新型轮廓自回归范式，它可以准确定位和分割遥感图像中的微小不规则目标。我们首先引入了一种受大脑启发的特征重新聚焦学习（BIFRL）模块，该模块通过从粗到细的方案逐步关注有效的对象特征，从而显着促进小对象的定位和分割。接下来，我们提出了一种注入形状感知轮廓先验的语言轮廓增强器（LCE）和一个基于角的轮廓采样器（CBCS）来提高掩模多边形重建保真度。最后，我们开发了一种自回归双解码器范式（ARDDP），它可以保持序列一致性，同时减轻多任务优化冲突。在不同场景、规模和任务范例下对 RefDIOR、RRSIS-D 和 OPT-RSVG 数据集进行的大量实验证明了变革性的性能增益：与基线 PolyFormer 相比，我们提出的 SeeFormer 在参考图像分割方面将 oIoU 和 mIoU 提高了 27.58% 和 39.37%，在 RefDIOR 数据集上将视觉基础提高了 18.94% 和 28.90%。该代码将在 https://github.com/IPIU-XDU/RSFM 上公开访问。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655147",
        "title": "Generalizable Egocentric Task Verification Via Cross-Modal Hybrid Hypergraph Matching",
        "link": "https://doi.org/10.1109/tpami.2026.3655147",
        "published": "2026",
        "author": "Xun Jiang, Xing Xu, Zheng Wang, Jingkuan Song, Fumin Shen, Heng Tao Shen",
        "summary": "Egocentric Task Verification (ETV) aims to determine if the operation flows of procedural tasks in egocentric videos align with the logic of given rules. Early works adopt the video-based verification paradigm that compares a reference video to the testing video, which limits the flexibility of model deployment. Recent researches incorporate reference textual rules instead of videos, describing the operational logic with natural language, but also raises the challenges of cross-modal heterogeneity and hierarchical misalignment between the two modalities. While previous works mainly address the cross-modal heterogeneity between vision and text modalities, they inevitably suffer from two additional key challenges: (1) Existing methods are mostly developed in synthetic domains, yet have not considered the issues of synthetic-to-realistic generalization challenges in real-world applications. (2) The intricate relations between visual content and textual rule involve multiple matching correlations, indicating high-order matching interactions. To address these issues, we proposed the Generalizable Egocentric Task Verification (GETV), and construct a cross-domain ETV benchmark dataset, EgoCross. It features synthetic-to-real cross-domain evaluation, covering both synthetic datasets for training and realistic datasets for testing, across three different types of tasks. Furthermore, we also propose a novel method for this challenge, termed Cross-modal Hybrid Hypergraph Matching (CHHM), which models the logical cross-modal matching in the GETV challenge as a heterogeneous hybrid hypergraph learning process, thus addressing intrinsic multiple matching correlations. Additionally, to tackle the problems of synthetic-to-realistic generalization, we enhance the cross-modal matching process with prototype-based graph representation alignment, which effectively mitigates the cross-domain gap. Extensive experiments on the existing two ETV benchmark datasets, i.e., EgoTV and CSV-NL, and our proposed GETV dataset EgoCross, demonstrate our approach establishes new state-of-the-art performance on both intra-domain and cross-domain challenges. Our project page is available at https://xunchn.github.io/EgoCross/.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过跨模态混合超图匹配进行可推广的以自我为中心的任务验证",
        "abstract_cn": "自我中心任务验证（ETV）旨在确定以自我为中心的视频中程序任务的操作流程是否符合给定规则的逻辑。早期的工作采用基于视频的验证范例，将参考视频与测试视频进行比较，这限制了模型部署的灵活性。最近的研究结合了参考文本规则而不是视频，用自然语言描述了操作逻辑，但也提出了跨模态异构性和两种模态之间的层次错位的挑战。虽然以前的工作主要解决视觉和文本模态之间的跨模态异构性，但它们不可避免地面临两个额外的关键挑战：（1）现有方法大多是在合成领域开发的，但没有考虑现实世界应用中合成到现实的泛化挑战问题。 (2)视觉内容与文本规则之间错综复杂的关系涉及多重匹配相关性，表明高阶匹配交互。为了解决这些问题，我们提出了通用化自我中心任务验证（GETV），并构建了跨域 ETV 基准数据集 EgoCross。它具有从合成到真实的跨域评估，涵盖用于训练的合成数据集和用于测试的真实数据集，涵盖三种不同类型的任务。此外，我们还针对这一挑战提出了一种新方法，称为跨模态混合超图匹配（CHHM），它将 GETV 挑战中的逻辑跨模态匹配建模为异构混合超图学习过程，从而解决内在的多重匹配相关性。此外，为了解决合成到现实泛化的问题，我们通过基于原型的图表示对齐来增强跨模式匹配过程，这有效地缓解了跨域差距。对现有的两个 ETV 基准数据集（即 EgoTV 和 CSV-NL）以及我们提出的 GETV 数据集 EgoCross 进行的广泛实验表明，我们的方法在域内和跨域挑战上都建立了新的最先进的性能。我们的项目页面位于 https://xunchn.github.io/EgoCross/。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655829",
        "title": "Enhance Before Fusion: Multi-View Graph Clustering With Graph Trend Filter",
        "link": "https://doi.org/10.1109/tpami.2026.3655829",
        "published": "2026",
        "author": "Penglei Wang, Jitao Lu, Danyang Wu, Rong Wang, Feiping Nie",
        "summary": "Recently, Multi-View Graph Clustering (MVGC) methods have achieved significant progress, leading to their wide adoption in various applications. However, most MVGC methods merely pursue consistent information by simply fusing multi-view graphs, ignoring the cross-view interactions among them, which limits the ceiling of their performance. To make up for this deficiency, we design a credible cross-view graph enhancement module to explore the credible topological structure, while accomplishing cross-view interactions, to boost clustering performance in multi-view graph scenarios. Besides, we reconsider the graph clustering task from the perspective of graph signal processing. From this novel perspective, we adapt the high-order Graph Trend Filter to reveal the inhomogeneities in graph smoothness levels and further consider the brand-new local preference in MVGC, which provides theoretical guidance for graph clustering. Building on these insights, we propose the Enhanced Graph Trend Filter Clustering (EGTFC) method and present an effective algorithm accompanied by corresponding theoretical analyses to tackle the optimization problem inherent in EGTFC. Finally, substantial experimental results on twelve benchmark datasets demonstrate the effectiveness of our proposals and the superiority over thirteen state-of-the-art MVGC methods.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "融合前增强：使用图趋势过滤器进行多视图图聚类",
        "abstract_cn": "最近，多视图图聚类（MVGC）方法取得了重大进展，使其在各种应用中得到广泛采用。然而，大多数MVGC方法只是通过简单地融合多视图图来追求一致的信息，忽略了它们之间的跨视图交互，这限制了它们的性能上限。为了弥补这一缺陷，我们设计了可信的跨视图图增强模块来探索可信的拓扑结构，同时完成跨视图交互，以提高多视图图场景中的聚类性能。此外，我们从图信号处理的角度重新考虑图聚类任务。从这个新颖的角度出发，我们采用高阶图趋势过滤器来揭示图平滑度的不均匀性，并进一步考虑MVGC中全新的局部偏好，这为图聚类提供了理论指导。基于这些见解，我们提出了增强型图趋势过滤聚类（EGTFC）方法，并提出了一种有效的算法以及相应的理论分析来解决 EGTFC 固有的优化问题。最后，在 12 个基准数据集上的大量实验结果证明了我们的建议的有效性以及相对于 13 种最先进的 MVGC 方法的优越性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655456",
        "title": "Scalable Semi-supervised Learning with Discriminative Label Propagation and Correction",
        "link": "https://doi.org/10.1109/tpami.2026.3655456",
        "published": "2026",
        "author": "Bingbing Jiang, Jie Wen, Zidong Wang, Weiguo Sheng, Zhiwen Yu, Huanhuan Chen, Weiping Ding",
        "summary": "Semi-supervised learning can leverage both labeled and unlabeled samples simultaneously to improve performance. However, existing methods often present the following issues: (1) The emphasis of learning is put on either the similarity structures or the regression losses of data, neglecting the interaction between them. (2) The similarity structures among boundary samples might be unreliable, which misleads label propagation and impairs the performance of models on out-of-sample data. (3) They often involve the inverses of high-order matrices, making them inefficient in computation. To overcome these issues, we propose a scalable semi-supervised learning framework with Discriminative Label Propagation and Correction (DLPC), which collaboratively exploits the regression losses and similarity structures of data. Particularly, each sample is projected onto the independent class labels associated with nonnegative adjustment vectors rather than the propagated labels, such that the distances between samples from different classes are naturally enlarged, making regression losses more effective for boundary samples. Benefiting from this, the regression losses can guide the propagation of labels in boundary areas. Thus, the label information is first propagated through dynamically optimized graph structures and then corrected by the regression losses, effectively improving the quality of labels and facilitating feature projection learning. Furthermore, an accelerated solution has been developed to reduce the computational costs of DLPC on sample scales, thereby making it scalable to relatively large-scale problems. Moreover, the proposed DLPC can not only be applied to single-view scenarios but also extended to multi-view tasks. Additionally, an optimization strategy with fast convergence has been presented for DLPC, and extensive experiments demonstrate the effectiveness and superiority of DLPC over state-of-the-art competitors.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "具有判别性标签传播和校正的可扩展半监督学习",
        "abstract_cn": "半监督学习可以同时利用标记和未标记样本来提高性能。然而，现有的方法常常存在以下问题：（1）学习的重点要么放在数据的相似结构上，要么放在数据的回归损失上，而忽略了它们之间的相互作用。 （2）边界样本之间的相似性结构可能不可靠，这会误导标签传播并损害模型在样本外数据上的性能。 (3)它们经常涉及高阶矩阵的逆，使得计算效率低下。为了克服这些问题，我们提出了一种具有判别性标签传播和校正（DLPC）的可扩展半监督学习框架，该框架协作利用数据的回归损失和相似性结构。特别地，每个样本被投影到与非负调整向量相关联的独立类标签上，而不是传播标签上，使得来自不同类的样本之间的距离自然地扩大，使得回归损失对于边界样本更有效。受益于此，回归损失可以指导边界区域中标签的传播。因此，标签信息首先通过动态优化的图结构传播，然后通过回归损失进行校正，有效提高标签的质量并促进特征投影学习。此外，还开发了一种加速解决方案来降低样本规模上 DLPC 的计算成本，从而使其能够扩展到相对大规模的问题。此外，所提出的DLPC不仅可以应用于单视图场景，还可以扩展到多视图任务。此外，还为 DLPC 提出了一种快速收敛的优化策略，并且大量的实验证明了 DLPC 相对于最先进的竞争对手的有效性和优越性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655641",
        "title": "Defying Distractions in Multimodal Tasks: A Novel Benchmark for Large Vision-Language Models",
        "link": "https://doi.org/10.1109/tpami.2026.3655641",
        "published": "2026",
        "author": "Jinhui Yang, Ming Jiang, Qi Zhao",
        "summary": "Large Vision-Language Models (LVLMs) with \"multimodal distractibility,\" where plausible but irrelevant visual or textual inputs cause significant drops in reasoning consistency and lead to unreliable outputs. This paper introduces a comprehensive framework to systematically diagnose, evaluate, and mitigate this critical challenge. We present three core components: the large-scale IR-VQA benchmark to surface these vulnerabilities across four paradigms; novel diagnostic metrics, Positive Consistency (PC) and Negative Consistency (NC), which move beyond standard accuracy to rigorously measure a model's reasoning stability; and the Relevance-Gated Multimodal Routing (RGMR) mechanism, a novel, lightweight module that proactively and dynamically filters distractions at inference time. Our experiments reveal that state-of-the-art models exhibit significant drops in consistency on IR-VQA. We demonstrate that finetuning on IR-VQA and deploying RGMR substantially improve model robustness where standard prompting fails. Our comprehensive analysis of model behaviors under different types of distractions and the underlying reasoning failures provides a clear path forward for developing more reliable multimodal systems.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "克服多模式任务中的干扰：大型视觉语言模型的新基准",
        "abstract_cn": "具有“多模式分散性”的大型视觉语言模型（LVLM），其中看似合理但不相关的视觉或文本输入会导致推理一致性显着下降并导致输出不可靠。本文介绍了一个综合框架来系统地诊断、评估和缓解这一关键挑战。我们提出了三个核心组件：大规模 IR-VQA 基准测试，以跨四种范式显示这些漏洞；新颖的诊断指标，积极一致性（PC）和消极一致性（NC），超越标准精度，严格衡量模型的推理稳定性；相关性门控多模态路由 (RGMR) 机制，这是一种新颖的轻量级模块，可以在推理时主动、动态地过滤干扰。我们的实验表明，最先进的模型在 IR-VQA 上的一致性显着下降。我们证明，在标准提示失败的情况下，对 IR-VQA 进行微调并部署 RGMR 可以显着提高模型的稳健性。我们对不同类型的干扰下的模型行为和潜在的推理失败进行了全面分析，为开发更可靠的多模式系统提供了明确的前进道路。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3653620",
        "title": "Toward Accurate Image Generation via Dynamic Generative Image Transformer",
        "link": "https://doi.org/10.1109/tpami.2026.3653620",
        "published": "2026",
        "author": "Zhendong Mao, Mengqi Huang, Yijing Lin, Quan Wang, Lei Zhang, Yongdong Zhang",
        "summary": "Existing generative image transformers follow a two-stage generation paradigm, where the first stage learns a codebook to encode images into discrete codes via vector quantization, and the second stage completes the image generation based on the learned codebook. However, existing methods ignore the naturally varying information densities across different image regions and indiscriminately encode fixed-size regions into fixed-length codes, resulting in insufficient encoding in important regions and redundant encoding in unimportant ones, which degrades both the image generation quality and speed. To address this challenge, we propose a novel information-density-based variable-length image coding and generation framework. In the first stage, our Dynamic Quantization VAE++ (DQVAE++) performs information-adaptive encoding by assigning variable-length codes to image regions according to their information densities, yielding more accurate and robust code representations. In the second stage, the Dynamic Generative Image Transformer (DGiT) enables information-adaptive image generation in both autoregressive and non-autoregressive manners. Specifically, for autoregressive (AR) generation, DGiT-AR generates images autoregressively from coarse-grained regions (smooth areas with fewer codes) to fine-grained regions (detailed areas with more codes). This is accomplished through a novel stacked-transformer architecture that alternately models the position and content of image codes, and a novel heterogeneous embedding scheme to distinguish codes of different granularities. Similarly, for non-autoregressive (NAR) generation, DGiT-NAR introduces a novel information-prioritized mask scheduling mechanism, prioritizing the generation of key structural regions with higher information density. This enables more coherent modeling of global structures initially, followed by a more effective synthesis of local details subsequently. Comprehensive experiments on unconditional and conditional image generation validate the superiority of our proposed variable-length coding in both effectiveness and efficiency.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过动态生成图像转换器实现精确图像生成",
        "abstract_cn": "现有的生成图像变换器遵循两阶段生成范例，其中第一阶段学习码本，通过矢量量化将图像编码为离散代码，第二阶段基于学习的码本完成图像生成。然而，现有方法忽略了不同图像区域之间自然变化的信息密度，不加区别地将固定大小的区域编码为固定长度的代码，导致重要区域编码不足，不重要区域编码冗余，从而降低了图像生成质量和速度。为了应对这一挑战，我们提出了一种新颖的基于信息密度的可变长度图像编码和生成框架。在第一阶段，我们的动态量化 VAE++ (DQVAE++) 通过根据图像区域的信息密度将可变长度代码分配给图像区域来执行信息自适应编码，从而产生更准确、更鲁棒的代码表示。在第二阶段，动态生成图像变换器（DGiT）能够以自回归和非自回归方式生成信息自适应图像。具体来说，对于自回归 (AR) 生成，DGiT-AR 从粗粒度区域（具有较少代码的平滑区域）到细粒度区域（具有较多代码的详细区域）自回归生成图像。这是通过一种新颖的堆叠变压器架构来实现的，该架构交替模拟图像代码的位置和内容，以及一种新颖的异构嵌入方案来区分不同粒度的代码。类似地，对于非自回归（NAR）生成，DGiT-NAR 引入了一种新颖的信息优先掩模调度机制，优先生成具有更高信息密度的关键结构区域。这使得最初能够对全局结构进行更连贯的建模，然后对局部细节进行更有效的综合。无条件和条件图像生成的综合实验验证了我们提出的可变长度编码在有效性和效率方面的优越性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655694",
        "title": "A General Image Fusion Approach Exploiting Gradient Transfer Learning and Fusion Rule Unfolding",
        "link": "https://doi.org/10.1109/tpami.2026.3655694",
        "published": "2026",
        "author": "Wu Wang, Liang-Jian Deng, Qi Cao, Gemine Vivone",
        "summary": "The goal of a deep learning-based general image fusion method is to solve multiple image fusion tasks with a single model, thereby facilitating the deployment of models in practical applications. However, existing methods fail to provide an efficient and comprehensive solution from both model training and network design perspectives. Regarding model training, current approaches cannot effectively leverage complementary information across different tasks. In terms of network design, they rely on experience-based network designs. To address these issues, we propose a comprehensive framework for general image fusion using the newly proposed gradient transfer learning and fusion rule unfolding. To leverage complementary information across different tasks during training, we propose a sequential gradient-transfer framework based on the idea that different image fusion tasks often exhibit complementary structural details and that image gradients effectively capture these details. To move beyond heuristic-based network design, we evolved a fundamental image fusion rule and integrated it into a deep equilibrium model, resulting in a more efficient and versatile image fusion network capable of uniformly handling various fusion tasks. Considering three different image fusion tasks, i.e., multi-focus image fusion, multi-exposure image fusion, and infrared and visible image fusion, our method not only produces images with richer structural information but also achieves highly competitive objective metrics. Furthermore, the results of generalization experiments on previously unseen image fusion tasks, i.e., medical image fusion, demonstrate that our method significantly outperforms competing approaches. The code will be available upon possible acceptance.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "利用梯度迁移学习和融合规则展开的通用图像融合方法",
        "abstract_cn": "基于深度学习的通用图像融合方法的目标是用单个模型解决多个图像融合任务，从而促进模型在实际应用中的部署。然而，现有的方法无法从模型训练和网络设计的角度提供有效且全面的解决方案。关于模型训练，当前的方法无法有效地利用不同任务之间的互补信息。在网络设计方面，他们依赖于基于经验的网络设计。为了解决这些问题，我们使用新提出的梯度转移学习和融合规则展开，提出了通用图像融合的综合框架。为了在训练过程中利用不同任务之间的互补信息，我们提出了一种顺序梯度传输框架，该框架基于以下想法：不同的图像融合任务通常表现出互补的结构细节，并且图像梯度有效地捕获这些细节。为了超越基于启发式的网络设计，我们发展了基本的图像融合规则并将其集成到深度平衡模型中，从而形成了更高效、更通用的图像融合网络，能够统一处理各种融合任务。考虑到三种不同的图像融合任务，即多焦点图像融合、多曝光图像融合以及红外和可见光图像融合，我们的方法不仅产生具有更丰富结构信息的图像，而且实现了极具竞争力的客观指标。此外，对以前未见过的图像融合任务（即医学图像融合）的泛化实验结果表明，我们的方法明显优于竞争方法。该代码将在可能被接受后提供。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656169",
        "title": "Momentor++: Advancing Video Large Language Models With Fine-Grained Long Video Reasoning",
        "link": "https://doi.org/10.1109/tpami.2026.3656169",
        "published": "2026",
        "author": "Juncheng Li, Minghe Gao, Xiangnan He, Siliang Tang, Weishi Zheng, Jun Xiao, Meng Wang, Tat-Seng Chua, Yueting Zhuang",
        "summary": "Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "Momentor++：通过细粒度长视频推理推进视频大语言模型",
        "abstract_cn": "大型语言模型 (LLM) 在理解和管理基于文本的任务方面表现出卓越的熟练程度。许多作品试图将这些功能转移到视频领域，称为视频LLM。然而，当前的视频法学硕士只能掌握粗粒度的语义，无法有效地处理涉及特定视频片段的理解或定位的任务。为了应对这些挑战，我们提出了 Momentor，一种视频法学硕士，旨在执行细粒度的时间理解任务。为了方便Momentor的训练，我们开发了一个自动数据生成引擎来构建Moment-10 M，这是一个具有分段级指令数据的大规模视频指令数据集。在之前发布的 Momentor 和 Moment-10 M 数据集的基础上，我们通过引入时空令牌合并（STTC）方法进一步扩展了这项工作，该方法可以以无参数的方式在时空上合并冗余视觉令牌，从而显着提高计算效率，同时保留细粒度的视觉细节。我们将 STTC 与 Momentor 集成来开发 Momentor++ 并在各种基准上验证其性能。 Momentor 展示了细粒度时间理解和本地化方面的强大功能。此外，Momentor++ 擅长高效处理和分析具有复杂事件的扩展视频，展示了在处理广泛的时间上下文方面的显着进步。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656742",
        "title": "Active Adversarial Noise Suppression for Image Forgery Localization",
        "link": "https://doi.org/10.1109/tpami.2026.3656742",
        "published": "2026",
        "author": "Rongxuan Peng, Shunquan Tan, Xianbo Mo, Alex C. Kot, Jiwu Huang",
        "summary": "Recent advances in deep learning have significantly propelled the development of image forgery localization. However, existing models remain highly vulnerable to adversarial attacks: imperceptible noise added to forged images can severely mislead these models. In this paper, we address this challenge with an Adversarial Noise Suppression Module (ANSM) that generates a defensive perturbation to suppress the attack effect of adversarial noise. We observe that forgery-relevant features extracted from adversarial and original forged images exhibit distinct distributions. To bridge this gap, we introduce Forgery-relevant Features Alignment (FFA) as a first-stage training strategy, which reduces distributional discrepancies by minimizing the channel-wise Kullback-Leibler divergence between these features. To further refine the defensive perturbation, we design a second-stage training strategy, termed Mask-guided Refinement (MgR), which incorporates a dual-mask constraint. MgR ensures that the defensive perturbation remains effective for both adversarial and original forged images, recovering forgery localization accuracy to their original level. Extensive experiments across various attack algorithms demonstrate that our method significantly restores the forgery localization model's performance on adversarial images. Notably, when ANSM is applied to original forged images, the performance remains nearly unaffected. To our best knowledge, this is the first report of adversarial defense in image forgery localization tasks. We have released the source code and anti-forensics dataset.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于图像伪造定位的主动对抗性噪声抑制",
        "abstract_cn": "深度学习的最新进展极大地推动了图像伪造定位的发展。然而，现有模型仍然非常容易受到对抗性攻击：伪造图像中添加的难以察觉的噪声可能会严重误导这些模型。在本文中，我们通过对抗性噪声抑制模块（ANSM）来应对这一挑战，该模块生成防御性扰动以抑制对抗性噪声的攻击效果。我们观察到，从对抗性和原始伪造图像中提取的伪造相关特征表现出不同的分布。为了弥补这一差距，我们引入了伪造相关特征对齐（FFA）作为第一阶段的训练策略，它通过最小化这些特征之间的通道方向 Kullback-Leibler 散度来减少分布差异。为了进一步细化防御扰动，我们设计了第二阶段训练策略，称为掩模引导细化（MgR），其中包含双掩模约束。 MgR 确保防御扰动对于对抗性和原始伪造图像仍然有效，将伪造定位精度恢复到原始水平。各种攻击算法的大量实验表明，我们的方法显着恢复了伪造定位模型在对抗图像上的性能。值得注意的是，当 ANSM 应用于原始伪造图像时，性能几乎不受影响。据我们所知，这是图像伪造定位任务中对抗性防御的第一份报告。我们已经发布了源代码和反取证数据集。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656763",
        "title": "Unifying Multi-modal Hair Editing via Proxy Feature Blending",
        "link": "https://doi.org/10.1109/tpami.2026.3656763",
        "published": "2026",
        "author": "Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Can Wang, Weiming Zhang, Gang Hua, Nenghai Yu",
        "summary": "Hair editing is a long-standing problem in computer vision that demands both fine-grained local control and intuitive user interactions across diverse modalities. Despite the remarkable progress of GANs and diffusion models, existing methods still lack a unified framework that simultaneously supports arbitrary interaction modes (e.g., text, sketch, mask, and reference image) while ensuring precise editing and faithful preservation of irrelevant attributes. In this work, we introduce a novel paradigm that reformulates hair editing as proxy-based hair transfer. Specifically, we leverage the dense and semantically disentangled latent space of StyleGAN for precise manipulation and exploit its feature space for disentangled attribute preservation, thereby decoupling the objectives of editing and preservation. Our framework unifies different modalities by converting editing conditions into distinct transfer proxies, whose features are seamlessly blended to achieve global or local edits. Beyond 2D, we extend our paradigm to 3D-aware settings by incorporating EG3D and PanoHead, where we propose a multi-view boosted hair feature localization strategy together with 3D-tailored proxy generation methods that exploit the inherent properties of 3D-aware generative models. Extensive experiments demonstrate that our method consistently outperforms prior approaches in editing effects, attribute preservation, visual naturalness, and multi-view consistency, while offering unprecedented support for multimodal and mixed-modal interactions.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "通过代理特征混合统一多模式头发编辑",
        "abstract_cn": "头发编辑是计算机视觉中长期存在的问题，需要细粒度的本地控制和跨多种模式的直观用户交互。尽管 GAN 和扩散模型取得了显着进展，但现有方法仍然缺乏一个统一的框架，能够同时支持任意交互模式（例如文本、草图、掩模和参考图像），同时确保精确编辑和忠实保存不相关属性。在这项工作中，我们引入了一种新颖的范例，将头发编辑重新表述为基于代理的头发转移。具体来说，我们利用 StyleGAN 的密集且语义解缠的潜在空间进行精确操作，并利用其特征空间进行解缠结的属性保存，从而将编辑和保存的目标解耦。我们的框架通过将编辑条件转换为不同的传输代理来统一不同的模式，其功能无缝混合以实现全局或本地编辑。除了 2D 之外，我们还通过合并 EG3D 和 PanoHead 将我们的范例扩展到 3D 感知设置，其中我们提出了多视图增强头发特征定位策略以及利用 3D 感知生成模型的固有属性的 3D 定制代理生成方法。大量的实验表明，我们的方法在编辑效果、属性保留、视觉自然度和多视图一致性方面始终优于先前的方法，同时为多模式和混合模式交互提供前所未有的支持。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656494",
        "title": "First-Order Cross-Domain Meta Learning for Few-Shot Remote Sensing Object Classification",
        "link": "https://doi.org/10.1109/tpami.2026.3656494",
        "published": "2026",
        "author": "Wenda Zhao, Yunxiang Li, Haipeng Wang, Huchuan Lu",
        "summary": "Remote sensing images exhibit intrinsic domain complexity arising from multi-source sensor variances, which heterogeneity fundamentally challenges conventional cross-domain few-shot methods that assume simple distribution shifts. Addressing this, we propose a first-order Cross-Domain Meta Learning (CDML) for few-shot remote sensing object classification. CDML implements a dual-stage domain adaptation task as the fundamental meta-learning unit, and includes a cross-domain meta-train phase (CDMTrain) and a cross-domain meta-test phase (CDMTest). In CDMTrain, we propose an inner-loop multi-domain few-shot task sampling, which enables a teacher model encapsulate both cross-category discriminative features and authentic inter-domain distributional divergence. This alternating cyclic learning paradigm captures genuine domain shifts, with each update direction progressively guiding the model toward parameters that balance multi-domain performance. In CDMTest, we evaluate a domain diversity enhancement by transferring teacher parameters to the student model for cross-domain capability assessment on the reserved pseudo-unseen domain. The task-level design progressively improves domain generalization through iterative domain adaptive task learning. Meanwhile, to mitigate the conflicts and inadequacies caused by multi-domain scenarios, we propose a learnable affine transformation model. It adaptively learns affine transformation parameters through intermediate layer features to fine-tune the update direction. Extensive experiments on five remote sensing classification benchmarks demonstrate a superior performance of the proposed method compared with the state-of-the-art methods. The code will be released at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/lyxdlut/CDML</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "用于少样本遥感对象分类的一阶跨域元学习",
        "abstract_cn": "遥感图像表现出由多源传感器差异引起的固有域复杂性，这种异质性从根本上挑战了假设简单分布变化的传统跨域少样本方法。为了解决这个问题，我们提出了一种用于少样本遥感对象分类的一阶跨域元学习（CDML）。 CDML实现双阶段域适应任务作为基本元学习单元，并包括跨域元训练阶段（CDMTrain）和跨域元测试阶段（CDMTest）。在 CDMTrain 中，我们提出了一种内循环多域少样本任务采样，使教师模型能够封装跨类别判别特征和真实的域间分布散度。这种交替循环学习范式捕获了真正的域变化，每个更新方向都逐步引导模型走向平衡多域性能的参数。在 CDMTest 中，我们通过将教师参数传输到学生模型来评估域多样性增强，以在保留的伪不可见域上进行跨域能力评估。任务级设计通过迭代领域自适应任务学习逐步提高领域泛化能力。同时，为了缓解多领域场景造成的冲突和不足，我们提出了一种可学习的仿射变换模型。它通过中间层特征自适应地学习仿射变换参数来微调更新方向。对五个遥感分类基准的广泛实验表明，与最先进的方法相比，所提出的方法具有优越的性能。该代码将发布于：<uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/lyxdlut/CDML</uri>。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3655896",
        "title": "CAKGE: Context-aware Adaptive Learning for Dynamic Knowledge Graph Embeddings",
        "link": "https://doi.org/10.1109/tpami.2026.3655896",
        "published": "2026",
        "author": "Zongsheng Cao, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, Qingming Huang",
        "summary": "Knowledge graph embeddings (KGE) are effective for representing factual data for numerous applications. However, real-world facts continually evolve, necessitating ongoing updates to knowledge graphs as new information emerges. Under these circumstances, existing KGE models in transductive, inductive, and continual learning settings are prone to catastrophic forgetting or require costly retraining to integrate new information. To address these challenges, we propose a novel model called the <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">C</b>ontext-aware <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</b>daptive learning model for <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">K</b>nowledge <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</b>raph <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">E</b>mbeddings (<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CAKGE</b>). Our model first identifies semantic-relevant entities and uncovers latent relational paths to facilitate the acquisition of new knowledge. To ensure the paths are semantically aligned with the query, we employ a context-aware fusion module, which leverages multiple specialized expert networks to assess and integrate the relevance of these relational paths. Building on this, we introduce an adaptive message aggregation module that incorporates a knowledge replay strategy, enabling the model to integrate both new and existing knowledge efficiently, without retraining the knowledge graph. Additionally, to mitigate catastrophic forgetting, we reformulate the challenge of aligning new with existing knowledge as a graph-matching task using the Fused Gromov-Wasserstein distance, enabling the alignment of old and new knowledge from both semantic and topological perspectives. Furthermore, we provide theoretical guarantees for the expressiveness and reasoning ability of CAKGE, showing that it is the first unified framework tackling transductive, inductive, and continual settings. Extensive experiments show that CAKGE achieves state-of-the-art performance, demonstrating its effectiveness in dynamic KGE modeling.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "CAKGE：动态知识图嵌入的上下文感知自适应学习",
        "abstract_cn": "知识图嵌入（KGE）对于表示众多应用程序的事实数据非常有效。然而，现实世界的事实不断发展，需要随着新信息的出现而不断更新知识图谱。在这些情况下，现有的转导式、归纳式和持续学习环境中的 KGE 模型很容易出现灾难性遗忘，或者需要昂贵的再训练来整合新信息。为了应对这些挑战，我们提出了一种名为 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">C</b>ontext-aware <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" 的新颖模型xmlns:xlink=\"http://www.w3.org/1999/xlink\"><bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">知识的自适应学习模型<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">G</b>raph <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">E</b>嵌入（<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CAKGE</b>）。我们的模型首先识别语义相关的实体并揭示潜在的关系路径以促进新知识的获取。为了确保路径在语义上与查询一致，我们采用了上下文感知融合模块，该模块利用多个专门的专家网络来评估和集成这些关系路径的相关性。在此基础上，我们引入了一个自适应消息聚合模块，该模块结合了知识重放策略，使模型能够有效地集成新知识和现有知识，而无需重新训练知识图。此外，为了减轻灾难性遗忘，我们使用融合的 Gromov-Wasserstein 距离将新知识与现有知识对齐的挑战重新表述为图形匹配任务，从而从语义和拓扑角度实现新旧知识的对齐。此外，我们为 CAKGE 的表达能力和推理能力提供了理论保证，表明它是第一个处理转导、归纳和连续设置的统一框架。大量实验表明 CAKGE 实现了最先进的性能，证明了其在动态 KGE 建模中的有效性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656670",
        "title": "Abstracting Concept-Changing Rules for Solving Raven's Progressive Matrix Problems",
        "link": "https://doi.org/10.1109/tpami.2026.3656670",
        "published": "2026",
        "author": "Fan Shi, Bin Li, Xiangyang Xue",
        "summary": "The abstract visual reasoning ability in human intelligence benefits discovering underlying rules in the novel environment. Raven's Progressive Matrix (RPM) is a classic test to realize such ability in machine intelligence by selecting from candidates. Recent studies suggest that solving RPM in an answer-generation way boosts a more in-depth understanding of rules. However, existing generative solvers cannot discover the global concept-changing rules without auxiliary supervision (e.g., rule annotations and distractors in candidate sets). To this end, we propose a deep latent variable model for <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">C</b>oncept-changing <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">R</b>ule <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">AB</b>straction (CRAB) by learning interpretable concepts and parsing concept-changing rules in the latent space. With the iterative learning process, CRAB can automatically abstract global rules shared on the dataset on each concept and form the learnable prior knowledge of global rules. CRAB outperforms the baselines trained without auxiliary supervision in the arbitrary-position answer generation task and achieves comparable and even higher accuracy than the compared models trained with auxiliary supervision. Finally, we conduct experiments to illustrate the interpretability of CRAB in concept learning, answer selection, and global rule abstraction.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "抽象解决 Raven 渐进矩阵问题的概念改变规则",
        "abstract_cn": "人类智能中的抽象视觉推理能力有利于发现新环境中的潜在规则。 Raven 的渐进矩阵（RPM）是通过从候选者中进行选择来实现机器智能中这种能力的经典测试。最近的研究表明，以答案生成的方式解决 RPM 可以促进对规则的更深入的理解。然而，现有的生成求解器在没有辅助监督的情况下无法发现全局概念改变规则（例如，候选集中的规则注释和干扰项）。为此，我们提出了一个深度潜在变量模型，用于 <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">C</b>oncept-change <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">规则<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">AB</b>牵引（CRAB）通过学习可解释的概念和解析潜在空间中的概念改变规则。通过迭代学习过程，CRAB可以自动抽象出数据集上共享的每个概念的全局规则，形成可学习的全局规则先验知识。 CRAB 在任意位置答案生成任务中优于没有辅助监督训练的基线，并且与使用辅助监督训练的比较模型相比，达到了相当甚至更高的精度。最后，我们进行实验来说明 CRAB 在概念学习、答案选择和全局规则抽象方面的可解释性。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656825",
        "title": "DSNeRF: Dynamic View Synthesis for Ultra-Fast Scenes from Continuous Spike Streams",
        "link": "https://doi.org/10.1109/tpami.2026.3656825",
        "published": "2026",
        "author": "Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang",
        "summary": "Spike cameras generate binary spikes in response to light intensity changes, enabling high-speed visual perception with unprecedented temporal resolution. However, the unique characteristics of spike stream present significant challenges for reconstructing dense 3D scene representations, particularly in dynamic environments and under non-ideal lighting conditions. In this paper, we introduce DSNeRF, the first method to derive a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRF's multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. We propose a novel mapping from pixel rays to the spike domain, integrating the spike generation process directly into NeRF training. Specifically, DSNeRF introduces an integrate-and-fire neuron layer that models non-idealities to capture intrinsic camera noise, including both random and fixed-pattern spike noise, thereby enhancing scene fidelity. Additionally, we propose a motion-guided spiking neuron layer and a long-term rendering photometric loss to better align dynamic spike streams, ensuring accurate scene geometry. Our method optimizes neural radiance fields to render photorealistic novel views from continuous spike streams, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations on both real and simulated sequences validate the effectiveness of our approach. The dataset and source code will be released at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/BIT-Vision/DSNeRF</uri>.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "DSNeRF：来自连续尖峰流的超快速场景的动态视图合成",
        "abstract_cn": "尖峰相机可响应光强度变化而生成二进制尖峰，从而实现具有前所未有的时间分辨率的高速视觉感知。然而，尖峰流的独特特性给重建密集的 3D 场景表示带来了重大挑战，特别是在动态环境和非理想照明条件下。在本文中，我们介绍了 DSNeRF，这是第一种从尖峰相机数据导出基于 NeRF 的体积场景表示的方法。我们的方法利用 NeRF 的多视图一致性来建立强大的自我监督，有效消除错误测量，并在不同的现实世界照明场景中的极其嘈杂的输入中揭示相干结构。我们提出了一种从像素射线到尖峰域的新颖映射，将尖峰生成过程直接集成到 NeRF 训练中。具体来说，DSNeRF 引入了一个集成和激发神经元层，该层对非理想模型进行建模以捕获固有的相机噪声，包括随机和固定模式尖峰噪声，从而增强场景保真度。此外，我们提出了运动引导尖峰神经元层和长期渲染光度损失，以更好地对齐动态尖峰流，确保准确的场景几何形状。我们的方法优化了神经辐射场，从连续的尖峰流中呈现逼真的新颖视图，在某些场景中展示了相对于其他视觉传感器的优势。对真实序列和模拟序列的实证评估验证了我们方法的有效性。数据集和源代码将在 <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/BIT-Vision/DSNeRF</uri> 发布。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3656947",
        "title": "Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena",
        "link": "https://doi.org/10.1109/tpami.2026.3656947",
        "published": "2026",
        "author": "Philip Naumann, Jacob Kauffmann, Grégoire Montavon",
        "summary": "Wasserstein distances provide a powerful framework for comparing data distributions. They can be used to analyze processes over time or to detect inhomogeneities within data. However, simply calculating the Wasserstein distance or analyzing the corresponding transport plan (or coupling) may not be sufficient for understanding what factors contribute to a high or low Wasserstein distance. In this work, we propose a novel solution based on Explainable AI that allows us to efficiently and accurately attribute Wasserstein distances to various data components, including data subgroups, input features, or interpretable subspaces. Our method achieves high accuracy across diverse datasets and Wasserstein distance specifications, and its practical utility is demonstrated in three use cases.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "Wasserstein 距离变得可解释：洞察数据集变化和传输现象",
        "abstract_cn": "Wasserstein 距离为比较数据分布提供了一个强大的框架。它们可用于分析随时间变化的过程或检测数据中的不均匀性。然而，仅仅计算 Wasserstein 距离或分析相应的传输计划（或耦合）可能不足以了解哪些因素导致 Wasserstein 距离高或低。在这项工作中，我们提出了一种基于可解释人工智能的新颖解决方案，使我们能够高效、准确地将 Wasserstein 距离归因于各种数据组件，包括数据子组、输入特征或可解释子空间。我们的方法在不同的数据集和 Wasserstein 距离规范中实现了高精度，并且其实用性在三个用例中得到了证明。"
    },
    {
        "id": "https://doi.org/10.1109/tpami.2026.3657249",
        "title": "Positive Data Augmentation Based on Manifold Heuristic Optimization for Image Classification",
        "link": "https://doi.org/10.1109/tpami.2026.3657249",
        "published": "2026",
        "author": "Fangqing Liu, Han Huang, Fujian Feng, Xueming Yan, Zhifeng Hao",
        "summary": "Data augmentation is crucial for addressing insufficient training data, especially for augmenting positive samples. However, existing methods mostly rely on neural network-based feedback for data augmentation and often overlook the optimization of feature distribution. In this study, we present a practical, distribution-preserving data augmentation pipeline that augments positive samples by optimizing a feature indicator (e.g., two-dimensional entropy), aiming to maintain alignment with the original data distribution. Inspired by the manifold hypothesis, we propose a Manifold Heuristic Optimization Algorithm (MHOA), which augments positive samples by exploring the low-dimensional Euclidean space around object contour pixels instead of the entire decision space. Guided by a “distribution-preservation-first” perspective, our approach explicitly optimizes fidelity to the original data manifold and only retains augmented samples whose feature statistics (e.g., mean, variance) align with the source class. It significantly improves image classification accuracy across neural networks, outperforming state-of-the-art data augmentation methods—especially when the dataset's feature indicator follows a Gaussian distribution. The algorithm's search space, focused on neighborhoods of key feature pixels, is the core driver of its superior performance.",
        "journal": "IEEE Trans. Pattern Anal. Mach. Intell.",
        "title_cn": "基于流形启发式优化的图像分类正数据增强",
        "abstract_cn": "数据增强对于解决训练数据不足的问题至关重要，尤其是增强正样本。然而，现有的方法大多依赖于基于神经网络的反馈来进行数据增强，并且常常忽视特征分布的优化。在本研究中，我们提出了一种实用的、保持分布的数据增强管道，它通过优化特征指标（例如二维熵）来增强正样本，旨在保持与原始数据分布的对齐。受流形假设的启发，我们提出了流形启发式优化算法（MHOA），该算法通过探索对象轮廓像素周围的低维欧几里德空间而不是整个决策空间来增强正样本。在“分布保留第一”的视角指导下，我们的方法显式地优化了原始数据流形的保真度，并且仅保留其特征统计数据（例如均值、方差）与源类一致的增强样本。它显着提高了神经网络的图像分类准确性，优于最先进的数据增强方法，尤其是当数据集的特征指标遵循高斯分布时。该算法的搜索空间专注于关键特征像素的邻域，是其卓越性能的核心驱动力。"
    },
    {
        "id": "https://doi.org/10.1364/josaa.569931",
        "title": "Dynamic display full-space spin-decoupled four-channel metasurface hologram based on photosensitive silicon",
        "link": "https://doi.org/10.1364/josaa.569931",
        "published": "2025-09-23",
        "author": "Jianwen Wu, Yan Li, Kaixiang Cheng, Rui Liu, Dingyi Wang, Yi Liu",
        "summary": "<jats:p>Full-space metasurfaces have attracted much interest due to their ability to simultaneously manipulate the reflection and transmission of incident waves. Manipulating multifunctional devices with greater degrees of freedom is an ongoing trend of development. The realization of full-space multi-channel holographic and its dynamic display in the terahertz region remains a challenge. In this paper, a two-resonator terahertz metasurface based on photosensitive silicon is proposed to achieve integrated reflective–transmissive functionality. The conjugate response of a unit structure consisting of two photosensitive silicon resonators is decoupled by introducing the propagation phase and the geometric phase. Four independent channels can be dynamically displayed simultaneously in full space at the same frequency, with their output modulated by both the polarization of the incident wave and the state of the photosensitive silicon. As an illustrative example, a four-channel dynamic display metasurface is designed to project three-bit polarization-encoded holographic images. In addition, by employing the convolution theorem and the generalized Snell’s law, dynamic control of beam deflection and multi-beam generation is realized for four-channel holographic projections. This work not only proposes a generic strategy for realizing full-space multifunctional metasurfaces, but also has applications in miniaturized optical systems, high-capacity optical encryption and data storage.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于光敏硅的动态显示全空间自旋解耦四通道超表面全息图",
        "abstract_cn": "<jats:p>全空间超表面由于能够同时操纵入射波的反射和传输而引起了人们的广泛兴趣。以更大的自由度操纵多功能设备是持续的发展趋势。全空间多通道全息的实现及其在太赫兹区域的动态显示仍然是一个挑战。在本文中，提出了一种基于光敏硅的双谐振器太赫兹超表面，以实现集成的反射-透射功能。通过引入传播相位和几何相位来解耦由两个光敏硅谐振器组成的单元结构的共轭响应。四个独立通道可以在全空间以相同的频率同时动态显示，其输出由入射波的偏振和光敏硅的状态调制。作为说明性示例，四通道动态显示超表面被设计用于投影三位偏振编码全息图像。此外，利用卷积定理和广义斯涅尔定律，实现了四通道全息投影光束偏转和多光束生成的动态控制。这项工作不仅提出了实现全空间多功能超表面的通用策略，而且在小型化光学系统、大容量光学加密和数据存储方面也有应用。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.569883",
        "title": "Unintended wavefront aberration changes associated with the re-blocking of scleral lenses",
        "link": "https://doi.org/10.1364/josaa.569883",
        "published": "2025-09-25",
        "author": "Nasim Maddah, Alexander W. Schill, Jason D. Marsack",
        "summary": "<jats:p>This study quantifies the unintended changes in wavefront aberrations within a sample of commercial scleral lenses due to the re-blocking step in the patch-cutting method for manufacturing wavefront-guided scleral lenses. The wavefront aberrations for a sample of 15 sphero-cylindrical scleral lenses were measured (Measurement 1) using a wavefront sensor. The lenses were mounted (re-blocked) onto a lens mount, removed (de-blocked) from the lens mount, and again measured (Measurement 2). The lenses were measured a third time (Measurement 3) after a period of storage. The difference in wavefront aberrations between Measurements 1 and 2 served as a treatment condition, and the difference between Measurements 2 and 3 served as a control condition. This study demonstrated that re-blocking of commercial scleral lenses induces small but measurable changes in the aberration structure of the lenses that may be important once aberrations from all steps in the method are considered.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "与巩膜镜重新遮挡相关的意外波前像差变化",
        "abstract_cn": "<jats:p>这项研究量化了商业巩膜镜片样本中波前像差的意外变化，这是由于制造波前引导巩膜镜片的贴片切割方法中的重新阻挡步骤造成的。使用波前传感器测量 15 个球柱巩膜镜片样品的波前像差（测量 1）。将镜头安装（重新阻挡）到镜头安装座上，从镜头安装座上取下（解除阻挡），然后再次测量（测量 2）。存放一段时间后，对镜片进行第三次测量（测量 3）。测量1和2之间的波前像差差作为治疗条件，测量2和3之间的波前像差差作为对照条件。这项研究表明，商业巩膜镜片的重新遮挡会导致镜片的像差结构发生微小但可测量的变化，一旦考虑到该方法中所有步骤的像差，这一变化可能很重要。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.570746",
        "title": "Visual crowding reduces blur sensitivity  in peripheral vision: a study using natural textures in virtual reality",
        "link": "https://doi.org/10.1364/josaa.570746",
        "published": "2025-09-26",
        "author": "Rivo Andriamanalina, Mohamed-Chaker Larabi, Steven Le Moan",
        "summary": "<jats:p>Peripheral vision plays a crucial role in extended reality by supporting immersion, navigation, and spatial awareness beyond the central field of view. However, it also presents perceptual differences compared to central vision, such as reduced visual acuity. One of the most significant challenges is visual crowding, where the presence of nearby elements disrupts the recognition of a peripheral target. While extensively studied in visual neuroscience, the practical implications of crowding for immersive image processing remain largely unexplored. This study investigates how crowding affects the perception of blur in natural textures within an extended reality environment. Participants were asked to detect blur in peripheral texture patches, presented in isolation or surrounded by similar textures. The results confirm that the presence of surrounding textures impairs blur sensitivity, with the strength of this effect depending on the properties of the textures. Specifically, crowding is more pronounced when the target and its surroundings are visually similar or when the central texture can be easily inferred from the surrounding content. By understanding how crowding happens in realistic scenes, this work opens perspectives for perceptually guided rendering and compression strategies in extended reality, where imperceptible degradations can be introduced without compromising visual quality.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "视觉拥挤降低了周边视觉的模糊敏感性：一项在虚拟现实中使用自然纹理的研究",
        "abstract_cn": "<jats:p>周边视觉通过支持中心视野之外的沉浸、导航和空间意识，在扩展现实中发挥着至关重要的作用。然而，与中心视力相比，它也存在感知差异，例如视力下降。最重大的挑战之一是视觉拥挤，附近元素的存在会扰乱对外围目标的识别。尽管在视觉神经科学领域进行了广泛的研究，但拥挤对沉浸式图像处理的实际影响在很大程度上仍未得到探索。这项研究调查了拥挤如何影响扩展现实环境中自然纹理的模糊感知。参与者被要求检测外围纹理斑块中的模糊，这些纹理斑块是单独呈现的或被相似纹理包围的。结果证实，周围纹理的存在会损害模糊灵敏度，这种影响的强度取决于纹理的属性。具体来说，当目标与其周围环境在视觉上相似或可以轻松从周围内容推断出中心纹理时，拥挤更加明显。通过了解现实场景中拥挤是如何发生的，这项工作为扩展现实中的感知引导渲染和压缩策略开辟了前景，其中可以在不影响视觉质量的情况下引入难以察觉的退化。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.575689",
        "title": "Shaping Gaussian modes through truncation and apodization: theory and interpretation",
        "link": "https://doi.org/10.1364/josaa.575689",
        "published": "2025-10-01",
        "author": "Abdelhalim Bencheikh, Sofiane Haddadi",
        "summary": "<jats:p>The transformation of Gaussian beams into structured intensity profiles, such as flat-top and ring-shaped distributions, is a longstanding goal in beam shaping. Conventional methods using spatial light modulators, digital micromirror devices, or interferometry are effective but often bulky and expensive. This paper explores a simpler, low-cost alternative by shaping Gaussian beams through soft and hard truncation. We examine this approach in both Cartesian and cylindrical coordinate systems, covering beam types such as the cosine beam, cosine–Gaussian beam, elegant Hermite–Gaussian beam, truncated cosine beam, and truncated Hermite–Gaussian Beam, along with their cylindrical counterparts: the Bessel beam, Bessel–Gaussian beam, elegant Laguerre–Gaussian beam, truncated Bessel beam, and truncated Laguerre–Gaussian beam. Using mathematical asymptotics and Fourier optics, we provide theoretical insight into how truncation and spatial modulation shape the far-field beam profiles. This framework not only explains the formation of flat-top and ring-shaped beams but also supports the development of compact, passive beam-shaping systems.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "通过截断和变迹塑造高斯模式：理论和解释",
        "abstract_cn": "<jats:p>将高斯光束转换为结构化强度分布（例如平顶和环形分布）是光束整形的长期目标。使用空间光调制器、数字微镜器件或干涉测量的传统方法是有效的，但通常体积庞大且昂贵。本文通过软截断和硬截断来塑造高斯光束，探索了一种更简单、低成本的替代方案。我们在笛卡尔和圆柱坐标系中研究了这种方法，涵盖余弦光束、余弦-高斯光束、优雅埃尔米特-高斯光束、截断余弦光束和截断埃尔米特-高斯光束等光束类型，以及它们的圆柱对应物：贝塞尔光束、贝塞尔-高斯光束、优雅拉盖尔-高斯光束、截断贝塞尔光束和截断贝塞尔光束拉盖尔-高斯光束。利用数学渐近学和傅立叶光学，我们提供了关于截断和空间调制如何塑造远场光束轮廓的理论见解。该框架不仅解释了平顶和环形光束的形成，而且支持紧凑、无源光束整形系统的开发。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573559",
        "title": "Numerical analysis of the resolution limit in microparticle-assisted super-resolution microscopy",
        "link": "https://doi.org/10.1364/josaa.573559",
        "published": "2025-10-01",
        "author": "Arlen R. Bekirov, Zengbo Wang, Nina A. Lystseva, Boris S. Luk’yanchuk, Andrey A. Fedyanin",
        "summary": "<jats:p>Visualization in the virtual image formed by dielectric microparticles has been shown to enable the distinction of objects that remain indistinguishable under direct observation. We perform the resolution analysis based on a full two-dimensional simulation in the TE mode of optical image formation, taking into account the diffraction of partially coherent light on the microparticle and the objects under study. The oscillating nature of optical resolution is demonstrated depending on the size of the microparticle. The presence of strong resonances is observed in both transmission and reflection modes. It is shown that as the size of the object decreases, the optical resolution tends to the classical limit. An analytical estimate for the resolution criterion is presented.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "微粒辅助超分辨率显微镜分辨率极限的数值分析",
        "abstract_cn": "<jats:p>由介电微粒形成的虚拟图像的可视化已被证明可以区分直接观察下无法区分的物体。我们基于光学成像 TE 模式下的全二维模拟进行分辨率分析，同时考虑了部分相干光在微粒和所研究物体上的衍射。光学分辨率的振荡性质取决于微粒的尺寸。在透射和反射模式下都观察到强共振的存在。结果表明，随着物体尺寸的减小，光学分辨率趋于经典极限。提出了分辨率标准的分析估计。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.575219",
        "title": "Characteristics and application research of partially coherent array beams in laser display systems",
        "link": "https://doi.org/10.1364/josaa.575219",
        "published": "2025-10-01",
        "author": "Chenkun Mi, Chunhao Liang, Zhaohui Yang, Kun Tang, Xuan Zhao, Hongxin Liu, Yulong Liu",
        "summary": "<jats:p>A partially coherent rectangular symmetric array beam for the light source of the laser display system is proposed. The speckle of the laser display system is simulated theoretically by using the angular spectrum method combined with the random phase screen, and the experimental results are given. It is found that unlike traditional lasers, with the increase of the brightness of the laser display system, the speckle contrast does not increase significantly, i.e., the image quality does not decline. The purpose of increasing the brightness of the laser display system without affecting the image quality is achieved by either increasing the number of arrays or increasing the output of laser. The array offset does not affect speckle contrast. Similar to the traditional partially coherent beam, the speckle of the laser display system can be effectively suppressed by reducing the spatial coherence of the partially coherent array beam while keeping the number of arrays unchanged. As a light source, partially coherent array beams provide a novel means to resolve the contradiction between the increase in brightness leading to an increase in speckle contrast and affecting imaging quality, which is different from the traditional methods, and will have a good application prospect in large-size laser display systems or outdoor laser display systems.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "部分相干阵列光束在激光显示系统中的特性及应用研究",
        "abstract_cn": "<jats:p>提出了一种用于激光显示系统光源的部分相干矩形对称阵列光束。采用角谱法结合随机相位屏对激光显示系统的散斑进行了理论模拟，并给出了实验结果。研究发现，与传统激光器不同，随着激光显示系统亮度的增加，散斑对比度并没有明显增加，即图像质量没有下降。在不影响图像质量的情况下提高激光显示系统的亮度的目的是通过增加阵列数量或增加激光的输出来达到的。阵列偏移不影响散斑对比度。与传统的部分相干光束类似，在保持阵列数量不变的情况下，通过降低部分相干阵列光束的空间相干性，可以有效抑制激光显示系统的散斑。部分相干阵列光束作为光源，为解决亮度增加导致散斑对比度增加与影响成像质量之间的矛盾提供了一种不同于传统方法的新颖手段，在大尺寸激光显示系统或户外激光显示系统中将具有良好的应用前景。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.572879",
        "title": "Optical system design of an elliptical-hyperbolic lens set for semiconductor laser beam shaping and collimating a hollow beam",
        "link": "https://doi.org/10.1364/josaa.572879",
        "published": "2025-10-06",
        "author": "Yuhang Zhang, Jiaoyu Sun, Nuo Chen, Chujie Wu, Dongyin Wang, Xunhong Chen, Ping Jiang, Huajun Yang",
        "summary": "<jats:p>In order to solve the problem of energy loss at the center of the Cassegrain antenna, an optical system based on an elliptical-hyperbolic lens set is innovatively designed in this paper, which can shape and convert the elliptical divergent beam emitted from a semiconductor laser into a collimated hollow beam. The system consists of a lens set for collimating and shaping (C-S lens set) the laser beam and a lens set for the generation of a collimated hollow (C-H lens set) beam: the C-S lens set is responsible for shaping the elliptically divergent beam into a collimated circular beam, and the C-H lens set is used to generate a collimated hollow beam. The simulation results show that when the output beam diameter is 17.33 mm, the C-S lens set reduces the ratio of beam waist (RBW) from 1.554 to 1.014, and the standard deviation of the edge beam radius samples (SDRS) decreases from 4.227 to 0.0866 mm. The transmission efficiency of the optical system is 92.28% at 1550 nm, and the broad-spectrum performance is maintained after taking into account the practical loss factors.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "用于半导体激光光束整形和准直空心光束的椭圆双曲透镜组的光学系统设计",
        "abstract_cn": "<jats:p>为了解决卡塞格伦天线中心的能量损失问题，本文创新设计了一种基于椭圆双曲透镜组的光学系统，可以将半导体激光器发出的椭圆发散光束整形并转换为准直空心光束。该系统由用于准直和整形激光束的透镜组（C-S透镜组）和用于产生准直空心光束的透镜组（C-H透镜组）组成：C-S透镜组负责将椭圆发散光束整形为准直圆形光束，C-H透镜组用于产生准直空心光束。仿真结果表明，当输出光束直径为17.33 mm时，C-S透镜组将光束腰比（RBW）从1.554减小到1.014，边缘光束半径样本的标准偏差（SDRS）从4.227减小到0.0866 mm。光学系统在1550 nm处的传输效率为92.28%，在考虑实际损耗因素后保持了广谱性能。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.572970",
        "title": "Correlations of the phase fluctuations in the presence of weak scintillations",
        "link": "https://doi.org/10.1364/josaa.572970",
        "published": "2025-10-03",
        "author": "I. V. Kolokolov, V. V. Lebedev, F. A. Starikov",
        "summary": "<jats:p>We investigate the pair correlation function of the random phase component of light waves propagating through turbulent media. Specifically, we focus on the correlation function of phase gradients, considering the case of Gaussian initial profiles, with special attention paid to plane and spherical waves. The calculations are carried out within the framework of the principal order of perturbation theory.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "存在弱闪烁时相位波动的相关性",
        "abstract_cn": "<jats:p>我们研究了通过湍流介质传播的光波的随机相位分量的配对相关函数。具体来说，我们关注相位梯度的相关函数，考虑高斯初始轮廓的情况，特别关注平面波和球面波。计算是在主阶微扰理论的框架内进行的。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574772",
        "title": "Research for a type of aspheric E-H hollow beam shaping and fiber-coupled defocus characteristics",
        "link": "https://doi.org/10.1364/josaa.574772",
        "published": "2025-10-06",
        "author": "Jiaoyu Sun, Yuhang Zhang, Shunda Wang, Yan Zhang, Xunhong Chen, Ping Jiang, Huajun Yang",
        "summary": "<jats:p>The integration of space optical communication and fiber optical system plays a vital role in satellite communications and deep space exploration. However, achieving efficient fiber coupling under defocus conditions remains a key challenge. Consequently, a new, to our knowledge, aspheric shaping lens set composed of elliptical and hyperboloidal (E-H) surfaces is proposed in this study. This E-H lens set transforms the hollow beam emitted from the receiving end of a Cassegrain antenna into a solid beam, thereby reducing the loss of central energy inherent in hollow beam transmission. An optical system incorporating the Cassegrain antenna and the aspheric lens set is designed, with the lens surface equations derived based on the principle of equal optical path. Furthermore, considering chamfer design and Fresnel reflection loss, a new, to our knowledge, hyperbolic Fresnel (H-F) coupling lens is researched. This system achieves a coupling efficiency of 79.55% for a fiber mode field radius of 5 µm, with fiber defocus ranges of 2.36 µm (lateral offset), 68.34 µm (longitudinal offset), and 47.24 mrad (angular jitter). Importantly, within these defocus ranges, offsets of the optical fiber receiver do not cause a significant variation in the coupling efficiency of the optical communication system.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "一种非球面E-H空心光束整形及光纤耦合离焦特性研究",
        "abstract_cn": "<jats:p>空间光通信与光纤系统的融合在卫星通信和深空探测中发挥着至关重要的作用。然而，在散焦条件下实现有效的光纤耦合仍然是一个关键挑战。因此，据我们所知，本研究提出了一种由椭圆形和双曲面（E-H）表面组成的新型非球面成形透镜组。该E-H透镜组将从卡塞格伦天线接收端发射的空心光束转变为实心光束，从而减少空心光束传输中固有的中心能量的损失。设计了卡塞格伦天线和非球面透镜组的光学系统，并根据等光程原理推导了透镜表面方程。此外，考虑倒角设计和菲涅尔反射损耗，据我们所知，研究了一种新的双曲菲涅尔（H-F）耦合透镜。该系统在光纤模场半径为 5 µm 时实现了 79.55% 的耦合效率，光纤离焦范围为 2.36 µm（横向偏移）、68.34 µm（纵向偏移）和 47.24 mrad（角度抖动）。重要的是，在这些离焦范围内，光纤接收器的偏移不会导致光通信系统耦合效率的显着变化。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573032",
        "title": "Topological charge recognition of vortex beams based on a convolutional neural network",
        "link": "https://doi.org/10.1364/josaa.573032",
        "published": "2025-10-02",
        "author": "Tengfei Chai, Xiaoyun Liu, Hongwei Wang, Yumeihui Jin, Yueqiu Jiang",
        "summary": "<jats:p>The application of vortex beams in optical communications is significantly constrained by wavefront distortions caused by atmospheric turbulence and system-induced aberrations. In high-energy laser systems, spherical and coma aberrations are commonly introduced due to thermal effects during manufacturing or beam propagation. To address the challenge of topological charge identification under such complex distortions, this paper proposes an enhanced IResNet18 model capable of simultaneously predicting the topological charge, spherical aberration coefficient, and coma aberration coefficient from distorted intensity patterns. Three experimental configurations are designed: vortex beams with spherical aberration, vortex beams with coma aberration, and vortex beams with combined spherical and coma aberrations. This study systematically investigates the impact of varying propagation distances and turbulence intensities on model performance. Results demonstrate that the proposed model achieves superior accuracy and training efficiency compared to existing models. These findings provide a robust framework for aberration-aware vortex beam recognition and offer valuable insights for enhancing the reliability of free-space optical communication systems.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于卷积神经网络的涡旋光束拓扑电荷识别",
        "abstract_cn": "<jats:p>涡旋光束在光通信中的应用受到大气湍流和系统引起的像差引起的波前畸变的严重限制。在高能激光系统中，由于制造或光束传播过程中的热效应，通常会引入球差和彗形像差。为了解决这种复杂畸变下拓扑电荷识别的挑战，本文提出了一种增强的IResNet18模型，能够根据畸变强度模式同时预测拓扑电荷、球面像差系数和彗形像差系数。设计了三种实验配置：具有球差的涡旋光束、具有彗差的涡旋光束以及具有组合球差和彗差的涡旋光束。这项研究系统地研究了不同的传播距离和湍流强度对模型性能的影响。结果表明，与现有模型相比，所提出的模型具有更高的准确性和训练效率。这些发现为像差感知涡旋光束识别提供了一个强大的框架，并为提高自由空间光通信系统的可靠性提供了宝贵的见解。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574858",
        "title": "Hybrid defocus sensor with an enhanced dynamic range",
        "link": "https://doi.org/10.1364/josaa.574858",
        "published": "2025-10-03",
        "author": "Santanu Konwar, Nimish Gupta",
        "summary": "<jats:p>Defocus is a major optical aberration affecting imaging and beam propagation across various fields. While existing measurement techniques often require complex wavefront reconstruction, modal wavefront sensors (MWSs) and astigmatic defocus sensors (ADSs) offer simpler, intensity-based methods but suffer from limited dynamic range. This study proposes a novel defocus sensing technique that combines MWS and ADS principles, using intensity measurements at just two defocus planes. The method enables fast, straightforward implementation with a significantly enhanced dynamic range. Theoretical analysis, supported by simulations, demonstrates its advantages over conventional sensors. An implementation strategy using computer-generated holography is also presented, with experimental and simulation results aligning well with theory.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "具有增强动态范围的混合散焦传感器",
        "abstract_cn": "<jats:p>散焦是影响各个领域的成像和光束传播的主要光学像差。虽然现有的测量技术通常需要复杂的波前重建，但模态波前传感器 (MWS) 和像散散焦传感器 (ADS) 提供更简单、基于强度的方法，但动态范围有限。这项研究提出了一种新颖的散焦传感技术，该技术结合了 MWS 和 ADS 原理，仅在两个散焦平面上使用强度测量。该方法能够快速、直接地实现，并显着增强动态范围。模拟支持的理论分析证明了其相对于传统传感器的优势。还提出了一种使用计算机生成全息术的实施策略，实验和模拟结果与理论非常吻合。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.571923",
        "title": "Three-dimensional Scheimpflug principle and its application to full-focus positioning",
        "link": "https://doi.org/10.1364/josaa.571923",
        "published": "2025-10-08",
        "author": "Kazuyuki Kobayashi, Momoka Ichikawa, Kazuki Nishi",
        "summary": "<jats:p>The Scheimpflug principle specifies the relationship between the object, lens, and image planes, providing conditions for focusing the entire image plane even for tilted objects. This classical principle is confined to unidirectional tilting and has not been extended to camera systems with two-axis tilts. Herein, we present an extension of the Scheimpflug principle to three dimensions, demonstrating that images can be decomposed and focused independently in the vertical and horizontal directions. This simplifies focusing on the overall image, which requires a two-dimensional search, into a combination of one-dimensional searches. The proposed theory was validated through practical experimentation using cameras.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "三维Scheimpflug原理及其在全焦点定位中的应用",
        "abstract_cn": "<jats:p>Scheimpflug 原理指定了物体、镜头和图像平面之间的关系，为即使对于倾斜的物体也能聚焦整个图像平面提供了条件。这一经典原理仅限于单向倾斜，尚未扩展到具有两轴倾斜的相机系统。在这里，我们将 Scheimpflug 原理扩展到三个维度，证明图像可以在垂直和水平方向上独立分解和聚焦。这将需要二维搜索的对整个图像的关注简化为一维搜索的组合。所提出的理论通过使用相机的实际实验得到了验证。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.570928",
        "title": "Spiral adaptive Fresnel lens: wave optics",
        "link": "https://doi.org/10.1364/josaa.570928",
        "published": "2025-10-06",
        "author": "Maik Locher, Elise Blackmore, Zhao-Kai Wang, Johannes Courtial",
        "summary": "<jats:p>We recently introduced adaptive Fresnel lenses (AFLs) based on complementary cylindrical lenses “bent into” spirals. Here, we start to investigate the wave optics of such components. We calculate typical point-spread functions and investigate diffraction effects in the case of narrow windings. We design a modification that results in the phase profile for light that passes through corresponding windings being that of a parabolic lens, exactly, but only in the case of infinitesimally thin phase components. Our findings contribute to the improvement and understanding of spiral AFLs.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "螺旋自适应菲涅耳透镜：波动光学",
        "abstract_cn": "<jats:p>我们最近推出了基于“弯曲成”螺旋形的互补柱面透镜的自适应菲涅尔透镜 (AFL)。在这里，我们开始研究此类组件的波动光学。我们计算典型的点扩散函数并研究窄绕组情况下的衍射效应。我们设计了一种修改，使穿过相应绕组的光的相位轮廓准确地成为抛物面透镜的相位轮廓，但仅限于相位分量无限小的情况。我们的研究结果有助于改进和理解螺旋 AFL。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.571895",
        "title": "MSSPUNet: phase unwrapping using a multi-scale, multi-stage deep neural network for digital holographic tomography",
        "link": "https://doi.org/10.1364/josaa.571895",
        "published": "2025-10-10",
        "author": "Yaoqing Xie, Youxing Li, Hongwei Li, Libo Yuan",
        "summary": "<jats:p>Digital holographic tomography (DHT) is an advanced phase-imaging-based measurement technique widely used for 3D reconstruction. However, the generated phase images often suffer from significant noise interference and irregular distortions, posing challenges for accurate reconstruction. Phase unwrapping, a critical preprocessing step for 3D reconstruction in holographic tomography, is essential to correct phase discontinuities. Traditional phase unwrapping methods frequently lack the robustness and reliability required for practical applications. To address these limitations, we explore deep learning approaches and identify that existing frameworks predominantly rely on single-stage methods, which suffer from inadequate multi-scale feature fusion and a lack of phase continuity constraints, hindering high-precision cross-scale phase unwrapping. To overcome these challenges, we propose MSSPUNet, a multi-scale, multi-stage transformer network, which leverages latent features across different scales to enhance cross-scale feature fusion. This approach achieves synergistic optimization in noise suppression, phase jump correction, and detail preservation. The network was trained on extensive simulated datasets and benchmarked against several existing phase unwrapping methods. Furthermore, we validated its performance using real DHT images of cells, organoids, phantoms, and conventional 3D-printed structures. Experimental results demonstrate that MSSPUNet offers superior accuracy, enhanced robustness, and stronger generalization capabilities compared to existing methods.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "MSSPUNet：使用多尺度、多级深度神经网络进行数字全息断层扫描的相位展开",
        "abstract_cn": "<jats:p>数字全息断层扫描 (DHT) 是一种先进的基于相位成像的测量技术，广泛用于 3D 重建。然而，生成的相位图像经常受到明显的噪声干扰和不规则失真，给精确重建带来了挑战。相位展开是全息断层扫描 3D 重建的关键预处理步骤，对于纠正相位不连续性至关重要。传统的相位展开方法常常缺乏实际应用所需的鲁棒性和可靠性。为了解决这些局限性，我们探索了深度学习方法，并发现现有框架主要依赖于单阶段方法，其多尺度特征融合不足且缺乏相位连续性约束，阻碍了高精度跨尺度相位展开。为了克服这些挑战，我们提出了 MSSPUNet，一种多尺度、多级变压器网络，它利用不同尺度的潜在特征来增强跨尺度特征融合。该方法实现了噪声抑制、相位跳变校正和细节保留的协同优化。该网络接受了广泛的模拟数据集的训练，并针对几种现有的相位展开方法进行了基准测试。此外，我们使用细胞、类器官、模型和传统 3D 打印结构的真实 DHT 图像验证了其性能。实验结果表明，与现有方法相比，MSSPUNet 具有更高的准确性、更强的鲁棒性和更强的泛化能力。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573744",
        "title": "Time-domain modeling of finite coherence in resonance-based spectroscopic sensing",
        "link": "https://doi.org/10.1364/josaa.573744",
        "published": "2025-10-13",
        "author": "Mohammad Hossein Motavas, Mohamed Najih, Andrew G. Kirk",
        "summary": "<jats:p>Optical cavities serve as powerful tools for sensing experiments, often relying on narrow-linewidth laser sources to minimize the impact of optical coherence on measurements. However, demands for affordable integrated and miniaturized sensing platforms in point-of-care diagnostics, environmental monitoring, and similar applications motivate switching to sources with broader linewidths suitable for both monolithic and heterogeneous integrations. Time-domain measurement techniques such as cavity ring-down spectroscopy (CRDS) are widely used in conjunction with optical cavities, but to date there has been no universal model that quantifies the impact of partial coherence on the cavity temporal transfer function. We apply a linear systems theory approach to develop a closed-form analytic model for cavity-based sensing that quantifies the influence of source bandwidth (i.e., temporal coherence) on spectroscopic measurements in the time domain. This approach can be applied to a variety of cavity-based spectroscopies. In this study, cavity-enhanced absorption spectroscopy (CEAS) and CRDS paradigms have been examined using standing- and traveling-wave resonator examples. Results show that although increased cavity loss is the primary factor reducing output power and photon lifetime in both CEAS and CRDS, broader source linewidths can also influence cavity buildup and transmission and must be accounted for when modeling the system response. The model is consistent with known results in the literature and provides a framework for evaluating source detuning and coherence effects on cavity dynamics.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于共振的光谱传感中有限相干性的时域建模",
        "abstract_cn": "<jats:p>光学腔是传感实验的强大工具，通常依靠窄线宽激光源来最大限度地减少光学相干性对测量的影响。然而，护理点诊断、环境监测和类似应用对经济实惠的集成和小型化传感平台的需求促使人们转向适用于单片和异构集成的更宽线宽的信号源。腔衰荡光谱 (CRDS) 等时域测量技术广泛与光腔结合使用，但迄今为止，还没有量化部分相干性对腔时间传递函数影响的通用模型。我们应用线性系统理论方法来开发基于腔的传感的封闭式分析模型，该模型量化源带宽（即时间相干性）对时域光谱测量的影响。该方法可应用于各种基于腔的光谱学。在本研究中，使用驻波和行波谐振器示例对腔增强吸收光谱 (CEAS) 和 CRDS 范例进行了检查。结果表明，尽管腔损耗增加是 CEAS 和 CRDS 中输出功率和光子寿命降低的主要因素，但更宽的源线宽也会影响腔的形成和传输，在对系统响应进行建模时必须考虑到这一点。该模型与文献中的已知结果一致，并提供了评估源失谐和相干性对腔动力学影响的框架。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.570722",
        "title": "Spiral adaptive Fresnel lens: generalizations, improvements, and augmented-reality/virtual-reality simulations",
        "link": "https://doi.org/10.1364/josaa.570722",
        "published": "2025-10-06",
        "author": "Maik Locher, Di Wu, Johannes Courtial",
        "summary": "<jats:p>Two complementary cylindrical lenses, each bent into a spiral, can form the components of an adaptive Fresnel lens whose focusing power can be tuned by rotating the two components relative to each other [Armstrong <jats:italic toggle=\"yes\">et al</jats:italic>., <jats:mixed-citation publication-type=\"journal\"><jats:source>J. Opt. Soc. Am. A</jats:source> <jats:volume>42</jats:volume>, <jats:fpage>211</jats:fpage> (<jats:year>2025</jats:year>)<jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id><jats:issn>0740-3232</jats:issn><jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.540585</jats:pub-id></jats:mixed-citation>]. Corresponding windings of the cylindrical-lens spirals form the windings of the resulting adaptive Fresnel lens, and the addition of an Alvarez–Lohmann lens to the cylindrical lenses can improve the device through winding focusing—ensuring that each winding has the same focusing power as the Fresnel lens. Here we extend the type of spiral from logarithmic to Archimedean and hyperbolic; we introduce an alternative type of winding focusing by varying the separation between the two components; and we show how to design our adaptive Fresnel lenses so that, unlike the original design, they work best around non-zero focusing power. Finally, we present <jats:italic toggle=\"yes\">SpiralFresnelFrenzy</jats:italic>, a web app that allows interactive and immersive raytracing simulations of the view through adaptive Fresnel lenses using augmented reality (AR) or virtual reality (VR). Our work significantly generalizes and improves spiral adaptive Fresnel lenses.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "螺旋自适应菲涅尔透镜：概括、改进和增强现实/虚拟现实模拟",
        "abstract_cn": "<jats:p>两个互补的柱面透镜，每个都弯曲成螺旋形，可以形成自适应菲涅尔透镜的组件，其聚焦能力可以通过相对于彼此旋转两个组件来调节[Armstrong <jats:italictoggle=\"yes\">等人</jats:italic>., <jats:mixed-itationpublication-type=\"journal\"><jats:source>J. 选择。苏克。是。 A</jats:source> <jats:volume>42</jats:volume>, <jats:fpage>211</jats:fpage> (<jats:year>2025</jats:year>)<jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id><jats:issn>0740-3232</jats:issn><jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.540585</jats:pub-id></jats:mixed-itation>]。相应的柱面透镜螺旋线圈形成了自适应菲涅尔透镜的线圈，并且在柱面透镜上添加阿尔瓦雷斯-洛曼透镜可以通过卷绕聚焦来改进该装置，确保每个卷绕具有与菲涅尔透镜相同的聚焦能力。在这里，我们将螺旋类型从对数扩展到阿基米德和双曲；我们通过改变两个组件之间的间距引入了另一种类型的缠绕聚焦；我们展示了如何设计我们的自适应菲涅尔透镜，以便与原始设计不同，它们在非零聚焦能力下工作得最好。最后，我们推出了 <jats:italic Switching=\"yes\">SpiralFresnelFrenzy</jats:italic>，这是一款 Web 应用程序，允许使用增强现实 (AR) 或虚拟现实 (VR) 通过自适应菲涅耳透镜对视图进行交互式和沉浸式光线追踪模拟。我们的工作显着概括并改进了螺旋自适应菲涅尔透镜。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573725",
        "title": "Convolutional-neural-network-assisted parameter identification in elliptical Airy vortex beams",
        "link": "https://doi.org/10.1364/josaa.573725",
        "published": "2025-10-13",
        "author": "Bingsong Cao, Zhangrong Mei, Yonghua Mao, Peizhen Qiu, Kaikai Huang",
        "summary": "<jats:p>Convolutional neural networks (CNNs), widely employed in image recognition, show significant promise in identifying topological charges of vortex beams. Elliptical Airy vortex beams (EAVBs) introduce an additional elliptical parameter <jats:italic>t</jats:italic>, expand the dimensionality of Airy vortex beams, and thus offer an opportunity to increase the capacity for orbital angular momentum communication. In this work, we develop a compact CNN architecture to classify key parameters of EAVBs: the topological charge <jats:italic>m</jats:italic> and the elliptical parameter <jats:italic>t</jats:italic>. Trained on physics-augmented datasets that combine simulated and experimental autofocusing intensity patterns, the network achieves over 99.80% accuracy on a standard test set across three independent training runs. Its robustness is further validated on a set of unseen experimental patterns: it accurately recognizes all unaugmented new patterns and maintains consistent performance (98.24%–100%) on augmented variations. These results lay the groundwork for EAVB-based optical communications enhanced by machine learning, providing large capacity and high dimensionality to meet growing bandwidth requirements.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "椭圆艾里涡旋光束中的卷积神经网络辅助参数识别",
        "abstract_cn": "<jats:p>广泛应用于图像识别的卷积神经网络 (CNN) 在识别涡旋光束的拓扑电荷方面显示出巨大的前景。椭圆艾里涡旋光束（EAVB）引入了一个额外的椭圆参数<jats:italic>t</jat:italic>，扩展了艾里涡旋光束的维数，从而提供了提高轨道角动量通信能力的机会。在这项工作中，我们开发了一个紧凑的 CNN 架构来对 EAVB 的关键参数进行分类：拓扑电荷 <jats:italic>m</jats:italic> 和椭圆参数 <jats:italic>t</jats:italic>。该网络在结合了模拟和实验自动聚焦强度模式的物理增强数据集上进行训练，在三个独立的训练运行中在标准测试集上实现了超过 99.80% 的准确率。它的鲁棒性在一组看不见的实验模式上得到了进一步验证：它准确地识别所有未增强的新模式，并在增强的变化上保持一致的性能（98.24％–100％）。这些结果为通过机器学习增强的基于 EAVB 的光通信奠定了基础，提供大容量和高维度以满足不断增长的带宽需求。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574611",
        "title": "Systematic framework for the optimization and validation of a compact optical system using commercial off-the-shelf components",
        "link": "https://doi.org/10.1364/josaa.574611",
        "published": "2025-10-10",
        "author": "Hong-Lin Huang, Chien-Fang Ding",
        "summary": "<jats:p>This study presents and validates a systematic design framework for developing high-performance, low-magnification, compact optical systems composed of commercial off-the-shelf (COTS) components, specifically tailored for industrial applications under stringent spatial constraints. The efficacy of this methodology is demonstrated through the development of a machine vision system for the precision alignment of semiconductor probe cards. The design process is initiated by selecting the Cooke triplet as the foundational architecture, a fundamental anastigmatic structure capable of simultaneously correcting all primary aberrations. Subsequently, through a simulation-driven iterative optimization process, supplementary COTS lenses are strategically incorporated to achieve demanding performance specifications while adhering to a compact total track length of less than 50 mm. A rigorous tolerance analysis was performed to ensure the manufacturability of the design and to forecast the production yield. Following the finalization of the optical design, the corresponding optomechanical components were developed, and a prototype was assembled. The performance of the prototype was experimentally validated using a series of metrological standards, including a microscope stage micrometer, a USAF 1951 resolution test chart, and a grid distortion target. The empirical results exhibit a high degree of correlation with the predictions from a comprehensive system model that incorporates sensor effects, thereby validating the efficacy and predictive fidelity of the proposed framework. This methodology offers a robust and efficient pathway for creating high-performance, cost-effective optical solutions within spatially constrained industrial environments.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用商用现成组件优化和验证紧凑型光学系统的系统框架",
        "abstract_cn": "<jats:p>这项研究提出并验证了一个系统设计框架，用于开发由商用现成 (COTS) 组件组成的高性能、低放大倍数、紧凑型光学系统，专为严格空间限制下的工业应用量身定制。通过开发用于半导体探针卡精确对准的机器视觉系统，证明了该方法的有效性。设计过程首先选择库克三重态作为基础架构，这是一种能够同时校正所有主要像差的基本非像散结构。随后，通过仿真驱动的迭代优化过程，战略性地合并辅助 COTS 镜头，以实现苛刻的性能规格，同时保持小于 50 毫米的紧凑总轨道长度。进行了严格的公差分析，以确保设计的可制造性并预测生产良率。光学设计完成后，开发了相应的光机械部件，并组装了原型机。使用一系列计量标准对原型的性能进行了实验验证，包括显微镜级千分尺、USAF 1951 分辨率测试图和网格畸变目标。实证结果与包含传感器效应的综合系统模型的预测表现出高度相关性，从而验证了所提出框架的有效性和预测保真度。这种方法为在空间受限的工业环境中创建高性能、经济高效的光学解决方案提供了强大而有效的途径。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573454",
        "title": "High-uniformity panoramic infrared optical system design based on segment-wise dynamic curvature and gradient descent optimization",
        "link": "https://doi.org/10.1364/josaa.573454",
        "published": "2025-10-16",
        "author": "Zhen Zhang, Yunhan Huang, Wangxuan Sun, Zezheng Li, Qi Ju, Yu Ye, Zhiying Liu",
        "summary": "<jats:p>A new, to our knowledge, design method for panoramic reflective\n\t\t\t\t\tinfrared systems is proposed using dynamic curvature compensation and\n\t\t\t\t\tgradient descent to improve image irradiance uniformity. By combining\n\t\t\t\t\tray tracing and multi-parameter analysis, the reflective surface is\n\t\t\t\t\treplaced with segmented arcs, and the design is iteratively expanded.\n\t\t\t\t\tThis concise and effective method achieves 99.19% irradiance\n\t\t\t\t\tuniformity with accuracy five times higher than traditional methods.\n\t\t\t\t\tThe design time is only 0.17 s, and one-step forming eliminates the\n\t\t\t\t\tneed for secondary optimization, greatly simplifying the system design\n\t\t\t\t\tprocess. This provides new ideas for high-resolution, wide-field\n\t\t\t\t\tinfrared imaging system design.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于分段动态曲率和梯度下降优化的高均匀性全景红外光学系统设计",
        "abstract_cn": "<jats:p>据我们所知，一种新的全景反射设计方法\n\t\t\t\t\t红外系统被提出使用动态曲率补偿和\n\t\t\t\t\t梯度下降以提高图像辐照度均匀性。通过结合\n\t\t\t\t\t光线追踪和多参数分析，反射面为\n\t\t\t\t\t替换为分段圆弧，迭代扩展设计。\n\t\t\t\t\t这种简洁有效的方法可实现 99.19% 的辐照度\n\t\t\t\t\t均匀性和精度比传统方法高五倍。\n\t\t\t\t\t设计时间仅为0.17秒，一步成型消除了\n\t\t\t\t\t无需二次优化，大大简化系统设计\n\t\t\t\t\t过程。这为高分辨率、宽视场提供了新思路\n\t\t\t\t\t红外成像系统设计。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.572650",
        "title": "Phase screen generation method for general oceanic turbulence",
        "link": "https://doi.org/10.1364/josaa.572650",
        "published": "2025-10-16",
        "author": "Guangqing Liu, Xiang Yi, Mingjian Cheng",
        "summary": "<jats:p>Optical wave propagation through oceanic turbulence suffers from phase and amplitude distortions caused by refractive-index fluctuations due to temperature and salinity random variations in seawater. The split-step method has been widely employed for numerical simulation of light propagation in turbulent media, where the propagation path is discretized into multiple segments. This discretization ensures that each segment is sufficiently short such that refractive-index variations induce only phase distortions without affecting the amplitude. A critical component of this method involves accurate generation of thin phase screens (PSs) based on the refractive-index spatial power-law spectrum (RISPS). This study utilizes the general oceanic turbulence RISPS to create PSs. We show that optimal accuracy requires a hybrid implementation combining PWD (proposed by Paulson, Wu, and Davis) frequency-domain approach with Zernike polynomial-based spatial-domain decomposition.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "一般海洋湍流的相位屏生成方法",
        "abstract_cn": "<jats:p>通过海洋湍流传播的光波会遭受由于海水中温度和盐度随机变化导致的折射率波动引起的相位和幅度失真。分步法已广泛应用于湍流介质中光传播的数值模拟，其中传播路径被离散成多段。这种离散化确保每个段足够短，使得折射率变化仅引起相位失真而不影响幅度。该方法的一个关键组成部分涉及基于折射率空间幂律谱 (RISPS) 的薄相位屏幕 (PS) 的精确生成。本研究利用一般海洋湍流 RISPS 来创建 PS。我们表明，最佳精度需要将 PWD（由 Paulson、Wu 和 Davis 提出）频域方法与基于 Zernike 多项式的空间域分解相结合的混合实现。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574861",
        "title": "Beam moments of higher-order annular Gaussian beams in tissue turbulence",
        "link": "https://doi.org/10.1364/josaa.574861",
        "published": "2025-10-13",
        "author": "Serap Altay Arpali, Çağlar Arpali, Yahya Baykal",
        "summary": "<jats:p>Beam moments of the laser beam at the receiver plane were analyzed using our previously developed formula for the average light intensity of a higher-order annular Gaussian (HOAG) beam in the presence of biological tissue turbulence. HOAG beam moments are examined for the entities of power-in-the-bucket (PIB) and kurtosis across various tissue types such as the upper dermis (human), liver parenchyma (mouse), intestinal epithelium (mouse), and deep dermis (mouse). Moreover, beam moments are explored considering factors like the strength coefficient of the refractive-index fluctuations and the propagation distance. The PIB values for all HOAG beam modes are found to decrease exponentially and steadily, behaving similar to Gaussian beams as tissue length increases. As turbulence intensity increases, higher-order HOAG beam modes transfer optical energy to the receiver more efficiently than the lower order modes. Kurtosis analysis shows that at intermediate distances, the beam energy is distributed toward the edges, while at longer distances, the energy concentration is lower at the edges than at the center. This trend is reflected in increasing kurtosis values across all HOAG modes and tissue types. Considering the changes in PIB and kurtosis, higher-order HOAG modes transfer energy more conservatively within the tissue. Furthermore, the tissue type with the best transfer of optical power was observed to be the deep dermis (mouse).</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "组织湍流中高阶环形高斯光束的光束矩",
        "abstract_cn": "<jats:p>使用我们之前开发的公式来分析接收器平面处激光束的光束矩，该公式用于计算存在生物组织湍流时高阶环形高斯 (HOAG) 光束的平均光强度。检查 HOAG 光束矩的桶内功率 (PIB) 实体和各种组织类型的峰度，例如真皮上层（人类）、肝实质（小鼠）、肠上皮（小鼠）和深层真皮（小鼠）。此外，考虑折射率波动的强度系数和传播距离等因素来探索光束矩。研究发现，随着组织长度的增加，所有 HOAG 光束模式的 PIB 值均呈指数稳定下降，其表现与高斯光束类似。随着湍流强度的增加，高阶 HOAG 光束模式比低阶模式更有效地将光能传输到接收器。峰度分析表明，在中等距离处，光束能量向边缘分布，而在较长距离处，边缘处的能量集中度低于中心处。这种趋势反映在所有 HOAG 模式和组织类型的峰度值不断增加。考虑到 PIB 和峰度的变化，高阶 HOAG 模式在组织内更保守地传输能量。此外，观察到光功率传输最佳的组织类型是深层真皮（小鼠）。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576085",
        "title": "Second-scale atmospheric turbulence predicting with the Kolmogorov–Arnold network",
        "link": "https://doi.org/10.1364/josaa.576085",
        "published": "2025-10-16",
        "author": "Zhihao Wan, Haibin Zhan, Xianglei Meng, Huichuan Lin, Yan Li, Jun Zeng, Zhimin He, Yongtao Zhang, Huantin Chen, Jixiong Pu",
        "summary": "<jats:p>Free-space optical communications are susceptible to atmospheric turbulence. Conducting high-temporal-resolution atmospheric turbulence prediction can provide strategic support for the optimization and improvement of free-space optical communication systems, especially satellite-to-ground laser communication systems. This paper presents a new, to our knowledge, artificial neural network which is known as the Kolmogorov–Arnold network (KAN), for the prediction of atmospheric turbulence with a time resolution of 1 s using meteorological parameters. Based on the Kolmogorov–Arnold theorem, KAN establishes a nonlinear mapping relationship between meteorological parameters and the atmospheric turbulence strength. Compared with traditional neural networks, KAN is more effective at capturing complex correlations among meteorological parameters by introducing learnable nonlinear basis functions. Under the same meteorological conditions, when using the KANs to predict the atmospheric refractive-index structure constant (<jats:italic>C</jats:italic><jats:sub>\n      <jats:italic>n</jats:italic>\n    </jats:sub><jats:sup>2</jats:sup>) at a second-level time scale, the resulting mean absolute percentage error (MAPE) and symmetric mean absolute percentage error (SMAPE) are 31.54% and 31.44%, respectively. These two metrics are reduced by 6.74% and 4.35% compared to the traditional multi-layer perceptron (MLP) architecture.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "利用柯尔莫哥洛夫-阿诺德网络预测二级大气湍流",
        "abstract_cn": "<jats:p>自由空间光通信容易受到大气湍流的影响。开展高时间分辨率大气湍流预测可为自由空间光通信系统特别是星地激光通信系统的优化和改进提供战略支撑。据我们所知，本文提出了一种新的人工神经网络，称为柯尔莫哥洛夫-阿诺德网络 (KAN)，用于使用气象参数以 1 秒的时间分辨率预测大气湍流。 KAN基于柯尔莫哥洛夫-阿诺德定理，建立了气象参数与大气湍流强度之间的非线性映射关系。与传统神经网络相比，KAN 通过引入可学习的非线性基函数，能够更有效地捕捉气象参数之间的复杂相关性。在相同气象条件下，利用KAN预测大气折射率结构常数时（<jats:italic>C</jats:italic><jats:sub>\n      <jats:斜体>n</jats:斜体>\n    </jats:sub><jats:sup>2</jats:sup>）在二级时间尺度上，得到的平均绝对百分比误差（MAPE）和对称平均绝对百分比误差（SMAPE）分别为31.54%和31.44%。与传统的多层感知器（MLP）架构相比，这两个指标分别降低了 6.74% 和 4.35%。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.575039",
        "title": "Theoretical analysis of an axicon with negative refraction for infinite radial imaging",
        "link": "https://doi.org/10.1364/josaa.575039",
        "published": "2025-10-20",
        "author": "Rafael G. González-Acuña",
        "summary": "<jats:p>In this paper, we present a new optical device, to our knowledge, an axicon lens, which, for finite objects, sends the image radially to infinity. This axicon lens has a negative refractive index and is free from spherical aberration. Shown are several examples that, through ray tracing, confirm the functioning of this new artifact.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "负折射轴棱镜无限径向成像的理论分析",
        "abstract_cn": "<jats:p>在本文中，据我们所知，我们提出了一种新的光学设备，即轴锥透镜，对于有限的物体，它可以将图像径向发送到无穷远。该轴锥透镜具有负折射率并且没有球面像差。显示的几个示例通过光线追踪确认了这一新工件的功能。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573103",
        "title": "Performance analysis of equal-gain-diversity UWOC systems over exponentiated Weibull turbulence channels with nonzero-boresight pointing errors",
        "link": "https://doi.org/10.1364/josaa.573103",
        "published": "2025-10-29",
        "author": "Hongyan Jiang, Tao Liu, Xiaoji Li, Ahmad Fairuz Omar, Sujan Rajbhandari",
        "summary": "<jats:p>\n                    Underwater wireless optical communication (UWOC) systems face significant performance degradation due to turbulence-induced fading and pointing errors (PEs). This paper evaluates the bit error rate (BER) performance of spatial diversity UWOC systems with equal-gain combining (EGC) over independent exponentiated Weibull (EW) turbulence channels with nonzero-boresight PEs. A Parseval’s theorem-based approach is used to derive an analytical expression for the average BER of EGC UWOC systems, using the Fourier transform of the conditional error probability and the characteristic function (CHF) of independent individual fading channel coefficients. Combining EW turbulence and nonzero-boresight PEs, the probability density function and CHF of the fading channel coefficient are derived, and the corresponding frequency-domain integral is approximated using a Gauss–Chebyshev quadrature formula. Taking quadrature amplitude modulation scheme as an example, the numerical results are presented and verified using Monte Carlo simulations. It is demonstrated that EGC-based spatial diversity significantly improves system robustness against the turbulence-induced fading and PEs compared to single-input single-output systems, particularly under strong fading. For instance, a diversity order of eight offers diversity gains of 37.1, 25.2, and 17.5 dB at the BER of 10\n                    <jats:sup>−3</jats:sup>\n                    when scintillation indexes are 2.5983, 0.7335, and 0.2551, respectively. The study is helpful to evaluate the BER performance of the EGC diversity UWOC system over complex fading channels.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "具有非零视轴指向误差的指数威布尔湍流通道上等增益分集 UWOC 系统的性能分析",
        "abstract_cn": "<贾茨：p>\n                    由于湍流引起的衰落和指向误差 (PE)，水下无线光通信 (UWOC) 系统面临着严重的性能下降。本文评估了具有等增益组合 (EGC) 的空间分集 UWOC 系统在具有非零视轴 PE 的独立指数威布尔 (EW) 湍流通道上的误码率 (BER) 性能。基于 Parseval 定理的方法用于使用条件误差概率的傅里叶变换和独立个体衰落信道系数的特征函数 (CHF) 来推导 EGC UWOC 系统的平均 BER 的解析表达式。结合EW湍流和非零视轴PE，推导了衰落信道系数的概率密度函数和CHF，并使用高斯-切比雪夫求积公式逼近相应的频域积分。以正交幅度调制方案为例，利用蒙特卡罗模拟给出并验证了数值结果。结果表明，与单输入单输出系统相比，基于 EGC 的空间分集显着提高了系统对湍流引起的衰落和 PE 的鲁棒性，特别是在强衰落下。例如，分集阶数为 8 时，BER 为 10 时可提供 37.1、25.2 和 17.5 dB 的分集增益\n                    <jats:sup>−3</jats:sup>\n                    当闪烁指数分别为2.5983、0.7335和0.2551时。该研究有助于评估EGC分集UWOC系统在复杂衰落信道上的BER性能。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573957",
        "title": "Vectorial ray tracing scheme for arbitrary complex amplitude profiles through a given optical system",
        "link": "https://doi.org/10.1364/josaa.573957",
        "published": "2025-10-31",
        "author": "Shilpa Singh, Bosanta R. Boruah",
        "summary": "<jats:p>A comprehensive analysis of focal energy density is important in designing and improving the performance of any optical system that uses focused light beams. In this work, we propose a geometrical ray tracing-based scheme that can compute the energy densities or spot diagrams corresponding to different polarizations in the imaging plane due to any arbitrary user-defined beam. In our model, we incorporate lens-specific parameters such as radius of curvature, thickness, focal length, and refractive index, the spatial light modulator plane generating an arbitrary beam profile and 4f relay lens pairs, to trace both paraxial and skew rays up to the imaging plane through different refracting surfaces, providing a more realistic computational approach to obtain the focal spot. In particular, our model incorporates the vectorial nature of the light for each ray to facilitate computation of the vectorial spot diagrams, which are in effect a representation of the energy densities, corresponding to different orthogonal polarizations. We have implemented our model using the open-source programming language Python. The results using both low and high numerical aperture lenses indicate interesting similarities as well as distinctions in comparison with those obtained using vectorial diffraction theory.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "通过给定光学系统的任意复振幅分布的矢量光线追踪方案",
        "abstract_cn": "<jats:p>聚焦能量密度的全面分析对于设计和改进任何使用聚焦光束的光学系统的性能非常重要。在这项工作中，我们提出了一种基于几何射线追踪的方案，该方案可以计算由于任意用户定义的光束而与成像平面中的不同偏振相对应的能量密度或点图。在我们的模型中，我们结合了特定于透镜的参数，例如曲率半径、厚度、焦距和折射率、生成任意光束轮廓的空间光调制器平面和 4f 中继透镜对，以通过不同的折射表面追踪近轴和偏斜光线到成像平面，从而提供更真实的计算方法来获得焦斑。特别是，我们的模型结合了每条光线的光的矢量性质，以方便矢量点图的计算，矢量点图实际上是对应于不同正交偏振的能量密度的表示。我们使用开源编程语言 Python 实现了我们的模型。与使用矢量衍射理论获得的结果相比，使用低数值孔径透镜和高数值孔径透镜得到的结果显示出有趣的相似之处和区别。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576308",
        "title": "Vergence-based ocular wavefront expansions in diopters: orthogonal functions, clinical metrics, and visualization tools",
        "link": "https://doi.org/10.1364/josaa.576308",
        "published": "2025-11-06",
        "author": "Damien Gatinel, Jacques Malet",
        "summary": "<jats:p>\n                    We introduce two families of vergence functions to express ocular wavefront aberrations in diopters, bridging aberrometry, and clinical refraction. First, we build a fully orthogonal vergence basis (\n                    <jats:italic>V</jats:italic>\n                    ~), analogous to Zernike polynomials, which preserves mode orthogonality and supports unbiased coefficient statistics. In our VL–VH basis (\n                    <jats:italic>V</jats:italic>\n                    ), a clear separation between low-degree and high-degree prevents the intrusion of low-degree terms into high-degree modes, which could otherwise hinder direct clinical interpretation. The vergence function expansions in both bases are derived from wavefront slopes through radial differentiation. We demonstrate their clinical utility through three cases: a normal eye, a keratoconic eye, and a post-myopic LASIK eye. The VL–VH basis provides stable refraction estimates across pupil sizes by fitting low-degree terms over central regions, closely matching subjective refraction. In contrast, the orthogonal\n                    <jats:italic>V</jats:italic>\n                    ~ basis shows pupil-dependent refraction due to peripheral wavefront influence. In eyes with significant spherical aberration, the bases yield markedly different refractive predictions, with VL–VH better aligning with clinical measurements. Pyramid plots, dioptric maps, and coefficient histograms facilitate aberration visualization and diagnosis. These vergence-based tools enhance the integration of advanced aberrometry into clinical practice.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于聚散度的眼部波前屈光度扩展：正交函数、临床指标和可视化工具",
        "abstract_cn": "<贾茨：p>\n                    我们引入了两个聚散函数系列来表达屈光度、桥接像差测量和临床屈光的眼部波前像差。首先，我们建立一个完全正交的聚散基础（\n                    <贾茨：斜体>V</贾茨：斜体>\n                    ~)，类似于 Zernike 多项式，它保留模式正交性并支持无偏系数统计。在我们的 VL–VH 基础上（\n                    <贾茨：斜体>V</贾茨：斜体>\n                    ），低阶和高阶之间的明确分离可以防止低阶术语侵入高阶模式，否则可能会妨碍直接的临床解释。两个基底的聚散函数展开都是通过径向微分从波前斜率导出的。我们通过三个案例展示了它们的临床实用性：正常眼、圆锥角膜眼和近视后 LASIK 眼。 VL-VH 基础通过在中心区域拟合低阶项，提供跨瞳孔大小的稳定屈光估计，与主观验光紧密匹配。相比之下，正交\n                    <贾茨：斜体>V</贾茨：斜体>\n                    ~ 基础显示由于周边波前影响而导致的瞳孔相关屈光。在具有显着球差的眼睛中，基础产生明显不同的屈光预测，VL-VH 与临床测量更好地一致。金字塔图、屈光图和系数直方图有助于像差可视化和诊断。这些基于聚散度的工具增强了先进的像差测量与临床实践的整合。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.577465",
        "title": "Propagation of an inverse super-Gaussian non-uniformly correlated partially coherent vector beam in atmospheric turbulence",
        "link": "https://doi.org/10.1364/josaa.577465",
        "published": "2025-10-31",
        "author": "Zeyu Zhou, Jun He, Xuanbing Yang, Feng Zhou",
        "summary": "<jats:p>Vector beams with customizable topological charges hold significant potential for augmenting the capacity of optical communication systems. However, their propagation through random media like atmospheric turbulence induces wavefront distortion and intensity scintillation, severely degrading performance. To address this, we propose and design an inverse super-Gaussian non-uniformly correlated vector beam (ISGNCVB) with a tunable coherence structure. We thoroughly investigate its propagation characteristics in turbulent conditions. Our results demonstrate that the non-uniform correlation design confers remarkable self-focusing effects and stabilizes the received intensity profile. This leads to a signal-to-noise ratio (SNR) gain of up to 2 dB compared to its uniformly correlated counterpart. Furthermore, the polarization mode purity of the ISGNCVB exhibits enhanced robustness against turbulence-induced degradation. These advantages make the ISGNCVB a highly promising candidate for high-performance free-space optical communication links.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "大气湍流中逆超高斯非均匀相关部分相干矢量光束的传播",
        "abstract_cn": "<jats:p>具有可定制拓扑电荷的矢量光束在增强光通信系统容量方面具有巨大潜力。然而，它们通过大气湍流等随机介质传播会引起波前畸变和强度闪烁，严重降低性能。为了解决这个问题，我们提出并设计了一种具有可调相干结构的逆超高斯非均匀相关矢量光束（ISGNCVB）。我们彻底研究了其在湍流条件下的传播特性。我们的结果表明，非均匀相关设计具有显着的自聚焦效应并稳定了接收到的强度分布。与一致相关的对应物相比，这导致信噪比 (SNR) 增益高达 2 dB。此外，ISGNCVB 的偏振模式纯度对湍流引起的退化表现出增强的鲁棒性。这些优点使 ISGNCVB 成为高性能自由空间光通信链路的极具前景的候选者。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.578331",
        "title": "Wave pulses with unusual asymptotical behavior at infinity",
        "link": "https://doi.org/10.1364/josaa.578331",
        "published": "2025-10-24",
        "author": "Peeter Saari, Ioannis M. Besieris",
        "summary": "<jats:p>\n                    The behavior of wave signals in the far zone is not only of theoretical\n\t\t\t\t\tinterest but also of paramount practical importance in communications\n\t\t\t\t\tand other fields of applications of optical, electromagnetic, or\n\t\t\t\t\tacoustic waves. A long time ago, T. T. Wu [\n                    <jats:mixed-citation publication-type=\"journal\">\n                      <jats:source>J. Appl. Phys.</jats:source>\n                      <jats:volume>57</jats:volume>\n                      ,\n                      <jats:fpage>2370</jats:fpage>\n                      (\n                      <jats:year>1985</jats:year>\n                      )\n                      <jats:pub-id pub-id-type=\"coden\">JAPIAU</jats:pub-id>\n                      <jats:issn>0021-8979</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1063/1.335465</jats:pub-id>\n                    </jats:mixed-citation>\n                    ]\n\t\t\t\t\tintroduced models of “electromagnetic missiles” whose\n\t\t\t\t\tdecay could be made arbitrarily slower than the usual inverse distance\n\t\t\t\t\tby an appropriate choice of the high-frequency portion of the source\n\t\t\t\t\tspectrum. Very recent work by Plachenov and Kiselev [\n                    <jats:mixed-citation publication-type=\"journal\">\n                      <jats:source>Diff. Eqs.</jats:source>\n                      <jats:volume>60</jats:volume>\n                      ,\n                      <jats:fpage>1634</jats:fpage>\n                      (\n                      <jats:year>2024</jats:year>\n                      )\n                      <jats:pub-id pub-id-type=\"doi\">10.1134/S001226612460250X</jats:pub-id>\n                    </jats:mixed-citation>\n                    ]\n\t\t\t\t\tintroduced a finite-energy scalar wave solution, different from\n\t\t\t\t\tWu’s, decaying slower than inversely proportional with the\n\t\t\t\t\tdistance. A physical explanation for the unusual asymptotic behavior\n\t\t\t\t\tof the latter will be given in this paper. Furthermore, two additional\n\t\t\t\t\texamples of scalar wave pulses characterized by abnormal slow decay in\n\t\t\t\t\tthe far zone will be given, and their asymptotic behavior will be\n\t\t\t\t\tdiscussed. A proof of feasibility of acoustic and electromagnetic\n\t\t\t\t\tfields with the abnormal asymptotics will be described.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "无穷远处具有异常渐近行为的波脉冲",
        "abstract_cn": "<贾茨：p>\n                    远区波信号的行为不仅是理论上的\n\t\t\t\t\t有趣但在通信中也具有极其重要的实际意义\n\t\t\t\t\t以及光学、电磁或其他应用领域\n\t\t\t\t\t声波。很久以前，吴T. T. [\n                    <jats：混合引用出版物类型=“期刊”>\n                      <jats：来源>J.应用。物理。</jats:来源>\n                      <贾茨：卷> 57 </贾茨：卷>\n                      ,\n                      <贾茨：fpage>2370</贾茨：fpage>\n                      (\n                      <jats:year>1985</jats:year>\n                      ）\n                      <jats:pub-id pub-id-type=\"coden\">JAPIAU</jats:pub-id>\n                      <jats:issn>0021-8979</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1063/1.335465</jats:pub-id>\n                    </jats：混合引用>\n                    ]\n\t\t\t\t\t推出了“电磁导弹”型号\n\t\t\t\t\t衰减可以比通常的反距离任意慢\n\t\t\t\t\t通过适当选择源的高频部分\n\t\t\t\t\t频谱。 Plachenov 和 Kiselev 的最新作品 [\n                    <jats：混合引用出版物类型=“期刊”>\n                      <jats：来源>差异。等式</jats：来源>\n                      <贾茨：卷> 60 </贾茨：卷>\n                      ,\n                      <贾茨：fpage>1634</贾茨：fpage>\n                      (\n                      <jats:year>2024</jats:year>\n                      ）\n                      <jats:pub-id pub-id-type=\"doi\">10.1134/S001226612460250X</jats:pub-id>\n                    </jats：混合引用>\n                    ]\n\t\t\t\t\t引入了有限能量标量波解，不同于\n\t\t\t\t\tWu 的衰减速度比与\n\t\t\t\t\t距离。对异常渐近行为的物理解释\n\t\t\t\t\t本文将给出后者的内容。此外，另外还有两个\n\t\t\t\t\t以异常缓慢衰减为特征的标量波脉冲示例\n\t\t\t\t\t将给出远区，并且它们的渐近行为将是\n\t\t\t\t\t讨论过。声学和电磁学可行性证明\n\t\t\t\t\t将描述具有异常渐近的场。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581123",
        "title": "Analytical aberration theory for plane-symmetric optical systems and its application in distortion analysis of spectrometers: erratum",
        "link": "https://doi.org/10.1364/josaa.581123",
        "published": "2025-11-06",
        "author": "Yuxuan Liu, Jannick P. Rolland",
        "summary": "<jats:p>\n                    The present erratum is intended to make corrections to several equations in our paper [\n                    <jats:mixed-citation publication-type=\"journal\">\n                      <jats:source>J. Opt. Soc. Am. A</jats:source>\n                      <jats:volume>42</jats:volume>\n                      ,\n                      <jats:fpage>362</jats:fpage>\n                      (\n                      <jats:year>2025</jats:year>\n                      )\n                      <jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id>\n                      <jats:issn>0740-3232</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.547743</jats:pub-id>\n                    </jats:mixed-citation>\n                    ]. The related numerical results are updated accordingly.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "平面对称光学系统的解析像差理论及其在光谱仪畸变分析中的应用：勘误表",
        "abstract_cn": "<贾茨：p>\n                    目前的勘误旨在对我们论文中的几个方程进行修正[\n                    <jats：混合引用出版物类型=“期刊”>\n                      <jats：来源>J.选择。苏克。是。 A</jats:来源>\n                      <贾茨：卷> 42 </贾茨：卷>\n                      ,\n                      <jats:fpage>362</jats:fpage>\n                      (\n                      <jats:year>2025</jats:year>\n                      ）\n                      <jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id>\n                      <jats:issn>0740-3232</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.547743</jats:pub-id>\n                    </jats：混合引用>\n                    ]。相关数值结果也相应更新。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581576",
        "title": "Local QFAPHFMs magnitude domain color image watermark detector based on a truncated Weibull–Rayleigh distribution: publisher’s note",
        "link": "https://doi.org/10.1364/josaa.581576",
        "published": "2025-10-16",
        "author": "Siyu Yang",
        "summary": "<jats:p>\n                    This publisher’s note corrects the author affiliation in the article\n                    <jats:mixed-citation publication-type=\"journal\">\n                      <jats:source>J. Opt. Soc. Am.\n\t\t\t\tA</jats:source>\n                      <jats:volume>42</jats:volume>\n                      ,\n                      <jats:fpage>885</jats:fpage>\n                      (\n                      <jats:year>2025</jats:year>\n                      )\n                      <jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id>\n                      <jats:issn>0740-3232</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.559284</jats:pub-id>\n                    </jats:mixed-citation>\n                    .\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于截断威布尔-瑞利分布的本地 QFAPHFM 幅度域彩色图像水印检测器：出版商注释",
        "abstract_cn": "<贾茨：p>\n                    此出版商注释更正了文章中的作者隶属关系\n                    <jats：混合引用出版物类型=“期刊”>\n                      <jats：来源>J.选择。苏克。是。\n\t\t\t\tA</jats:来源>\n                      <贾茨：卷> 42 </贾茨：卷>\n                      ,\n                      <贾茨：fpage>885</贾茨：fpage>\n                      (\n                      <jats:year>2025</jats:year>\n                      ）\n                      <jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id>\n                      <jats:issn>0740-3232</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.559284</jats:pub-id>\n                    </jats：混合引用>\n                    。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.577455",
        "title": "Polarization-state independence of backscattering efficiency of an isotropic chiral sphere",
        "link": "https://doi.org/10.1364/josaa.577455",
        "published": "2025-11-07",
        "author": "Héctor M. Iga-Buitrón, Tom G. Mackay, Akhlesh Lakhtakia",
        "summary": "<jats:p>Theoretical analysis revealed that the backscattering efficiency of an isotropic chiral sphere is independent of the polarization state of the incident plane wave, whereas the extinction, total scattering, absorption, and forward-scattering efficiencies are generally not. Furthermore, the handedness of the backscattered field in the far zone is the reverse of that of the incident plane wave. Finally, there can be non-zero values of the chirality parameter such that either the extinction, total scattering, absorption, or forward-scattering efficiency is also independent of the polarization state of the incident plane wave.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "各向同性手性球体后向散射效率的偏振态独立性",
        "abstract_cn": "<jats:p>理论分析表明，各向同性手性球的后向散射效率与入射平面波的偏振态无关，而消光、总散射、吸收和前向散射效率通常则不然。此外，远区反向散射场的旋向性与入射平面波的旋向性相反。最后，手性参数可以有非零值，使得消光、总散射、吸收或前向散射效率也与入射平面波的偏振态无关。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.577504",
        "title": "Fourier hybrid circular Airy vortex beam",
        "link": "https://doi.org/10.1364/josaa.577504",
        "published": "2025-11-06",
        "author": "Bingsong Cao, Zhangrong Mei, Yonghua Mao, Peizhen Qiu, Kaikai Huang",
        "summary": "<jats:p>The Fourier hybrid circular Airy vortex beam (FHCAVB) is synthesized in the Fourier domain by preserving the intrinsic phase profile from the Fourier transform of the circular Airy vortex beam (CAVB), while substituting its amplitude with a Gaussian distribution. Despite these modifications, the FHCAVB retains key properties of the original CAVB, offering enhanced autofocusing contrast and maintaining controllability through the same set of parameters. This beam can be efficiently generated using pure-phase devices via a Fourier transform setup. In our experiments, the FHCAVB was successfully produced using a phase-only spatial light modulator (SLM) in a Fourier transform configuration, with results aligning well with simulations. As a high-fidelity approximation of the CAVB, this method for generating autofocusing Airy (AAF) vortex beams shows significant potential for applications in optical trapping, tweezing, communications, and related fields.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "傅里叶混合圆形艾里涡旋光束",
        "abstract_cn": "<jats:p>傅里叶混合圆形艾里涡旋光束 (FHCAVB) 是在傅里叶域中合成的，通过保留圆形艾里涡旋光束 (CAVB) 的傅里叶变换的固有相位轮廓，同时用高斯分布代替其幅度。尽管进行了这些修改，FHCAVB 仍保留了原始 CAVB 的关键特性，提供增强的自动对焦对比度并通过同一组参数保持可控性。该光束可以通过傅里叶变换装置使用纯相位设备有效地产生。在我们的实验中，使用傅里叶变换配置中的纯相位空间光调制器 (SLM) 成功生产了 FHCAVB，结果与模拟非常吻合。作为 CAVB 的高保真近似，这种生成自动聚焦艾里 (AAF) 涡旋光束的方法在光学捕获、镊子、通信和相关领域显示出巨大的应用潜力。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576326",
        "title": "Streamlines of modified vector Mathieu–Gauss beams",
        "link": "https://doi.org/10.1364/josaa.576326",
        "published": "2025-11-06",
        "author": "Missael Sepulveda-Rios, Julio C. Gutiérrez-Vega",
        "summary": "<jats:p>We characterize the streamlines and polarization structure of the modified vector Mathieu–Gauss (mvMG) beams, a class of vector solutions to the paraxial Maxwell equations in elliptic cylindrical coordinates. We derive analytical expressions for the TE and TM modes and demonstrate that their streamlines can be obtained either exactly or via asymptotic and trigonometric approximations. Furthermore, we explore superpositions of the TE and TM modes, including circularly polarized helical beams, and provide closed-form expressions for the corresponding streamlines and Stokes parameters. Our results reveal the topological richness of mvMG beams and offer new tools for their description, with potential applications in structured light, optical singularities, and beam shaping in non-Cartesian coordinate systems.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "修正矢量马蒂厄-高斯光束的流线",
        "abstract_cn": "<jats:p>我们描述了修正矢量马蒂厄-高斯 (mvMG) 光束的流线和偏振结构，这是椭圆柱坐标系中近轴麦克斯韦方程组的一类矢量解。我们推导了 TE 和 TM 模式的解析表达式，并证明它们的流线可以精确地获得，也可以通过渐近和三角近似获得。此外，我们探索了 TE 和 TM 模式的叠加，包括圆偏振螺旋光束，并为相应的流线和斯托克斯参数提供了封闭式表达式。我们的结果揭示了 mvMG 光束的拓扑丰富性，并为其描述提供了新的工具，在结构光、光学奇点和非笛卡尔坐标系中的光束整形方面具有潜在的应用。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.565216",
        "title": "Disorder effect in a 2D array of spherical particles on the electromagnetic field on their surface",
        "link": "https://doi.org/10.1364/josaa.565216",
        "published": "2025-11-12",
        "author": "Valery A. Loiko, Alexander A. Miskevich, Natalia A. Loiko",
        "summary": "<jats:p>\n                    The influence of disorder in the spatial arrangement of identical, homogeneous spherical particles of an infinite two-dimensional (2D) array on the energy density spectra of the electric and magnetic fields on their surfaces under normal incidence of a plane electromagnetic wave is studied. The consideration is based on a semi-analytical statistical method (SASM) developed by us. Radial distribution functions based on the hard-disk model are used to simulate particle arrangements in arrays. We wrote a formula for this function describing the perfect azimuthally averaged lattice and analyzed in detail the energy densities for different deviations of particle centers from the nodes of the perfect lattice. The calculation results for a partially ordered array and imperfect and perfect lattices of silver (Ag), crystalline silicon (c-Si), and titanium oxide (TiO\n                    <jats:sub>2</jats:sub>\n                    ) particles with sizes of 50 and 300 nm are presented in the wavelength range of 0.3–1.1 µm for a host medium with a refractive index close to that of water. They demonstrate the contribution of the disorder effect to the optical response of the system and allow finding the optimal characteristics of lattice-induced resonances for energy densities on the particle surface. Such data are necessary for solving problems of increasing the efficiency of converting light energy absorbed by the system into other types of energy. The spectra of energy densities obtained under the SASM are in excellent agreement with the data of the numerical finite element method (FEM). To complete the picture, the near-field data are accompanied by far-field data for the incoherent component of the light.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "二维球形颗粒阵列对其表面电磁场的无序效应",
        "abstract_cn": "<贾茨：p>\n                    研究了平面电磁波法向入射下无限二维（2D）阵列中相同均匀球形粒子的空间排列无序对其表面电场和磁场能量密度谱的影响。该考虑基于我们开发的半分析统计方法（SASM）。基于硬盘模型的径向分布函数用于模拟阵列中的粒子排列。我们为这个函数编写了一个公式来描述完美的方位平均晶格，并详细分析了粒子中心与完美晶格节点的不同偏差的能量密度。银 (Ag)、晶体硅 (c-Si) 和氧化钛 (TiO) 的部分有序阵列以及不完美和完美晶格的计算结果\n                    <贾茨：子>2</贾茨：子>\n                    ) 对于折射率接近水的主体介质，尺寸为 50 和 300 nm 的颗粒呈现在 0.3–1.1 µm 的波长范围内。它们证明了无序效应对系统光学响应的​​贡献，并允许找到颗粒表面能量密度的晶格诱导共振的最佳特性。这些数据对于解决提高将系统吸收的光能转换成其他类型的能量的效率的问题是必要的。 SASM 下获得的能量密度谱与数值有限元法 (FEM) 的数据非常吻合。为了完成这幅图，近场数据还附有光的非相干分量的远场数据。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581916",
        "title": "Single-molecule orientation localization microscopy I: fundamental limits: erratum",
        "link": "https://doi.org/10.1364/josaa.581916",
        "published": "2025-11-06",
        "author": "Oumeng Zhang, Matthew D. Lew",
        "summary": "<jats:p>\n                    This erratum corrects a typographical error in our paper [\n                    <jats:mixed-citation publication-type=\"journal\">\n                      <jats:source>J. Opt. Soc. Am. A</jats:source>\n                      <jats:volume>38</jats:volume>\n                      ,\n                      <jats:fpage>277</jats:fpage>\n                      (\n                      <jats:year>2021</jats:year>\n                      )\n                      <jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id>\n                      <jats:issn>0740-3232</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.411981</jats:pub-id>\n                    </jats:mixed-citation>\n                    ].\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "单分子取向定位显微镜 I：基本限制：勘误表",
        "abstract_cn": "<贾茨：p>\n                    此勘误更正了我们论文中的一个印刷错误 [\n                    <jats：混合引用出版物类型=“期刊”>\n                      <jats：来源>J.选择。苏克。是。 A</jats:来源>\n                      <贾茨：卷> 38 </贾茨：卷>\n                      ,\n                      <jats:fpage>277</jats:fpage>\n                      (\n                      <jats:year>2021</jats:year>\n                      ）\n                      <jats:pub-id pub-id-type=\"coden\">JOAOD6</jats:pub-id>\n                      <jats:issn>0740-3232</jats:issn>\n                      <jats:pub-id pub-id-type=\"doi\">10.1364/JOSAA.411981</jats:pub-id>\n                    </jats：混合引用>\n                    ]。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576439",
        "title": "Ultra-low-loss hollow-core anti-resonant fiber",
        "link": "https://doi.org/10.1364/josaa.576439",
        "published": "2025-11-13",
        "author": "Qiang Liu, Peiqing Xing, Yudan Sun, Xiaotian Yao, Guangrong Sun, Tingting Lv, Jingwei Lv, Paul K. Chu, Chao Liu",
        "summary": "<jats:p>\n                    In this paper, an ultra-low-loss hollow-core anti-resonant fiber (HC-ARF) operating in the near-infrared band is proposed. The ARF is based on six nested circular tubes made of silica. Then a straight rod is added to the nested inner circular tube to reduce the transmission loss. The low-loss transmission performance of the HC-ARF is analyzed in detail using the finite element method, and the structural parameters are further optimized. The results show that the confinement loss is lower than 7.91×10\n                    <jats:sup>−7</jats:sup>\n                    dB/m in the range of 1.54–1.78 µm. Especially the confinement loss is as low as 1.31×10\n                    <jats:sup>−7</jats:sup>\n                    dB/m at 1.55 µm. In addition, the HC-ARF has good bending resistance. The bending loss can be kept below 8.02×10\n                    <jats:sup>−5</jats:sup>\n                    dB/m as the bending radius is larger than 5 cm. The performance of the designed ARF is significantly better than previously reported results, and the cladding structure of the ARF is simple and easy to process. It has great potential for commercial applications.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "超低损耗空心抗谐振光纤",
        "abstract_cn": "<贾茨：p>\n                    本文提出了一种工作在近红外波段的超低损耗空心抗谐振光纤（HC-ARF）。 ARF 基于六个由二氧化硅制成的嵌套圆管。然后在嵌套的内圆管中添加直杆，以减少传输损耗。利用有限元方法对HC-ARF的低损耗传输性能进行了详细分析，并进一步优化了结构参数。结果表明，约束损失低于7.91×10\n                    <jats:sup>−7</jats:sup>\n                    dB/m 范围为 1.54–1.78 µm。尤其是约束损失低至1.31×10\n                    <jats:sup>−7</jats:sup>\n                    1.55 µm 处的 dB/m。此外，HC-ARF具有良好的抗弯性能。弯曲损耗可保持在8.02×10以下\n                    <jats:sup>−5</jats:sup>\n                    dB/m（弯曲半径大于 5 厘米）。所设计的ARF的性能明显优于之前报道的结果，并且ARF的包层结构简单且易于加工。它具有巨大的商业应用潜力。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574393",
        "title": "Scalar computational primitives with perturbative phase interferometry",
        "link": "https://doi.org/10.1364/josaa.574393",
        "published": "2025-11-14",
        "author": "Christopher R. Schwarze, Anthony D. Manni, David S. Simon, Alexander V. Sergienko",
        "summary": "<jats:p>We describe how weak phase modulations applied to classical coherent light in specially modified linear interferometers can be used to perform primitive computational tasks. Instead of encoding operations within a fixed unitary state, the operations are enacted by moving from one state to another. This harnesses the particular phase parameterization of an interferometer, allowing entirely linear optics to produce nonlinear operations such as division and powers. This is due to the nonlinear structure of the underlying phase parameterizations. The realized operations are approximate but can be made more accurate by decreasing the size of the input perturbations. For each operation, the inputs and outputs are changes in phase relative to a fixed bias point. The output phase is ultimately read out as a change in optical power.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用微扰相位干涉测量的标量计算基元",
        "abstract_cn": "<jats:p>我们描述了如何在经过特殊修改的线性干涉仪中将弱相位调制应用于经典相干光来执行原始计算任务。这些操作不是在固定的单一状态内进行编码操作，而是通过从一种状态移动到另一种状态来执行。这利用了干涉仪的特定相位参数化，允许完全线性光学产生非线性运算，例如除法和幂。这是由于底层相位参数化的非线性结构造成的。实现的操作是近似的，但可以通过减小输入扰动的大小来变得更加准确。对于每个操作，输入和输出的相位相对于固定偏置点发生变化。输出相位最终作为光功率的变化读出。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.572793",
        "title": "Nano-plasmonic dual-mode probe for near-vector field scanning optical microscopy",
        "link": "https://doi.org/10.1364/josaa.572793",
        "published": "2025-11-14",
        "author": "Yujie Gao, Yupei Zhang, Shanlong Guo, Guoliang Jin, Jinhong Li, Xiaojin Yin",
        "summary": "<jats:p>The probe serves as a pivotal component in scanning near-field optical microscopy, enhancing its imaging resolution by capturing high-frequency information within the sample’s near-field region. However, a current limitation exists: a single probe can only respond to either the transverse or longitudinal vector field independently. This single polarization response not only escalates imaging costs but also leads to a loss of imaging information, thereby narrowing the application scope of scanning near-field microscopy. In this work, we propose a nano-plasmonic probe that has two transmission modes and can characterize the transverse and longitudinal near-fields using a single system. The proposed nano-plasmonic probe has a nanoparticle-on-aperture in the film structure that is designed on a tapered fiber tip. Finally, the feasibility of the fabrication of the nano-plasmonic probe using optical tweezers technology is analyzed and studied. This nano-plasmonic probe shows great potential for use in near-field optical microscopy applications.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "用于近矢量场扫描光学显微镜的纳米等离子体双模探针",
        "abstract_cn": "<jats:p>探针是扫描近场光学显微镜的关键组件，通过捕获样品近场区域内的高频信息来提高其成像分辨率。然而，目前存在一个限制：单个探头只能独立响应横向或纵向矢量场。这种单一的偏振响应不仅增加了成像成本，而且导致成像信息丢失，从而缩小了扫描近场显微镜的应用范围。在这项工作中，我们提出了一种纳米等离子体探针，它具有两种传输模式，可以使用单个系统表征横向和纵向近场。所提出的纳米等离子体探针在薄膜结构中具有孔径上的纳米粒子，该薄膜结构设计在锥形光​​纤尖端上。最后，对利用光镊技术制作纳米等离子体探针的可行性进行了分析和研究。这种纳米等离子体探针在近场光学显微镜应用中显示出巨大的潜力。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574392",
        "title": "Experimental investigation on the propagation  of partially coherent fractional vortex beams in atmospheric turbulence",
        "link": "https://doi.org/10.1364/josaa.574392",
        "published": "2025-11-18",
        "author": "Xiang Xu, Chuankai Luo, Xianmei Qian, Xiaoming Zhou, Wenyue Zhu",
        "summary": "<jats:p>Partially coherent vortex beams have attracted growing interest due to their enhanced robustness and unique propagation characteristics in complex media. In this work, we experimentally investigate the behavior of partially coherent fractional vortex beams as they propagate through atmospheric turbulence. The beams are generated using a phase-only spatial light modulator and a rotating ground-glass disk modeled by the Gaussian Schell framework, and their degree of partial coherence is quantitatively characterized using a Young’s double-slit interference plate. After transmission through a 1.2 m turbulence simulator, the effective beam radius exhibits a smoothed, quasi-linear growth trend between successive integer topological charges, indicating the suppression of discrete modal transitions by the combined effects of partial coherence and turbulence. The scintillation index decreases overall with increasing topological charge, while local enhancements near half-integer orders reveal the heightened turbulence sensitivity of modal interference. Moreover, partially obstructed PCFVBs show partial statistical self-reconstruction after turbulent propagation, whereas a fully coherent control under identical conditions shows no appreciable recovery, ruling out a purely diffractive fill-in. These results provide the first, to our knowledge, comprehensive experimental insight into the interplay among coherence, turbulence, and fractional vortex structure, offering new perspectives for designing turbulence-resistant structured-light systems.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "大气湍流中部分相干分数涡旋光束传播的实验研究",
        "abstract_cn": "<jats:p>部分相干涡旋光束由于其增强的鲁棒性和在复杂介质中独特的传播特性而引起了越来越多的兴趣。在这项工作中，我们通过实验研究了部分相干分数涡旋光束在大气湍流中传播时的行为。这些光束是使用纯相位空间光调制器和高斯谢尔框架建模的旋转磨砂玻璃盘生成的，并且使用杨氏双缝干涉板定量表征其部分相干程度。通过 1.2 m 湍流模拟器传输后，有效光束半径在连续整数拓扑电荷之间呈现出平滑的准线性增长趋势，表明部分相干和湍流的综合效应抑制了离散模态转变。随着拓扑电荷的增加，闪烁指数总体下降，而半整数阶附近的局部增强揭示了模态干扰的湍流敏感性增强。此外，部分受阻的 PCFVB 在湍流传播后显示出部分统计自重建，而在相同条件下的完全相干控制则显示没有明显的恢复，从而排除了纯粹的衍射填充。据我们所知，这些结果首次对相干性、湍流和分数涡旋结构之间的相互作用提供了全面的实验见解，为设计抗湍流结构光系统提供了新的视角。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574506",
        "title": "Imaging properties of freeform quadric reflectors via the theory of eigenmirrors",
        "link": "https://doi.org/10.1364/josaa.574506",
        "published": "2025-11-19",
        "author": "R. Andrew Hicks",
        "summary": "<jats:p>\n                    A surface\n                    <jats:italic>S</jats:italic>\n                    is said to be an eigensurface of a mirror\n                    <jats:italic>M</jats:italic>\n                    if the reflection of\n                    <jats:italic>S</jats:italic>\n                    in\n                    <jats:italic>M</jats:italic>\n                    appears undistorted. In that case,\n                    <jats:italic>M</jats:italic>\n                    is the eigenmirror corresponding to the eigensurface\n                    <jats:italic>S</jats:italic>\n                    . Rotationally symmetric eigenmirror/eigensurface pairs are easy to construct, but the situation for freeform mirrors requires consideration of a partial differential equation, the anti-eikonal equation. Here, we investigate freeform quadric eigenmirrors, classifying them into five families. In each case, the eigensurfaces are biquadratic, i.e., they satisfy equations of the form\n                    <jats:italic>ψ</jats:italic>\n                    (\n                    <jats:italic>x</jats:italic>\n                    <jats:sup>2</jats:sup>\n                    ,\n                    <jats:italic>y</jats:italic>\n                    <jats:sup>2</jats:sup>\n                    ,\n                    <jats:italic>z</jats:italic>\n                    <jats:sup>2</jats:sup>\n                    )=0, where\n                    <jats:italic>ψ</jats:italic>\n                    is a quadric polynomial. Lastly, we investigate some more general hypotheses and a connection to dynamical systems.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于特征镜理论的自由曲面二次反射镜的成像特性",
        "abstract_cn": "<贾茨：p>\n                    一个表面\n                    <jats:斜体>S</jats:斜体>\n                    据说是镜子的本征面\n                    <jats:斜体>M</jats:斜体>\n                    如果反射\n                    <jats:斜体>S</jats:斜体>\n                    在\n                    <jats:斜体>M</jats:斜体>\n                    看起来没有失真。在那种情况下，\n                    <jats:斜体>M</jats:斜体>\n                    是对应于特征面的特征镜\n                    <jats:斜体>S</jats:斜体>\n                    。旋转对称本征镜/本征面对很容易构造，但自由曲面镜的情况需要考虑偏微分方程，即反 Ekonal 方程。在这里，我们研究自由形式二次特征镜，将它们分为五个系列。在每种情况下，特征面都是双二次的，即它们满足以下形式的方程\n                    <jats:斜体>ψ</jats:斜体>\n                    (\n                    <jats:斜体>x</jats:斜体>\n                    <贾茨：sup>2</贾茨：sup>\n                    ,\n                    <jats:斜体>y</jats:斜体>\n                    <贾茨：sup>2</贾茨：sup>\n                    ,\n                    <jats:斜体>z</jats:斜体>\n                    <贾茨：sup>2</贾茨：sup>\n                    )=0，其中\n                    <jats:斜体>ψ</jats:斜体>\n                    是二次多项式。最后，我们研究了一些更一般的假设以及与动力系统的联系。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.569685",
        "title": "Common issues and human intervention in object detection from handcrafted features to deep learning: discussion",
        "link": "https://doi.org/10.1364/josaa.569685",
        "published": "2025-11-20",
        "author": "Michela Lecca, Simone Bianco",
        "summary": "<jats:p>While in traditional methods object detection is based on the handcrafted definition of relevant visual features and rules, in machine/deep learning methods this task is achieved by learning both features and rules from a training set. The traditional and machine/deep learning object detection workflows are often described as opposite because in the traditional framework, the visual features and rules to detect the object of interest are provided as input, while in the machine/deep learning-based framework they are automatically learned from the data depending on the task considered and constitute the final trained model. In this work, we analyze the object detection recipe, and we show that these two approaches actually present three common issues that require human supervision and ad hoc procedures to be addressed: the design of an object model suitable for the context, devices, and task at hand; the achievement of detection robustness against several factors like noise, image quality, changes in geometry, and light variations; and the definition of an appropriate matching function. We also briefly review some common metrics for evaluating object detection performance, proving that human intervention is crucial in this task as well. Our analysis aims at fostering a more aware use of the object detection approaches and stimulating new research for automating—where possible—the tasks that humans are still in charge of.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "从手工特征到深度学习的对象检测中的常见问题和人为干预：讨论",
        "abstract_cn": "<jats:p>在传统方法中，对象检测是基于相关视觉特征和规则的手工定义，而在机器/深度学习方法中，此任务是通过从训练集中学习特征和规则来实现的。传统和机器/深度学习对象检测工作流程通常被描述为相反的，因为在传统框架中，检测感兴趣对象的视觉特征和规则作为输入提供，而在基于机器/深度学习的框架中，它们根据所考虑的任务从数据中自动学习并构成最终的训练模型。在这项工作中，我们分析了对象检测方法，并表明这两种方法实际上提出了三个常见问题，需要人工监督和临时程序来解决：设计适合上下文、设备和手头任务的对象模型；针对噪声、图像质量、几何变化和光线变化等多种因素实现检测鲁棒性；以及适当匹配函数的定义。我们还简要回顾了一些用于评估对象检测性能的常见指标，证明人为干预在此任务中也至关重要。我们的分析旨在促进人们更清楚地使用对象检测方法，并激发新的研究，以在可能的情况下自动化人类仍然负责的任务。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573587",
        "title": "Developing variants of the Lucy–Richardson algorithm for coded aperture imaging: tutorial",
        "link": "https://doi.org/10.1364/josaa.573587",
        "published": "2025-11-19",
        "author": "Vijayakumar Anand",
        "summary": "<jats:p>Deconvolution methods, originally developed for image deblurring, are foundational to coded aperture imaging (CAI) technologies. Among these, the Lucy–Richardson algorithm (LRA), first introduced over half a century ago, has seen renewed interest in CAI applications in recent years. Uniquely, LRA incorporates both convolution and cross-correlation operations, with the latter effectively functioning as an internal deconvolution step, offering a versatile platform for innovation. This tutorial presents the fundamentals of CAI alongside a detailed formulation of LRA. Strategies for enhancing LRA performance through modifications to the cross-correlation step are explored in depth. Both established variants, such as LRA with power-law transformation and limited support constraint, the Lucy–Richardson–Rosen algorithm, and novel extensions, including the interlooped LRA, are introduced. Future directions for designing LRA variants tailored to specific imaging scenarios are also discussed. Step-by-step MATLAB code examples are provided to guide researchers in developing custom LRA-based deconvolution approaches for advanced imaging applications.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "开发用于编码孔径成像的 Lucy–Richardson 算法的变体：教程",
        "abstract_cn": "<jats:p>反卷积方法最初是为图像去模糊而开发的，是编码孔径成像 (CAI) 技术的基础。其中，半个多世纪前首次推出的 Lucy–Richardson 算法 (LRA) 近年来在 CAI 应用中重新引起人们的兴趣。独特的是，LRA 结合了卷积和互相关运算，后者有效地充当内部反卷积步骤，为创新提供了一个多功能平台。本教程介绍了 CAI 的基础知识以及 LRA 的详细公式。深入探讨了通过修改互相关步骤来增强 LRA 性能的策略。介绍了两种已建立的变体，例如具有幂律变换和有限支撑约束的 LRA、Lucy-Richardson-Rosen 算法以及新颖的扩展，包括互环 LRA。还讨论了针对特定成像场景设计 LRA 变体的未来方向。提供了分步 MATLAB 代码示例，以指导研究人员为高级成像应用开发基于 LRA 的定制反卷积方法。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576392",
        "title": "Dual-stage encryption framework for secure 3D information hiding using single-pixel imaging and diffusion models",
        "link": "https://doi.org/10.1364/josaa.576392",
        "published": "2025-11-19",
        "author": "Enzhi Xu, Chenxing Wang",
        "summary": "<jats:p>With the rapid advancement of three-dimensional (3D) imaging technologies, 3D data has been increasingly utilized in areas such as medical diagnostics, digital heritage preservation, and virtual reality, where protecting sensitive spatial information is essential. To address this need, 3D information hiding aims to securely embed depth data into a host image without introducing perceptible visual distortions. Unlike conventional two-dimensional (2D) information hiding, which focuses on pixel-level planar data, 3D hiding must preserve spatial structure, geometric consistency, and depth fidelity—posing greater challenges in terms of capacity, imperceptibility, and robustness. In this work, we propose, to our knowledge, a novel 3D information hiding method that integrates single-pixel imaging with diffusion models. A dual-stage encryption framework is employed to enhance both confidentiality and imperceptibility. Additionally, we introduce an adaptive weight allocation strategy that prioritizes regions of interest, thereby improving both hiding performance and recovery accuracy. Experimental results demonstrate that the proposed method significantly outperforms existing approaches in terms of hiding and recovery performance, showing strong potential for practical deployment in secure 3D data transmission and storage.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用单像素成像和扩散模型实现安全 3D 信息隐藏的双级加密框架",
        "abstract_cn": "<jats:p>随着三维 (3D) 成像技术的快速发展，3D 数据越来越多地应用于医疗诊断、数字遗产保护和虚拟现实等领域，保护敏感的空间信息至关重要。为了满足这一需求，3D 信息隐藏旨在将深度数据安全地嵌入到宿主图像中，而不会引入可察觉的视觉失真。与专注于像素级平面数据的传统二维 (2D) 信息隐藏不同，3D 隐藏必须保持空间结构、几何一致性和深度保真度，这在容量、不可感知性和鲁棒性方面提出了更大的挑战。在这项工作中，据我们所知，我们提出了一种新颖的 3D 信息隐藏方法，它将单像素成像与扩散模型相结合。采用双级加密框架来增强机密性和不易察觉性。此外，我们引入了一种自适应权重分配策略，可以优先考虑感兴趣的区域，从而提高隐藏性能和恢复精度。实验结果表明，该方法在隐藏和恢复性能方面显着优于现有方法，在安全3D数据传输和存储方面显示出巨大的实际部署潜力。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579506",
        "title": "Wideband aberration-corrected spectrometer using a single toroidal grating: theory and prototype",
        "link": "https://doi.org/10.1364/josaa.579506",
        "published": "2025-11-19",
        "author": "Meixia Chen, Xingshuo Wang, Chengyue Liu",
        "summary": "<jats:p>This work introduces a wideband aberration-corrected high-resolution (WACHR) spectrometer. The design replaces the conventional collimating mirror and planar grating in the Czerny–Turner configuration with a toroidal diffraction grating. Theoretical modeling and optical simulations demonstrate that this configuration simultaneously suppresses astigmatism and wavelength-dependent coma across a broad spectral range. Compared with traditional designs, the proposed spectrometer exhibits superior imaging performance and higher spectral resolution, achieving 0.51–0.68 nm over 400–800 nm. A prototype was fabricated and experimentally validated, and the results confirm the theoretical predictions. These findings highlight the potential of the WACHR spectrometer for compact and high-precision spectral imaging applications.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用单个环形光栅的宽带像差校正光谱仪：理论和原型",
        "abstract_cn": "<jats:p>这项工作介绍了一种宽带像差校正高分辨率 (WACHR) 光谱仪。该设计用环形衍射光栅取代了车尔尼-特纳配置中的传统准直镜和平面光栅。理论建模和光学模拟表明，这种配置可以在宽光谱范围内同时抑制像散和波长相关的慧差。与传统设计相比，所提出的光谱仪表现出卓越的成像性能和更高的光谱分辨率，在400-800 nm范围内实现0.51-0.68 nm。制造了原型并进行了实验验证，结果证实了理论预测。这些发现凸显了 WACHR 光谱仪在紧凑型高精度光谱成像应用中的潜力。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.565097",
        "title": "Three-dimensional reconstruction method for sparsely textured surfaces via freely moving line-structured-light-based binocular stereo vision",
        "link": "https://doi.org/10.1364/josaa.565097",
        "published": "2025-11-24",
        "author": "ShuaiShuai Jiang, Zhou Zhou, Yong Tao, Lei Zhou, Yue Wang, XiangJun Wang",
        "summary": "<jats:p>Three-dimensional (3D) reconstruction using line-structured-light-based stereo vision is one key technique in the field of computer vision measurement. However, most existing stereo vision methods are static and have limited measurement range. To overcome the defect in static stereo vision, a 3D reconstruction method for sparsely textured surfaces via freely moving line-structured-light-based binocular stereo vision is proposed in this paper. The light stripes projected on the surface are captured by left and right cameras, and the pixel coordinates of the light strips are extracted by the Steger algorithm. The 3D coordinates of the light stripes at each position of the cameras are calculated based on the principle of binocular intersection measurement. Subsequently, at least six feature points are required to estimate the coarse relative rotation and translation between adjacent positions, and the corresponding precise values are computed based on minimization of pixel reprojection errors. The camera pose at each position is ultimately obtained through step-by-step backward propagation by using the relative rotation and translation between all adjacent positions. Thus, the 3D shape of the surface is obtained quickly once the light strips captured at different positions are unified to the reference position. Moreover, two metallic surfaces with different geometric configurations are used for the 3D reconstruction test, and the 3D shapes of the metallic surfaces are reconstructed effectively, which proves the generalizability of the proposed method to different surface types. To evaluate the accuracy of 3D reconstruction, the 3D point cloud of one metallic plane is fitted into a plane, and the average vertical distance of the 3D point cloud from the fitting plane is regarded as the evaluation criterion. The experimental results reflect that the average vertical distance in each experimental trial and the average value of the standard deviation of vertical distance do not exceed 1.52 and 0.97 mm, respectively.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于自由移动线结构光双目立体视觉的稀疏纹理表面三维重建方法",
        "abstract_cn": "<jats:p>使用基于线结构光的立体视觉进行三维 (3D) 重建是计算机视觉测量领域的一项关键技术。然而，大多数现有的立体视觉方法是静态的并且测量范围有限。为了克服静态立体视觉的缺陷，本文提出了一种基于自由移动线结构光的双目立体视觉的稀疏纹理表面的3D重建方法。左右摄像头拍摄投射在表面的光带，并通过Steger算法提取光带的像素坐标。根据双目相交测量原理，计算出相机各位置光带的3D坐标。随后，需要至少六个特征点来估计相邻位置之间的粗略相对旋转和平移，并基于像素重投影误差的最小化计算相应的精确值。利用所有相邻位置之间的相对旋转和平移，通过逐步向后传播，最终获得每个位置的相机位姿。因此，一旦在不同位置捕获的光带统一到参考位置，就可以快速获得表面的3D形状。此外，使用两个具有不同几何配置的金属表面进行3D重建测试，并且有效地重建了金属表面的3D形状，证明了该方法对不同表面类型的普适性。为了评价3D重建的精度，将一个金属平面的3D点云拟合到一个平面中，并以3D点云距拟合平面的平均垂直距离作为评价标准。实验结果表明，每次实验的平均垂直距离和垂直距离标准差的平均值分别不超过1.52和0.97毫米。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576467",
        "title": "Off-axis system misalignment detection technique based on hybrid features of the wavelet transform and the Gabor filter",
        "link": "https://doi.org/10.1364/josaa.576467",
        "published": "2025-11-26",
        "author": "Weihua Tang, Wei Tang, Zhengwei Tang, Dong He, Yongmei Huang, Qiang Wang",
        "summary": "<jats:p>\n                    Optical system misalignment aberrations caused by various factors such as environmental conditions and mechanical jitter of optical components can lead to a decline in the imaging quality of the system. The non-rotational symmetry structure and the coupling effect of different degrees of freedom of off-axis reflective telescopes make them more sensitive to misalignment aberrations. Moreover, when the\n                    <jats:italic>Z</jats:italic>\n                    -axis of the secondary mirror in an off-axis system has eccentricity errors, using a defocused PSF to solve the misalignment will result in a decrease in the accuracy of the lateral misalignment solution. Therefore, this paper proposes a misalignment error detection technology based on the wavelet transform and the Gabor filter and establishes a nonlinear mapping relationship between the wavelet transform-Gabor filter hybrid features of defocused PSF images and the misalignment of the secondary mirror through ShuffleNetV2. Using the hybrid features extracted by the biorthogonal wavelet transform and the Gabor filter as the input of the neural network can reduce the difficulty of decoupling the aberration features of defocused PSF images, thereby improving the accuracy of solving the misalignment of the secondary mirror. The algorithm was verified through an actual off-axis two-mirror afocal system, proving its feasibility and effectiveness.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于小波变换和Gabor滤波器混合特征的离轴系统失准检测技术",
        "abstract_cn": "<贾茨：p>\n                    环境条件、光学元件的机械抖动等多种因素引起的光学系统失准像差会导致系统成像质量下降。离轴反射望远镜的非旋转对称结构和不同自由度的耦合效应使其对失准像差更加敏感。此外，当\n                    <jats:斜体>Z</jats:斜体>\n                    离轴系统中副镜的轴存在偏心误差，采用散焦PSF求解失准会导致横向失准求解精度下降。因此，本文提出一种基于小波变换和Gabor滤波器的失准误差检测技术，并通过ShuffleNetV2建立离焦PSF图像的小波变换-Gabor滤波器混合特征与次镜失准之间的非线性映射关系。利用双正交小波变换和Gabor滤波器提取的混合特征作为神经网络的输入，可以降低散焦PSF图像像差特征解耦的难度，从而提高解决次镜失准的精度。通过实际离轴两镜无焦系统验证了该算法的可行性和有效性。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581296",
        "title": "Nontrivial beam modulation with polarization gratings",
        "link": "https://doi.org/10.1364/josaa.581296",
        "published": "2025-12-01",
        "author": "Zixuan Cai, Elvis Pillinen, Meilan Luo, Matias Koivurova",
        "summary": "<jats:p>We underline some of the general properties of polarization gratings, discuss their effect on partially coherent and partially polarized light, and discuss how they can be used to produce exotic polarization states. Specifically, we highlight a particular instability in the modulation of linearly polarized input fields, leading to the formation of spatially variant polarization states. In addition, we introduce what we believe to be a new model field called full Poincaré pulse. Our results provide flexibility for generating complex polarization structures from simple input, with potential applications in structured light and polarization-dependent systems.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用偏振光栅进行非平凡的光束调制",
        "abstract_cn": "<jats:p>我们强调了偏振光栅的一些一般特性，讨论了它们对部分相干光和部分偏振光的影响，并讨论了如何使用它们来产生奇异的偏振态。具体来说，我们强调了线性偏振输入场调制中的特殊不稳定性，导致空间变化的偏振态的形成。此外，我们还介绍了一个新的模型场，称为全庞加莱脉冲。我们的结果为从简单的输入生成复杂的偏振结构提供了灵活性，在结构光和偏振相关系统中具有潜在的应用。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573428",
        "title": "Method of field expansions for doubly layered media with a quasiperiodic interface",
        "link": "https://doi.org/10.1364/josaa.573428",
        "published": "2025-12-03",
        "author": "David P. Nicholls",
        "summary": "<jats:p>Periodic structures play a crucial role in applied optics; however, generalized forms of periodicity are becoming increasingly important in electromagnetics. Due not only to their importance but also to the high cost and great difficulty in producing these at the nanoscale required, numerical simulation of these devices is of extraordinary importance. In this contribution, the author derives, implements, and validates the generalization of a high-order perturbation of surface (HOPS) algorithm (the method of field expansions—FE) for the numerical simulation of layered media scattering to account for quasiperiodic interfaces. Due to their interfacial character, these HOPS approaches are substantially faster than their volumetric counterparts such as finite difference or finite element methods. Additionally, our approach can address structures that the classical FE method would find onerous (for interfaces with widely disparate periods) or impossible (for profiles of incommensurate periods) due to its enhanced capability of simulating quasiperiodic interfaces. Beyond validating the implementation, the author also investigates the (nonlinear) dispersion relation of surface plasmon resonances on a sequence of increasingly challenging vacuum–silver structures featuring a quasiperiodic interface.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "具有准周期界面的双层介质的场展开方法",
        "abstract_cn": "<jats:p>周期性结构在应用光学中发挥着至关重要的作用；然而，广义形式的周期性在电磁学中变得越来越重要。不仅由于它们的重要性，而且由于在纳米尺度上生产它们的成本高且难度大，这些设备的数值模拟非常重要。在这篇文章中，作者推导、实现并验证了高阶表面扰动 (HOPS) 算法（场展开方法 - FE）的推广，用于层状介质散射的数值模拟，以解释准周期界面。由于其界面特性，这些 HOPS 方法比有限差分或有限元方法等体积对应方法快得多。此外，我们的方法可以解决传统有限元方法会发现繁重（对于周期相差很大的界面）或不可能（对于不相称周期的轮廓）的结构，因为它增强了模拟准周期界面的能力。除了验证实现之外，作者还研究了一系列越来越具有挑战性的具有准周期界面的真空-银结构上的表面等离子体共振的（非线性）色散关系。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581149",
        "title": "Field correlations in jet engine exhaust turbulence",
        "link": "https://doi.org/10.1364/josaa.581149",
        "published": "2025-12-04",
        "author": "Yahya Baykal",
        "summary": "<jats:p>Field correlations of collimated Gaussian beams are formulated and examined in jet engine exhaust turbulence. Variations of the field correlations are evaluated against the changes in the parameters of the wireless optical communication link and the jet engine exhaust turbulence. It is found that for all the link and turbulence parameters of interest, as the diagonal distance at the receiver plane increases, the field correlation decreases. Also, at the same diagonal distance from the receiver plane, field correlations tend to become smaller as the receiver points are at a larger distance from the receiver origin, at a smaller source size, wavelength, and inner scale values of jet engine exhaust turbulence. On the other hand, field correlations have a tendency to attain larger values at smaller link length, structure constant, amplitude coefficient for the additional high frequency spectrum area, outer scale of inhomogeneity, and the outer scale values of jet engine exhaust turbulence.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "喷气发动机排气湍流的场相关性",
        "abstract_cn": "<jats:p>在喷气发动机排气湍流中制定并检查了准直高斯光束的场相关性。根据无线光通信链路和喷气发动机排气湍流参数的变化来评估场相关性的变化。研究发现，对于所有感兴趣的链路和湍流参数，随着接收器平面对角线距离的增加，场相关性降低。此外，在距接收器平面相同的对角线距离处，随着接收器点距接收器原点的距离较大、源尺寸、波长和喷气发动机排气湍流的内部尺度值较小，场相关性往往会变得更小。另一方面，在较小的链路长度、结构常数、附加高频频谱区域的振幅系数、不均匀性的外部尺度以及喷气发动机排气湍流的外部尺度值时，场相关性倾向于获得较大的值。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.578628",
        "title": "Automatic detection of dark stain defects in low-light-level image intensifiers based on uniformity correction and background texture filtering",
        "link": "https://doi.org/10.1364/josaa.578628",
        "published": "2025-12-08",
        "author": "Luzi Wang, Huamin Chen",
        "summary": "<jats:p>Dark stain defects are caused by the slow drying of cleaning agents accidentally sprayed on the cathode window during the manufacturing stage of low-light-level (LLL) image intensifiers. Their existence leads to a deterioration in the imaging quality of the device and weakens its target detection capability. The main drawback of the traditional subjective detection method is the introduction of artificial uncertainty, while sensitivity to background uniformity and texture is a weakness of existing objective detection methods. To address the above issues, an automatic detection method for such defects based on uniformity correction and background texture filtering is proposed. First, existing techniques are used to detect and eliminate regular defects in the target image, and a dual uniformity correction strategy based on multi-directional fixed area segmentation and grayscale compensation is utilized to alleviate background unevenness. After obtaining candidate target defects through region detection and region comparison, a texture filtering strategy based on Gaussian differential filtering of spectrograms is adopted to purify the image background. Finally, the frequency-domain bandpass filtering is combined with regional feature analysis to achieve automatic detection of dark stain defects. The experimental results demonstrate that the proposed UC strategy can produce ideal results for the studied case, and the performance of the presented TF method is superior to state-of-the-art relevant techniques. Compared with existing defect detection technologies, the proposed approach has higher detection accuracy, with an average detection accuracy of 94.2% during batch testing. Therefore, this method can be deemed as an effective automatic detection scheme for dark stain defects of LLL image intensifiers.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于均匀性校正和背景纹理过滤自动检测低光图像增强器中的暗污缺陷",
        "abstract_cn": "<jats:p>暗污缺陷是由于在低光 (LLL) 图像增强器的制造阶段意外喷洒在阴极窗口上的清洁剂干燥缓慢造成的。它们的存在会导致设备成像质量下降，削弱其目标检测能力。传统主观检测方法的主要缺点是引入了人为的不确定性，而对背景均匀性和纹理的敏感性是现有客观检测方法的弱点。针对上述问题，提出一种基于均匀性校正和背景纹理过滤的此类缺陷自动检测方法。首先，利用现有技术检测并消除目标图像中的规则缺陷，并利用基于多方向固定区域分割和灰度补偿的双重均匀性校正策略来缓解背景不均匀性。通过区域检测和区域比较获得候选目标缺陷后，采用基于谱图高斯差分滤波的纹理过滤策略来净化图像背景。最后，将频域带通滤波与区域特征分析相结合，实现暗色缺陷的自动检测。实验结果表明，所提出的UC策略可以为研究案例产生理想的结果，并且所提出的TF方法的性能优于最先进的相关技术。与现有的缺陷检测技术相比，该方法具有更高的检测精度，批量测试时平均检测精度为94.2%。因此，该方法不失为一种有效的微光像增强器暗污缺陷自动检测方案。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.578624",
        "title": "Adding personalized wavefront correction to a scleral lens after lens manufacture",
        "link": "https://doi.org/10.1364/josaa.578624",
        "published": "2025-12-18",
        "author": "Evan S. Elam, Nasim Maddah, Alexander W. Schill, Jason D. Marsack",
        "summary": "<jats:p>All past demonstrations of wavefront-guided scleral lenses have relied on the full-surface method (FSM) of lens manufacture, which requires the wavefront correction to be integrated into the design of a traditional scleral lens prior to lens manufacture. A novel patch-cutting method (PCM) of manufacture would rely on carving the wavefront correction into the anterior surface of a previously manufactured scleral lens. This study compared wavefront corrections manufactured with the FSM and PCM. Two duplicates of four unique wavefront-guided scleral lens designs were manufactured using both the FSM and PCM, resulting in a total of 16 test lenses. All lenses were optically profiled, and comparisons were made between each wavefront-guided lens and its design, its within-method of manufacture duplicate, and across methods of manufacture. This study demonstrated that utilizing the PCM does not induce variability in the manufacturing process beyond what is already observed in the FSM.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "在镜片制造后向巩膜镜片添加个性化波前校正",
        "abstract_cn": "<jats:p>过去所有波前引导巩膜镜片的演示都依赖于镜片制造的全表面方法 (FSM)，这需要在镜片制造之前将波前校正集成到传统巩膜镜片的设计中。一种新颖的贴片切割方法（PCM）的制造将依赖于将波前校正雕刻到先前制造的巩膜镜的前表面中。这项研究比较了用 FSM 和 PCM 制造的波前校正。使用 FSM 和 PCM 制造了四种独特的波前引导巩膜镜片设计的两个复制品，总共产生了 16 个测试镜片。所有镜片均经过光学分析，并对每个波前引导镜片及其设计、其内部制造方法内的副本以及不同制造方法进行了比较。这项研究表明，使用 PCM 不会引起制造过程中超出 FSM 中观察到的变化。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573464",
        "title": "Surface orientation ambiguity for single molecules at dielectric interfaces",
        "link": "https://doi.org/10.1364/josaa.573464",
        "published": "2025-12-08",
        "author": "E. Dey, M. Elorza, F. W. Foss, J. J. Gomez Cadenas, B. J. P. Jones",
        "summary": "<jats:p>Fluorescent molecules emit light in a dipole radiation pattern that can be used to infer their orientation through defocused fluorescence microscopy. Proper measurement of the orientation requires mathematical modeling of the radiation pattern expected for a dipole in the geometry of interest and subsequent comparison against experimental data. We point out an ambiguity in common calculations of these patterns that appears to compromise orientation measurements for molecules that are especially near dielectric surfaces. This results in a rotation of the measured emission dipole toward the surface for near-interface molecules, which can be mistaken for a preferentially horizontal orientation among the emitters. The proper treatment for on-surface emitters requires consideration of finite-sized current elements between two dielectric media, and we show that the theoretical ambiguity can be lifted via finite-element modeling. A prescription is provided for correcting measured orientations at arbitrary interfaces.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "介电界面处单分子的表面取向模糊性",
        "abstract_cn": "<jats:p>荧光分子以偶极辐射模式发光，可用于通过散焦荧光显微镜推断其方向。正确测量方向需要对感兴趣的几何形状中的偶极子预期的辐射方向图进行数学建模，并随后与实验数据进行比较。我们指出这些图案的常见计算中存在模糊性，这似乎影响了特别是靠近介电表面的分子的取向测量。这导致测量的发射偶极子向近界面分子的表面旋转，这可能被误认为发射器之间优先水平取向。对表面发射器的正确处理需要考虑两种介电介质之间的有限尺寸电流元素，我们表明可以通过有限元建模消除理论模糊性。提供了用于校正任意界面处测量方向的处方。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.582504",
        "title": "Poincaré sphere representation of the Wigner distribution function of Ince–Gauss beams",
        "link": "https://doi.org/10.1364/josaa.582504",
        "published": "2025-12-03",
        "author": "Agustín Domínguez-Cruz, Julio C. Gutiérrez-Vega",
        "summary": "<jats:p>The analytical expressions for mapping the Wigner distribution function (WDF) of even, odd, and helical Ince–Gauss (IG) beams onto the Poincaré sphere are presented. The analogy between the paraxial wave equation and the two-dimensional quantum harmonic oscillator allows us to use the SU(2) formalism for expressing the WDF in terms of quadratic invariants of the harmonic oscillator. This representation reveals the internal symmetries and patterns that characterize the phase-space structure of IG beams. Numerical simulations confirm the precision of the analytical formulas and illustrate the continuous transition between the Hermite, Ince, and Laguerre–Gauss families as the ellipticity parameter varies.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "因斯-高斯光束的维格纳分布函数的庞加莱球表示",
        "abstract_cn": "<jats:p>提出了将偶数、奇数和螺旋因斯高斯 (IG) 光束的维格纳分布函数 (WDF) 映射到庞加莱球上的解析表达式。近轴波动方程和二维量子谐振子之间的类比允许我们使用 SU(2) 形式来用谐振子的二次不变量来表达 WDF。这种表示揭示了表征 IG 光束相空间结构的内部对称性和图案。数值模拟证实了解析公式的精度，并说明了随着椭圆率参数变化，Hermite、Ince 和 Laguerre-Gauss 族之间的连续转变。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581877",
        "title": "Effect of optically gyrotropic active media on Bessel beams",
        "link": "https://doi.org/10.1364/josaa.581877",
        "published": "2025-12-05",
        "author": "Yuriy Egorov, Valentin Mostovskoy, Alexander Rubass",
        "summary": "<jats:p>Using non-diffracting beams (such as the Bessel beam) as an example, characteristic equations for the constants of beam propagation along the axis of the beam and the amplitude coefficients of the eigenmodes are found. It is shown that the constants of propagation and the amplitude coefficients do not depend on a specific class of non-diffracting beams but are determined exclusively by the characteristics of the crystal and the transverse wave number of the beam, which is specified experimentally. The eigenmodes of all non-diffracting beams are a superposition of TE and TM modes with the corresponding amplitude coefficients.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "光学陀螺活性介质对贝塞尔光束的影响",
        "abstract_cn": "<jats:p>以非衍射光束（如贝塞尔光束）为例，找到了光束沿光束轴传播常数和本征模振幅系数的特征方程。结果表明，传播常数和振幅系数并不取决于特定类别的非衍射光束，而是完全由晶体的特性和通过实验指定的光束横波数决定。所有非衍射光束的本征模都是TE模和TM模与相应振幅系数的叠加。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574046",
        "title": "Phase structure function and spatial coherence in underwater Rayleigh–Bénard turbulence: experimental characterization",
        "link": "https://doi.org/10.1364/josaa.574046",
        "published": "2025-12-10",
        "author": "Ebrahim Mohammadi Razi, Mohsen Dashti",
        "summary": "<jats:p>\n                    This study presents the first experimental characterization of the phase structure function (PSF) and the spatial coherence radius of a laser beam propagating through underwater Rayleigh–Bénard (RB) turbulence, using a two-channel moiré-based wavefront sensor. A collimated laser beam (\n                    <jats:italic>λ</jats:italic>\n                    =532nm, diameter=33mm) was passed horizontally through a temperature-controlled water tank, where convective turbulence was induced by vertical temperature differences (0–6°C). Measurements were performed at multiple vertical (21, 100, and 179 mm) and lateral (21 and 100 mm) positions to examine spatial variations in turbulence. Wavefront phases were reconstructed from moiré fringe patterns, and the PSF was computed in two orthogonal directions transverse to the laser beam’s propagation. The results revealed that underwater RB turbulence is both anisotropic and inhomogeneous in the cross-plane transverse to the beam propagation direction. While the turbulence intensity increases with the imposed temperature difference, the anisotropy observed in the PSF becomes more apparent at larger separation distances\n                    <jats:italic>r</jats:italic>\n                    . The analytical oceanic turbulence optical power spectrum model showed excellent agreement with the experimental PSF within the inertial subrange. Additionally, the spatial coherence radius was found to decrease with increasing temperature difference and was consistently smaller near the tank boundaries. These findings provide unique experimental validation for underwater turbulence models and offer insights critical to the design of robust underwater optical communication systems.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "水下瑞利-贝纳德湍流中的相结构函数和空间相干性：实验表征",
        "abstract_cn": "<贾茨：p>\n                    本研究首次使用基于莫尔条纹的双通道波前传感器对通过水下瑞利-贝纳德 (RB) 湍流传播的激光束的相位结构函数 (PSF) 和空间相干半径进行了实验表征。准直激光束（\n                    <jats:斜体>λ</jats:斜体>\n                    =532nm，直径=33mm）水平通过温控水箱，其中垂直温差（0-6°C）引起对流湍流。在多个垂直（21、100 和 179 毫米）和横向（21 和 100 毫米）位置进行测量，以检查湍流的空间变化。波前相位是根据莫尔条纹图案重建的，并且在横向于激光束传播的两个正交方向上计算PSF。结果表明，水下 RB 湍流在与光束传播方向垂直的横截面中既各向异性又不均匀。虽然湍流强度随着施加的温差而增加，但在 PSF 中观察到的各向异性在间距较大时变得更加明显\n                    <jats:斜体>r</jats:斜体>\n                    。分析的海洋湍流光功率谱模型与惯性子范围内的实验 PSF 非常吻合。此外，还发现空间相干半径随着温差的增加而减小，并且在储罐边界附近始终较小。这些发现为水下湍流模型提供了独特的实验验证，并为设计稳健的水下光通信系统提供了至关重要的见解。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.580640",
        "title": "On color differences in context",
        "link": "https://doi.org/10.1364/josaa.580640",
        "published": "2025-12-10",
        "author": "Lucia Becatti, Beatrice Sarti, Gabriele Simone, Alessandro Rizzi",
        "summary": "<jats:p>Over the last century, the study of color differences has attracted considerable attention, with numerous attempts to develop increasingly accurate perceptual metrics that, however, have progressed incrementally but never achieved full adequacy. Although research eventually moved from color spaces to appearance spaces, color was still examined without accounting for its surrounding context, even though visual context has long been known to strongly influence chromatic appearance. This work attempts to address the following question: is it meaningful to pursue marginal improvements in color-difference metrics that treat color in isolation, when embedding color within a visual context can produce appearance changes far greater than the precision gained by the most recent formulas? More broadly, is it still appropriate to measure color differences without accounting for their visual context? The results underscore the necessity for the development of a color metric that takes into account the spatial computations of the scene, thereby aligning more closely with the mechanisms of human vision.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "关于上下文中的颜色差异",
        "abstract_cn": "<jats:p>在上个世纪，对色差的研究引起了相当大的关注，人们进行了大量尝试来开发越来越准确的感知指标，然而，这些指标已经逐步取得进展，但从未达到完全充分的程度。尽管研究最终从色彩空间转移到外观空间，但仍然在没有考虑其周围环境的情况下对颜色进行了检查，尽管长期以来人们都知道视觉环境会强烈影响色彩外观。这项工作试图解决以下问题：当在视觉环境中嵌入颜色可以产生远远大于最新公式获得的精度的外观变化时，追求单独处理颜色的色差指标的边际改进是否有意义？更广泛地说，在不考虑视觉背景的情况下测量色差是否仍然合适？结果强调了开发颜色度量的必要性，该度量考虑到场景的空间计算，从而更接近人类视觉机制。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576403",
        "title": "Optical quality and anisotropy across the retina of Chinese children’s eyes",
        "link": "https://doi.org/10.1364/josaa.576403",
        "published": "2025-12-15",
        "author": "Simon Winter, Chang Liu, Zhenghua Lin, Pablo Artal, Weizhong Lan, Len Zheleznyak",
        "summary": "<jats:p>\n                    The goal of this study was to characterize the retinal image quality across the retina, in individual children, as a function of wavelength. The wavefront aberrations of 30 Chinese children (ages 12.8±1.8yrs) were measured with a custom-developed Shack–Hartmann wavefront sensor and analyzed at the fovea, 10 deg, 20 deg, and 30 deg in the nasal visual field (VF), −30\n                    <jats:italic>deg</jats:italic>\n                    temporal VF, and ±16\n                    <jats:italic>deg</jats:italic>\n                    in the vertical VF. Wavefront aberrations (e.g., oblique astigmatism) increased in the peripheral retina, and consequently, computed image quality degraded and exhibited a significant directionality. We quantified point spread function (PSF) orientation with optical anisotropy (OA), the ratio of radial by anti-radial (i.e., perpendicular to radial) volumetric modulation transfer functions. We found that the eccentricity-driven increase in OA was wavelength dependent: peripheral PSFs in the blue (405 nm) wavelength became more radial, whereas green (555 nm) and red (695 nm) wavelengths became more anti-radial as a function of eccentricity. Also, the peripheral PSF was more anti-radially oriented in myopes than in emmetropes. Further work is needed to uncover retinal mechanisms which may employ these chromatic and anisotropic cues for the sign of defocus.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "中国儿童眼睛视网膜的光学质量和各向异性",
        "abstract_cn": "<贾茨：p>\n                    这项研究的目的是表征个别儿童视网膜上的视网膜图像质量与波长的关系。使用定制开发的 Shack-Hartmann 波前传感器测量 30 名中国儿童（年龄 12.8±1.8 岁）的波前像差，并在鼻视野 (VF) 的中央凹、10°、20°和 30° 处进行分析，-30\n                    <jats:斜体>度</jats:斜体>\n                    颞 VF，±16\n                    <jats:斜体>度</jats:斜体>\n                    在垂直 VF 中。周边视网膜的波前像差（例如斜散光）增加，因此计算图像质量下降并表现出明显的方向性。我们用光学各向异性 (OA) 来量化点扩散函数 (PSF) 方向，光学各向异性是径向与反径向（即垂直于径向）体积调制传递函数的比率。我们发现偏心率驱动的 OA 增加与波长相关：蓝色 (405 nm) 波长的外围 PSF 变得更加径向，而绿色 (555 nm) 和红色 (695 nm) 波长作为偏心率的函数变得更加反径向。此外，近视眼患者的外周 PSF 比正视眼患者的外周 PSF 更呈反放射状。需要进一步的工作来揭示可能利用这些色彩和各向异性线索来产生散焦迹象的视网膜机制。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.577732",
        "title": "Importance of individual differences in cone spectral sensitivities and color matching functions and how to correct for them: tutorial",
        "link": "https://doi.org/10.1364/josaa.577732",
        "published": "2025-12-19",
        "author": "Andrew T. Rider, Andrew Stockman",
        "summary": "<jats:p>This tutorial covers human cone spectral sensitivities and their linear transformations, the color matching functions. We focus on the mean or standard functions and the individual differences that occur between observers and how we can correct for those differences. The differences arise mainly because of genetically determined spectral shifts of the L- and M-cone photopigment curves, variability in the optical densities of the lens and macular pigments, and variability in the optical densities of the three photopigments. These can lead to people seeing colors on displays and on printed or dyed material differently even though, according to color standards, they should all appear the same. Such discrepancies have become more apparent and their correction more urgent with the emergence of narrow-band light sources and primaries that expand the color gamut. The discrepancies can be reduced by using better color standards, but to eliminate them completely requires corrections be made for individual differences.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "锥体光谱灵敏度和颜色匹配函数个体差异的重要性以及如何纠正它们：教程",
        "abstract_cn": "<jats:p>本教程涵盖了人体锥体光谱灵敏度及其线性变换、颜色匹配函数。我们关注平均函数或标准函数以及观察者之间发生的个体差异，以及如何纠正这些差异。产生差异的主要原因是 L 锥体和 M 锥体感光色素曲线的遗传决定的光谱偏移、晶状体和黄斑色素的光学密度的变化以及三种感光色素的光学密度的变化。这些可能会导致人们在显示器上和印刷或染色材料上看到不同的颜色，即使根据颜色标准，它们应该看起来相同。随着窄带光源和扩大色域的原色的出现，这种差异变得更加明显，其纠正也更加紧迫。通过使用更好的颜色标准可以减少差异，但要完全消除差异，需要针对个体差异进行校正。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579346",
        "title": "Propagation of Laguerre–Gaussian beams in anisotropic atmospheric turbulence: analysis via two analytical and a computational method",
        "link": "https://doi.org/10.1364/josaa.579346",
        "published": "2025-12-19",
        "author": "Elaheh Adams, Miranda van Iersel",
        "summary": "<jats:p>Propagation of an orbital angular momentum beam—the Laguerre–Gaussian (LG) beam—through anisotropic non-Kolmogorov atmospheric turbulence is analyzed using three approaches: the extended Huygens–Fresnel (EHF) principle, the perturbation method, and computational wave-optics simulations (WOS). Two LG beams (modes 1 and 2) are evaluated on how their characteristics change with varying turbulence parameters. Beam irradiance profiles and spot sizes are computed, with the goal of assessing the relative stability of different modes. For the Kolmogorov power-law exponent, the results obtained via the perturbation method show good agreement with those from the EHF principle and WOS but only within the weak-to-moderate fluctuation regime. For stronger turbulence fluctuations, the results obtained via the perturbation method begin to deviate, whereas the EHF principle and WOS remain in good agreement. It is also shown that this type of alignment does not hold in other power-law exponents. Results obtained by the EHF principle and WOS show both the LG modes exhibit deformation under turbulence, though the lower-order mode shows a more pronounced rate of change, and the higher-order mode remains more confined and symmetric. The results obtained by the perturbation method are confirmed or rejected in different scenarios. Results obtained for two LG beams, and via all three methods, show saturation of the degree of ellipticity after some anisotropy ratios.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "拉盖尔-高斯光束在各向异性大气湍流中的传播：通过两种分析方法和计算方法进行分析",
        "abstract_cn": "<jats:p>使用三种方法分析了轨道角动量束（拉盖尔-高斯 (LG) 光束）在各向异性非柯尔莫哥洛夫大气湍流中的传播：扩展惠更斯-菲涅尔 (EHF) 原理、扰动方法和计算波动光学模拟 (WOS)。评估两个 LG 光束（模式 1 和 2），了解它们的特性如何随湍流参数变化而变化。计算光束辐照度分布和光斑尺寸，目的是评估不同模式的相对稳定性。对于 Kolmogorov 幂律指数，通过微扰法获得的结果与 EHF 原理和 WOS 的结果非常吻合，但仅在弱到中度波动范围内。对于较强的湍流波动，通过摄动方法获得的结果开始出现偏差，而EHF原理和WOS仍然保持良好的一致性。研究还表明，这种类型的对齐在其他幂律指数中并不成立。 EHF 原理和 WOS 获得的结果表明，两种 LG 模式在湍流下都表现出变形，尽管低阶模式显示出更明显的变化率，而高阶模式仍然更加受限和对称。微扰法得到的结果在不同的场景下得到证实或拒绝。通过所有三种方法获得的两根 LG 光束的结果显示，经过一定的各向异性比后，椭圆度达到饱和。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581349",
        "title": "Manipulation of the energy flow field in Bessel–Gaussian beams by polarization",
        "link": "https://doi.org/10.1364/josaa.581349",
        "published": "2025-12-18",
        "author": "Shuo Yu, Jinsong Li, Chundi Zheng",
        "summary": "<jats:p>\n                    Based on vector diffraction theory, this paper investigates the energy flow distribution and its variation characteristics of Bessel–Gaussian beams under polarization modulation. The results demonstrate that on the focal plane, the transverse energy flow with nearly zero intensity and the longitudinal energy flow with very high intensity are obtained, with the longitudinal energy flow dominating the total energy flow field. Furthermore, by adjusting the values of the azimuthal index\n                    <jats:italic>m</jats:italic>\n                    and the radial index\n                    <jats:italic>n</jats:italic>\n                    , vortex-shaped energy flow distributions can be achieved. The degree of vorticity and the number of energy flow spots can be controlled by adjusting the magnitude of the azimuthal index\n                    <jats:italic>m</jats:italic>\n                    , while the direction of the vortex is manipulated by altering the signs of the azimuthal index\n                    <jats:italic>m</jats:italic>\n                    and the radial index\n                    <jats:italic>n</jats:italic>\n                    . By adjusting the magnitude of the radial index\n                    <jats:italic>n</jats:italic>\n                    , size-tunable energy flow rings can be obtained. This polarization modulation strategy offers a novel approach for the flexible manipulation of focused optical fields. Furthermore, the characteristics of energy flow distributions and their dynamic variations will pave the way for new approaches in optical particle manipulation and trapping.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "通过偏振操纵贝塞尔-高斯光束的能量流场",
        "abstract_cn": "<贾茨：p>\n                    基于矢量衍射理论，研究了偏振调制下贝塞尔-高斯光束的能流分布及其变化特征。结果表明，在焦平面上，得到了强度接近为零的横向能流和强度很高的纵向能流，其中纵向能流在总能流场中占主导地位。此外，通过调整方位角指数的值\n                    <贾茨：斜体>米</贾茨：斜体>\n                    和径向指数\n                    <jats:斜体>n</jats:斜体>\n                    ，可以实现涡形能量流分布。通过调整方位角指数的大小可以控制涡度和能量流点的数量\n                    <贾茨：斜体>米</贾茨：斜体>\n                    ，而涡旋的方向是通过改变方位角指数的符号来操纵的\n                    <贾茨：斜体>米</贾茨：斜体>\n                    和径向指数\n                    <jats:斜体>n</jats:斜体>\n                    。通过调整径向指数的大小\n                    <jats:斜体>n</jats:斜体>\n                    ，可以获得尺寸可调的能量流环。这种偏振调制策略为灵活操纵聚焦光场提供了一种新颖的方法。此外，能量流分布的特征及其动态变化将为光学粒子操纵和捕获的新方法铺平道路。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581604",
        "title": "Optical particle trapping with focused pseudo-Schell model beams",
        "link": "https://doi.org/10.1364/josaa.581604",
        "published": "2025-12-15",
        "author": "Hao Lin, Mengwen Guo, Linfei Chen, Haidan Mao",
        "summary": "<jats:p>Based on pseudo-modal expansions of partially coherent beams, an analytical expression for the cross-spectral density function of pseudo-Schell model beams through an ABCD optical system is derived. The intensity distributions calculated using analytical method show excellent agreement with those by numerical integration. The radiation forces of focused pseudo-Schell model beams on Rayleigh dielectric particles are investigated. It is found that focused Gaussian pseudo-Schell model (GPSM) beams can stably trap high-refractive-index particles at the focus, while focused vortex Gaussian pseudo-Schell model (VGPSM) beams can trap low- and high-refractive-index particles simultaneously. Compared with focused Gaussian Schell-model (GSM) beams, focused GPSM beams generate stronger radiation forces under identical beam parameters, making them more effective for particle trapping. The vortex Gaussian Schell-model (VGSM) beams with a large coherence length exhibit a particle trapping capability comparable to that of VGPSM beam. The pseudo-modal expansion method of pseudo-Schell model beams through an ABCD optical system reduces the dimensionality of the integration and facilitates the synthesis and analysis of such beams in propagation. The findings provide a theoretical basis for the use of both GPSM beams and VGPSM beams in particle trapping.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用聚焦伪谢尔模型光束捕获光学粒子",
        "abstract_cn": "<jats:p>基于部分相干光束的赝模态展开，推导了通过ABCD光学系统的赝谢尔模型光束的交叉谱密度函数的解析表达式。使用解析方法计算的强度分布与数值积分计算的强度分布表现出极好的一致性。研究了聚焦伪谢尔模型光束在瑞利介电粒子上的辐射力。研究发现，聚焦高斯伪谢尔模型（GPSM）光束可以在焦点处稳定地捕获高折射率粒子，而聚焦涡旋高斯伪谢尔模型（VGPSM）光束可以同时捕获低折射率和高折射率粒子。与聚焦高斯谢尔模型 (GSM) 光束相比，聚焦 GPSM 光束在相同光束参数下产生更强的辐射力，使其更有效地捕获粒子。具有大相干长度的涡旋高斯谢尔模型（VGSM）光束表现出与 VGPSM 光束相当的粒子捕获能力。伪谢尔模型光束通过ABCD光学系统的伪模态展开方法降低了积分的维数，有利于此类光束在传播中的合成和分析。研究结果为GPSM光束和VGPSM光束在粒子捕获中的应用提供了理论依据。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579066",
        "title": "Analytical one-dimensional phase gratings in scalar diffraction theory",
        "link": "https://doi.org/10.1364/josaa.579066",
        "published": "2025-12-18",
        "author": "Andrew Ducharme, Kyle Cole, Devon Ball, Benjamin J. McMorran",
        "summary": "<jats:p>\n                    Diffraction gratings are a commonplace optical tool which split an incident beam into distinct diffraction orders via the far-field interference of light. However, using scalar diffraction theory to model a phase grating with phase profile\n                    <jats:italic>ϕ</jats:italic>\n                    (\n                    <jats:italic>x</jats:italic>\n                    ) requires Fourier transforming\n                    <jats:italic>exp</jats:italic>\n                    ⁡[\n                    <jats:italic>i</jats:italic>\n                    <jats:italic>ϕ</jats:italic>\n                    (\n                    <jats:italic>x</jats:italic>\n                    )], which predominantly requires numerical calculations outside of a few standard geometries. We present three designs, the full- and half-wave rectified sine wave and the parabola, which have exact analytical closed-form (non-numerical) intensities in every diffraction order. We also analytically describe the diffraction from a known profile interspersed with finite regions of no phase modulation. We fabricate full- and half-wave rectified gratings and demonstrate their electron diffraction. These designs are of the most use for small-wavelength optics with X-rays, electrons, neutrons, and other massive quanta.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "标量衍射理论中的解析一维相位光栅",
        "abstract_cn": "<贾茨：p>\n                    衍射光栅是一种常见的光学工具，它通过光的远场干涉将入射光束分成不同的衍射级。然而，使用标量衍射理论来模拟具有相位轮廓的相位光栅\n                    <jats:斜体> phi</jats:斜体>\n                    (\n                    <jats:斜体>x</jats:斜体>\n                    ) 需要傅里叶变换\n                    <jats:斜体>exp</jats:斜体>\n                    ⁡[\n                    <贾茨：斜体>我</贾茨：斜体>\n                    <jats:斜体> phi</jats:斜体>\n                    (\n                    <jats:斜体>x</jats:斜体>\n                    )]，这主要需要一些标准几何形状之外的数值计算。我们提出了三种设计，即全波和半波整流正弦波和抛物线，它们在每个衍射级中都具有精确的解析闭合形式（非数值）强度。我们还分析地描述了散布有无相位调制的有限区域的已知轮廓的衍射。我们制造全波和半波整流光栅并演示它们的电子衍射。这些设计最适用于 X 射线、电子、中子和其他大量子的小波长光学。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.582536",
        "title": "Quantifying the consequence of anterior-surface polishing time on the optical performance of wavefront-guided scleral lenses",
        "link": "https://doi.org/10.1364/josaa.582536",
        "published": "2026-01-05",
        "author": "Nasim Maddah, Alexander W. Schill, Evan S. Elam, Lan Chi Nguyen, Jason D. Marsack",
        "summary": "<jats:p>The process for manufacturing wavefront-guided scleral lenses includes polishing both the anterior and posterior lens surfaces. The purpose of this study was to determine if extended front-surface polishing durations induce aberration changes in wavefront-guided scleral lenses that exceed manufacturing tolerances. Two sets of six scleral lenses were manufactured. Each set contained one reference lens with only sphere correction and five wavefront-guided scleral lenses, all of which shared the same wavefront-correcting optics. After cutting, the anterior surfaces of the five wavefront-guided lenses in both sets were polished for differing durations. Set 1 lenses were polished with a used polishing cloth, and Set 2 were polished with a brand-new polishing cloth. Aberrations in each polished lens were measured with a wavefront sensor and examined as a function of polishing time. Changes in aberrations that would be clinically significant and/or greater than manufacturing tolerances were not observed for the material or polishing times studied or the remaining lifetime of the polishing cloth.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "量化前表面抛光时间对波前引导巩膜镜片光学性能的影响",
        "abstract_cn": "<jats:p>制造波前引导巩膜镜片的过程包括抛光镜片前表面和后表面。本研究的目的是确定延长前表面抛光持续时间是否会导致波前引导巩膜镜片的像差变化超出制造公差。制造了两套六个巩膜镜片。每组包含一个仅具有球面校正的参考镜片和五个波前引导巩膜镜片，所有这些镜片都共享相同的波前校正光学器件。切割后，两组中五个波前引导镜片的前表面被抛光不同的持续时间。第 1 组镜片使用旧抛光布进行抛光，第 2 组镜片使用全新抛光布进行抛光。使用波前传感器测量每个抛光镜片的像差，并将其作为抛光时间的函数进行检查。对于所研究的材料或抛光时间或抛光布的剩余寿命，没有观察到具有临床意义和/或大于制造公差的像差变化。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581912",
        "title": "Field correlations of a Gaussian vortex laser beam in vertical turbulent oceanic links",
        "link": "https://doi.org/10.1364/josaa.581912",
        "published": "2025-12-23",
        "author": "Hamza Gerçekcioğlu, Yahya Baykal",
        "summary": "<jats:p>\n                    Utilizing the extended Huygens–Fresnel principle, field correlations of a Gaussian vortex beam propagating in the vertical turbulent oceanic link are examined analytically and evaluated by simulation in the Atlantic Ocean at low- and mid-latitude and high-latitude summer. Our formulation is based on the coherence length of a spherical wave operating at the depth range between 3000 and 3500 m. Variations in the rate of dissipation of turbulent kinetic energy per unit mass of fluid\n                    <jats:italic>ε</jats:italic>\n                    , the rate of dissipation of the mean-squared temperature\n                    <jats:italic>χ</jats:italic>\n                    <jats:sub>\n                      <jats:italic>T</jats:italic>\n                    </jats:sub>\n                    , and the ratio of temperature to salinity contributions to the refractive index spectrum\n                    <jats:italic>ω</jats:italic>\n                    are taken into account at these depths in the underwater turbulent medium. The field correlation obtained using the coherence length found with the help of the depth-dependent power spectrum is expressed in detail. When the topological charge is selected considering the source size and propagation distance, it is seen that the normalized field correlation of the Gaussian vortex beam gives better results as compared to Gaussian beams.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "垂直湍流海洋链中高斯涡旋激光束的场相关性",
        "abstract_cn": "<贾茨：p>\n                    利用扩展的惠更斯-菲涅耳原理，对中低纬度和高纬度夏季大西洋中垂直湍流海洋链路中传播的高斯涡旋光束的场相关性进行了分析和模拟评估。我们的公式基于在 3000 至 3500 米深度范围内工作的球面波的相干长度。每单位质量流体的湍流动能耗散率的变化\n                    <jats:斜体>ε</jats:斜体>\n                    , 均方温度的耗散率\n                    <jats:斜体>χ</jats:斜体>\n                    <贾茨：子>\n                      <贾茨：斜体>T</贾茨：斜体>\n                    </贾茨：子>\n                    ，以及温度与盐度对折射率光谱的贡献之比\n                    <贾茨：斜体>ω</贾茨：斜体>\n                    在水下湍流介质的这些深度处被考虑。使用借助深度相关功率谱找到的相干长度获得的场相关性被详细表达。当考虑源尺寸和传播距离来选择拓扑电荷时，可以看出，与高斯光束相比，高斯涡旋光束的归一化场相关给出了更好的结果。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.580304",
        "title": "Design of a direct vision spectrometer with a double Amici prism for smile correction",
        "link": "https://doi.org/10.1364/josaa.580304",
        "published": "2026-01-08",
        "author": "Xiaolong Zhang, Haoshan Hao, Hui Du",
        "summary": "<jats:p>Traditional prism imaging spectrometers suffer from poor direct vision, low spectral linearity, and a large natural smile. Spurred by these limitations, this paper proposes a spectral imaging system based on a double Amici prism, for which the vector refraction law yields expressions for the prism’s smile and keystone. Accurate calculations of the smile and keystone for the single and compound prisms are derived using MATLAB. The characteristic curve of a smile is drawn and verified through simulations, with the designed system’s waveband ranging from 400 to 1000 nm, spectral resolution across the full waveband exceeding 12 nm, and the maximum smile and keystone values of 3.117 and 3.955 µm, respectively. Notably, the developed system is completely coaxial and meets the requirements of direct vision and smile correction. Furthermore, the new, to our knowledge, method, which combines the vector refraction law with MATLAB programming, is adopted, enabling quick and accurate determination of the smile characteristic of compound prisms. Overall, this paper has general significance for the design of the prism spectral imaging system for smile correction.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "双Amici棱镜微笑矫正直视光谱仪的设计",
        "abstract_cn": "<jats:p>传统的棱镜成像光谱仪存在直视效果差、光谱线性度低、笑容大等问题。受这些限制的影响，本文提出了一种基于双阿米西棱镜的光谱成像系统，其中矢量折射定律产生了棱镜的微笑和梯形失真的表达式。使用 MATLAB 对单棱镜和复合棱镜的微笑和梯形失真进行了精确计算。通过仿真绘制并验证了微笑特征曲线，设计系统的波段范围为400至1000 nm，全波段光谱分辨率超过12 nm，最大微笑值和梯形失真值分别为3.117和3.955 µm。值得注意的是，所开发的系统完全同轴，满足直视和微笑矫正的要求。此外，据我们所知，采用矢量折射定律与MATLAB编程相结合的新方法，能够快速、准确地测定复合棱镜的微笑特性。总体而言，本文对于微笑矫正棱镜光谱成像系统的设计具有普遍意义。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579577",
        "title": "Pupil aberration coefficients in plane-symmetric optical systems",
        "link": "https://doi.org/10.1364/josaa.579577",
        "published": "2026-01-08",
        "author": "Jessica A. Steidle, Joseph M. Howard, Jannick P. Rolland",
        "summary": "<jats:p>As optical systems are expected to meet increasingly strict demands, controlling pupil aberrations is increasingly important in optical design. Building on prior work on image aberration theory and leveraging the connection between pupil and image aberrations, we derive analytical expressions for 50 intrinsic pupil aberration coefficients for plane-symmetric systems, supported by both algebraic and numerical validation.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "平面对称光学系统中的光瞳像差系数",
        "abstract_cn": "<jats:p>随着光学系统需要满足越来越严格的要求，控制光瞳像差在光学设计中变得越来越重要。基于图像像差理论的先前工作并利用光瞳和图像像差之间的联系，我们推导了平面对称系统的 50 个本征光瞳像差系数的解析表达式，并得到代数和数值验证的支持。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.583999",
        "title": "Objects invisible from multiple directions",
        "link": "https://doi.org/10.1364/josaa.583999",
        "published": "2025-12-22",
        "author": "Ray Abney, Greg Gbur",
        "summary": "<jats:p>The first proposed invisibility cloaks required materials that are highly anisotropic, spatially inhomogeneous, and that possess a magnetic response. These properties are still difficult or impractical to achieve in practice, leading many researchers to explore simplified invisibility schemes that trade perfection for simplicity in design. In this paper, we investigate a traditional method by Devaney for constructing multi-angle invisibility devices, i.e., devices that are invisible for a finite number of directions of illumination in the weak scattering limit. We demonstrate that the scattering cross-section of these objects decreases dramatically as the number of invisibility directions is increased.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "物体从多个方向不可见",
        "abstract_cn": "<jats:p>第一个提出的隐形斗篷需要高度各向异性、空间不均匀且具有磁响应的材料。这些特性在实践中仍然很难或不切实际，导致许多研究人员探索简化的隐形方案，以牺牲完美来换取设计的简单性。在本文中，我们研究了 Devaney 构建多角度隐形设备的传统方法，即在弱散射极限下对有限数量的照明方向不可见的设备。我们证明，随着不可见方向数量的增加，这些物体的散射截面急剧减小。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579369",
        "title": "Adaptive Bayesian augmented Lagrangian algorithm for fluorescence molecular tomography",
        "link": "https://doi.org/10.1364/josaa.579369",
        "published": "2025-12-24",
        "author": "Bianbian Yang, Yiting He, Nannan Cai, Jun Zhang, Yi Chen, Yangyang Liu, Chengyi Gao, Huangjian Yi, Xin Cao",
        "summary": "<jats:p>\n                    Fluorescence molecular tomography (FMT) is a noninvasive imaging technique that enables the quantitative three-dimensional reconstruction of fluorescent probe distributions\n                    <jats:italic toggle=\"yes\">in vivo</jats:italic>\n                    . However, FMT reconstruction is limited in accuracy and reliability due to light scattering and the ill-posed inverse problem. In this paper, the adaptive Bayesian augmented Lagrangian (ABAL) algorithm is proposed, which adaptively adjusts the regularization parameter to promote sparsity and enhance robustness to noise, while significantly improving computational efficiency. By integrating sparse Bayesian learning (SBL) with the augmented Lagrangian (AL) framework, the approach addresses the computational challenges and non-convexity introduced by the iterative adjustment of regularization parameters in SBL. The inverse problem is reformulated as a weighted\n                    <jats:italic>L</jats:italic>\n                    <jats:sub>1</jats:sub>\n                    minimization with adaptive regularization and solved via the AL method, enhancing computational efficiency and mitigating the risk of local minima. Moreover, the adaptive regularization mechanism enables the method to dynamically adjust to data-specific characteristics, avoiding over-regularization or under-regularization and improving both stability and reconstruction accuracy. To evaluate the effectiveness of our method, a series of numerical simulations and implantation experiments were conducted. Results confirm that the ABAL method can achieve relatively accurate reconstruction performance compared to other approaches, with an average minimum localization error (LE) of 0.358 mm and an average Dice coefficient of 0.775. These results show relatively high localization accuracy, shape recovery, and robustness of the ABAL method in FMT reconstruction, indicating its potential for practical FMT application.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "用于荧光分子断层扫描的自适应贝叶斯增强拉格朗日算法",
        "abstract_cn": "<贾茨：p>\n                    荧光分子断层扫描（FMT）是一种非侵入性成像技术，能够定量重建荧光探针分布的三维\n                    <jats:italictoggle=\"yes\">体内</jats:italic>\n                    。然而，由于光散射和不适定逆问题，FMT重建的准确性和可靠性受到限制。本文提出了自适应贝叶斯增强拉格朗日（ABAL）算法，该算法自适应地调整正则化参数以促进稀疏性并增强对噪声的鲁棒性，同时显着提高计算效率。通过将稀疏贝叶斯学习 (SBL) 与增强拉格朗日 (AL) 框架相集成，该方法解决了 SBL 中正则化参数迭代调整带来的计算挑战和非凸性。反问题被重新表述为加权\n                    <贾茨：斜体>L</贾茨：斜体>\n                    <jats:sub>1</jats:sub>\n                    通过自适应正则化进行最小化，并通过 AL 方法求解，提高计算效率并减轻局部最小值的风险。此外，自适应正则化机制使该方法能够动态调整数据特定的特征，避免过度正则化或欠正则化，提高稳定性和重建精度。为了评估我们方法的有效性，进行了一系列数值模拟和植入实验。结果证实，与其他方法相比，ABAL 方法可以实现相对准确的重建性能，平均最小定位误差 (LE) 为 0.358 mm，平均 Dice 系数为 0.775。这些结果显示了ABAL方法在FMT重建中相对较高的定位精度、形状恢复和鲁棒性，表明了其实际FMT应用的潜力。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574207",
        "title": "Influence of irregular particles on the propagation of polarized pulsed laser beams through turbid underwater environments",
        "link": "https://doi.org/10.1364/josaa.574207",
        "published": "2025-12-23",
        "author": "Maria Ballesta-Garcia, Aleix R. Bobi-Olmo, Santiago Royo",
        "summary": "<jats:p>This study investigates polarized pulsed laser propagation in turbid\n\t\t\t\tunderwater environments by combining Monte Carlo simulations with the\n\t\t\t\tT-matrix scattering method to model irregularly shaped particles. Using\n\t\t\t\tsand and algae as representative scatterers, we examined the effects of\n\t\t\t\tparticle shape, composition, and turbidity on backscattered energy,\n\t\t\t\tpolarization, and temporal pulse characteristics. Results show that\n\t\t\t\tcircular polarization is more resilient to depolarization, especially in\n\t\t\t\tlow-absorption scattering media such as sand. Flattened particles enhance\n\t\t\t\tbackscattering, while elongated ones favor forward scattering. Algae, due\n\t\t\t\tto their absorption and complexity, induce stronger depolarization. These\n\t\t\t\tfindings highlight the importance of realistic particle modeling for\n\t\t\t\tadvancing underwater optical communication and imaging technologies.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "不规则颗粒对偏振脉冲激光束在浑浊水下环境中传播的影响",
        "abstract_cn": "<jats:p>这项研究调查了混浊中的偏振脉冲激光传播\n\t\t\t\t通过将蒙特卡罗模拟与水下环境相结合\n\t\t\t\tT 矩阵散射法对不规则形状颗粒进行建模。使用\n\t\t\t\t沙子和藻类作为代表性散射体，我们研究了\n\t\t\t\t反向散射能量的颗粒形状、成分和浊度，\n\t\t\t\t偏振和时间脉冲特性。结果表明\n\t\t\t\t圆偏振对去偏振更有弹性，特别是在\n\t\t\t\t低吸收散射介质，例如沙子。扁平颗粒增强\n\t\t\t\t后向散射，而拉长的则有利于前向散射。藻类，由于\n\t\t\t\t由于它们的吸收和复杂性，会引起更强的去极化。这些\n\t\t\t\t研究结果强调了现实粒子建模的重要性\n\t\t\t\t推进水下光通信和成像技术。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.576175",
        "title": "Comparison of the unmodified Rytov method and the modified Rytov method in obtaining scintillations in various strongly turbulent media",
        "link": "https://doi.org/10.1364/josaa.576175",
        "published": "2025-12-23",
        "author": "Yahya Baykal",
        "summary": "<jats:p>The scintillation index as evaluated by the unmodified (classical) Rytov method solution for weak turbulence and evaluated by the modified or the extended Rytov method solution for strong turbulence is compared in different turbulent media, such as non-Kolmogorov atmospheric, non-Kolmogorov jet engine exhaust, marine atmospheric, and oceanic turbulences. When the scintillations are evaluated against the turbulence strength for various non-Kolmogorov power law and source sizes, the distinction between the modified Rytov and the unmodified Rytov method solutions, as the strength of turbulence increases, is clearly observed in all the turbulent media. This distinction is emphasized when the comparison is made at larger power law and source sizes. The results in this paper will be helpful in optical wireless communication system performance evaluations.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "未改进的Rytov方法和改进的Rytov方法在各种强湍流介质中获得闪烁的比较",
        "abstract_cn": "<jats:p>在不同的湍流介质（例如非柯尔莫哥洛夫大气、非柯尔莫哥洛夫喷气发动机废气、海洋大气和海洋湍流）中，比较了通过未修改（经典）Rytov 方法解评估的弱湍流和通过修改或扩展的 Rytov 方法解评估的强湍流的闪烁指数。当根据各种非柯尔莫哥洛夫幂律和源尺寸的湍流强度评估闪烁时，随着湍流强度的增加，在所有湍流介质中都可以清楚地观察到修改后的 Rytov 和未修改的 Rytov 方法解决方案之间的区别。当在较大的幂律和源尺寸下进行比较时，这种区别会被强调。本文的研究结果对于光无线通信系统的性能评估具有一定的参考价值。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.577930",
        "title": "Multi-aperture system crossing using the extended\n                    <i>ABCD</i>\n                    formalism and mode conversion",
        "link": "https://doi.org/10.1364/josaa.577930",
        "published": "2025-12-24",
        "author": "Billel Bentouhami, Zaia Derrar Kaddour",
        "summary": "<jats:p>\n                    The aim of this paper is to solve the problem raised by the study of diffraction by a sequence of irises. Alternative methods to the diffraction integral, aimed at this problem, exist. However, these methods not only fail to provide results for the Fresnel region, but none of them address the scenario involving a large sequence of apertures. The method used here is simply based on mode conversion and the\n                    <jats:italic toggle=\"yes\">ABCD</jats:italic>\n                    formalism. We first provide the method assessment by studying an arbitrary sequence of three apertures. Using the well-known diffraction integral and our method, the transverse field distribution beyond each aperture was determined, and the accuracy of the superposition was successfully confirmed. Going beyond the set of few aperture studies, already available in the literature, we give field distributions from diverse sequences with a significant number of irises. Different sizes, numbers, and distances between apertures are considered, and interesting field distributions are obtained. Advantages of this kind of optical system are illustrated in this paper.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用扩展<i>ABCD</i>形式主义和模式转换的多孔径系统交叉",
        "abstract_cn": "<贾茨：p>\n                    本文的目的是解决研究一系列虹膜衍射所提出的问题。针对这个问题，存在衍射积分的替代方法。然而，这些方法不仅无法提供菲涅耳区域的结果，而且都没有解决涉及大孔径序列的情况。这里使用的方法只是基于模式转换和\n                    <jats：斜体切换=“是”> ABCD </ jats：斜体>\n                    形式主义。我们首先通过研究三个孔径的任意序列来提供方法评估。使用众所周知的衍射积分和我们的方法，确定了每个孔径之外的横向场分布，并成功确认了叠加的准确性。除了文献中已有的少数孔径研究之外，我们还给出了具有大量虹膜的不同序列的场分布。考虑不同的尺寸、数量和孔径之间的距离，并获得有趣的场分布。本文阐述了这种光学系统的优点。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.582595",
        "title": "Polarization intrinsic coherence Poincaré sphere",
        "link": "https://doi.org/10.1364/josaa.582595",
        "published": "2025-12-24",
        "author": "Philippe Réfrégier, Frédéric Galland, Julien Fade",
        "summary": "<jats:p>A Poincaré-like sphere for spatial coherence characteristics leading to “visual algorithms” is introduced. Deterministic and random Jones transformations, as well as coherence optimization between lights at two spatial locations, can be apprehended with simple geometric transformations analogous to the ones used with the standard Poincaré sphere for polarization. The joint representation of polarization and coherence characteristics in a single global polarization intrinsic coherence Poincaré sphere allows one to easily identify remarkable physical situations.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "偏振内在相干庞加莱球",
        "abstract_cn": "<jats:p>引入了用于空间相干特性的类庞加莱球体，从而产生了“视觉算法”。确定性和随机琼斯变换，以及两个空间位置的光之间的相干性优化，可以通过简单的几何变换来理解，类似于用于偏振的标准庞加莱球所使用的几何变换。单个全局偏振内在相干庞加莱球中偏振和相干特性的联合表示使人们能够轻松识别显着的物理情况。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.580042",
        "title": "Influence of phase filter shape and size on phase contrast imaging",
        "link": "https://doi.org/10.1364/josaa.580042",
        "published": "2025-12-23",
        "author": "T. B. Martínez-Hernández, J. A. Zenteno-Hernández, D. Sánchez-de-la-Llave, M. D. Iturbe-Castillo",
        "summary": "<jats:p>Phase contrast is a technique that allows the visualization of phase variations of an object by transforming them into intensity variations at the image plane when a phase filter is set at the Fourier plane of the optical system. Ideally, only the zero frequency of the object’s Fourier transform must be affected by the phase filter to have an intensity image that follows the object’s phase variations. The implementation of the technique in real systems requires a filter with a defined spatial size, and therefore, more Fourier frequencies can be phase shifted. In this work, we investigate the impact of the phase filter shape and size on the image contrast. Phase objects with binary and quadratic phase distribution are analyzed. The shape of the filter was modified from an abrupt binary to a smooth phase variation. We specifically examined four types of filters: disk, stepped conical, continuous conical, and Gaussian. Size variations were achieved by varying the radii of the filters, while maintaining their constant maximum phase values. The results demonstrate that the shape and size of the filter affect the obtained image, showing that filters with smooth variation maintain their capability for contrast even for radii larger than that of the main lobe of the object Fourier transform. Experimental results using a phase spatial light modulator to control the phase, form, and radius variation of the phase filter are presented. High consistency is achieved between numerical and experimental results.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "相位滤波器形状和尺寸对相差成像的影响",
        "abstract_cn": "<jat:p>相衬是一种技术，当在光学系统的傅里叶平面上设置相位滤波器时，通过将物体的相位变化转换为图像平面上的强度变化，可以将物体的相位变化可视化。理想情况下，只有物体傅立叶变换的零频率必须受到相位滤波器的影响，才能获得跟随物体相位变化的强度图像。在实际系统中实施该技术需要具有定义的空间尺寸的滤波器，因此可以相移更多的傅里叶频率。在这项工作中，我们研究了相位滤波器形状和尺寸对图像对比度的影响。分析具有二元和二次相位分布的相位对象。滤波器的形状从突变的二进制修改为平滑的相位变化。我们专门研究了四种类型的滤波器：盘式滤波器、阶梯式圆锥滤波器、连续圆锥滤波器和高斯滤波器。尺寸变化是通过改变滤波器的半径来实现的，同时保持其恒定的最大相位值。结果表明，滤波器的形状和尺寸会影响所获得的图像，表明即使半径大于对象傅里叶变换的主瓣，具有平滑变化的滤波器也能保持其对比度能力。给出了使用相位空间光调制器来控制相位滤波器的相位、形状和半径变化的实验结果。数值结果与实验结果具有较高的一致性。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.582004",
        "title": "High-density three-dimensional image reconstruction using rapid modulation of light",
        "link": "https://doi.org/10.1364/josaa.582004",
        "published": "2026-01-05",
        "author": "Jorge-Alberto Peralta-Ángeles, Mingyuan Hong, Mario\n\t\t\tA. Quiroz-Juárez, Omar\n\t\t\tS. Magaña-Loaiza, Roberto de\n\t\t\tJ. León-Montiel",
        "summary": "<jats:p>One of the most common methods for reconstructing three-dimensional (3D)\n\t\t\t\timages of real or computer-generated objects is digital and\n\t\t\t\tcomputer-generated holography, respectively. Both techniques rely on the\n\t\t\t\tuse of electro-optical devices that modify the phase or amplitude of light\n\t\t\t\tfields in a controlled manner, the so-called spatial light modulators.\n\t\t\t\tHowever, given that holography typically requires coherent light sources,\n\t\t\t\ta common problem with three-dimensional projection is the crosstalk\n\t\t\t\tbetween the layers that make up the 3D object. This limits full-depth\n\t\t\t\tcontrol and directly affects image quality. Interestingly, in the past few\n\t\t\t\tyears, several methods have proven to be effective in breaking layer\n\t\t\t\tcrosstalk by erasing the spatial coherence of light. A drawback of such\n\t\t\t\tsolutions is that, in many cases, additional optical resources are\n\t\t\t\trequired to achieve such a task. In this work, we present a method for\n\t\t\t\thigh-density reconstruction of three-dimensional objects using rapid\n\t\t\t\tmodulation of light fields by means of digital micromirror devices (DMDs).\n\t\t\t\tThe 3D reconstruction is performed by discretizing the object into\n\t\t\t\tmultiplane light-point contours, where the resolution of the contours is\n\t\t\t\tcontrolled by the density of the light points. The high refresh rate of\n\t\t\t\tthe DMD\n\t\t\t\t(∼10kHz) allows for a\n\t\t\t\treconstruction where each point of the 3D image is spatially and\n\t\t\t\ttemporally controlled by independent amplitude holograms, thus effectively\n\t\t\t\teliminating coherence-induced multiplane crosstalk without the need for\n\t\t\t\tadditional optical elements. Because of its simplicity and versatility, we\n\t\t\t\tbelieve that our method provides a practical route toward compact,\n\t\t\t\thigh-resolution 3D holographic projectors.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用快速光调制的高密度三维图像重建",
        "abstract_cn": "<jats:p>重建三维 (3D) 最常用的方法之一\n\t\t\t\t真实或计算机生成的物体的图像是数字的，并且\n\t\t\t\t分别是计算机生成的全息术。这两种技术都依赖于\n\t\t\t\t使用改变光的相位或振幅的电光装置\n\t\t\t\t以受控方式产生场，即所谓的空间光调制器。\n\t\t\t\t然而，考虑到全息术通常需要相干光源，\n\t\t\t\t三维投影的一个常见问题是串扰\n\t\t\t\t组成 3D 对象的各层之间。这限制了全深度\n\t\t\t\t控制，直接影响图像质量。有趣的是，在过去的几年里\n\t\t\t\t多年来，多种方法已被证明能有效破层\n\t\t\t\t通过消除光的空间相干性来消除串扰。这样的一个缺点是\n\t\t\t\t解决方案是，在许多情况下，需要额外的光学资源\n\t\t\t\t完成这样的任务所需要的。在这项工作中，我们提出了一种方法\n\t\t\t\t使用快速三维物体的高密度重建\n\t\t\t\t通过数字微镜器件 (DMD) 调制光场。\n\t\t\t\t3D 重建是通过将对象离散化为\n\t\t\t\t多平面光点轮廓，其中轮廓的分辨率为\n\t\t\t\t由光点的密度控制。高刷新率\n\t\t\t\tDMD\n\t\t\t\t（〜10kHz）允许\n\t\t\t\t3D 图像的每个点在空间上和\n\t\t\t\t由独立振幅全息图临时控制，因此有效\n\t\t\t\t消除相干引起的多平面串扰，无需\n\t\t\t\t附加光学元件。由于其简单性和多功能性，我们\n\t\t\t\t相信我们的方法提供了一条通往紧凑的实用途径，\n\t\t\t\t高分辨率 3D 全息投影仪。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.587429",
        "title": "Strengthening JOSA A—our new topical editors in action: editorial",
        "link": "https://doi.org/10.1364/josaa.587429",
        "published": "2025-12-23",
        "author": "Olga Korotkova",
        "summary": "<jats:p>Editor-in-Chief Olga Korotkova summarizes the Journal’s progress in 2025, recognizes editors who have completed their terms, and introduces editors who have recently joined the board.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "加强 JOSA A——我们新的主题编辑在行动：社论",
        "abstract_cn": "<jats:p>主编 Olga Korotkova 总结了期刊 2025 年的进展，表彰了已完成任期的编辑，并介绍了最近加入董事会的编辑。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.575935",
        "title": "Non-obstructive measurement technique for inner wall defects in small-diameter pipes",
        "link": "https://doi.org/10.1364/josaa.575935",
        "published": "2026-01-05",
        "author": "Mohan Yan, Xiangxiang Meng, Peng Yang, Rongzhen Lan, Yibiao Liu, Lanbao Zhang",
        "summary": "<jats:p>The high-pressure flexible hoses used in oil and gas drilling have small diameters and long dimensions. When the structured light method is used to detect the defects on their inner walls, there are problems such as the obstruction of the supporting structure and the inability to scan and measure the complete inner wall. For this purpose, a conical structure light unobstructed scanning measurement system based on the arrangement of dual cameras and lasers on the same side has been developed. Based on the pinhole imaging model and the properties of spatial conical surfaces, it is analyzed that in the lateral placement architecture where the camera and the conical structure light exciter are suitable for scanning measurement, the spatial conical light overlapped on the image plane, making it impossible to achieve a one-to-one correspondence between pixel coordinates and world coordinates. By adding a measurement camera in the vertical direction, the overlapping area of the horizontal camera’s imaging is supplemented to achieve blind-spot-free scanning and detection of the inner wall of the pipeline. Based on the system calibration of the conversion between pixel distance and three-dimensional spatial distance, tests were conducted on pipes with inner diameters of 51 and 64 mm containing defects of known sizes. The test results indicated that the root mean square error of the defect size at all positions was ≤0.076mm, and its good robustness within a certain pipe bending range was verified. Meanwhile, the tilt error and the irregular error of the laser strip edge existing in the measurement system were analyzed. The results show that this system has high application value in the measurement of inner wall defects of small-diameter and long-sized pipelines and can meet the actual engineering requirements.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "小口径管道内壁缺陷的无阻碍测量技术",
        "abstract_cn": "<jats:p>石油和天然气钻探中使用的高压软管直径小、尺寸长。采用结构光法检测其内壁缺陷时，存在支撑结构遮挡、无法扫描测量完整内壁等问题。为此，开发了基于双相机和激光器同侧布置的锥形结构光无遮挡扫描测量系统。基于针孔成像模型和空间锥面的性质，分析了在相机和锥体结构光激励器适合扫描测量的横向放置架构中，空间锥体光在像平面上重叠，无法实现像素坐标与世界坐标的一一对应。通过在垂直方向增加测量相机，补充水平相机成像的重叠区域，实现管道内壁的无盲点扫描检测。基于像素距离与三维空间距离转换的系统标定，对内径为51毫米和64毫米且含有已知尺寸缺陷的管道进行了测试。测试结果表明，各位置缺陷尺寸均方根误差≤0.076mm，验证了其在一定弯管范围内良好的鲁棒性。同时对测量系统中存在的激光带材边缘倾斜误差和不规则误差进行了分析。结果表明，该系统在小口径、长尺寸管道内壁缺陷测量中具有较高的应用价值，能够满足实际工程要求。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.568108",
        "title": "TURBO-RL: turbulence mitigation using reinforcement learning for severe optical aberrations",
        "link": "https://doi.org/10.1364/josaa.568108",
        "published": "2026-01-05",
        "author": "Hyunsoo Choi, Jiayu Chen, Vaneet Aggarwal, Zubin Jacob",
        "summary": "<jats:p>\n                    Optical distortion or aberration remains a vital challenge that prohibits high-resolution imaging in various applications such as space domain awareness, terrestrial remote sensing, and astronomy. However, due to the stochastic nature of these optical distortions, reducing their effect without directly measuring wavefronts is challenging. Furthermore, in the case of extreme turbulence, due to the limited size of the lenslet array in the wavefront sensor, the sensor fails to correctly quantify or minimize the image distortions of a guide star from turbulence. While numerous studies have shown effectiveness of guide star-based adaptive optics in mitigating mild turbulence, severe turbulence has remained a persistent challenge. To target this, we present TURBO-RL: TURBulence mitigatiOn using Reinforcement Learning, which uses just a single optical element (e.g., deformable mirror) to estimate and correct the wavefront errors from a guide star. TURBO-RL adopts reinforcement learning with a convolutional neural network to extract and estimate turbulence. Unlike other methods, TURBO-RL is capable of guide star imaging in severe turbulence (\n                    <jats:italic>D</jats:italic>\n                    /\n                    <jats:italic>r</jats:italic>\n                    <jats:sub>0</jats:sub>\n                    =100) with only about 590 photons, making it possible to overcome the strong turbulence and possibly replace bulky and expensive wavefront sensors.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "TURBO-RL：使用强化学习减轻严重光学像差的湍流",
        "abstract_cn": "<贾茨：p>\n                    光学畸变或像差仍然是一个至关重要的挑战，阻碍了空间域感知、地面遥感和天文学等各种应用中的高分辨率成像。然而，由于这些光学畸变的随机性，在不直接测量波前的情况下减少其影响是一项挑战。此外，在极端湍流的情况下，由于波前传感器中小透镜阵列的尺寸有限，传感器无法正确量化或最小化湍流造成的导星图像失真。虽然大量研究表明基于导星的自适应光学器件在减轻轻度湍流方面是有效的，但严重湍流仍然是一个持续存在的挑战。为了实现这一目标，我们提出了 TURBO-RL：使用强化学习的湍流缓解，它仅使用单个光学元件（例如可变形镜）来估计和纠正导星的波前误差。 TURBO-RL 采用卷积神经网络的强化学习来提取和估计湍流。与其他方法不同，TURBO-RL 能够在严重湍流中进行导星成像（\n                    <贾茨：斜体>D</贾茨：斜体>\n                    /\n                    <jats:斜体>r</jats:斜体>\n                    <贾茨：子>0</贾茨：子>\n                    =100），仅具有约 590 个光子，使得克服强湍流成为可能，并可能取代笨重且昂贵的波前传感器。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579383",
        "title": "Effect of the birefringence ratio on the elliptical radial carpet beam in uniaxial crystals",
        "link": "https://doi.org/10.1364/josaa.579383",
        "published": "2026-01-06",
        "author": "Tuo Gao, Jing Cheng",
        "summary": "<jats:p>Propagating a plane wave modulated by an amplitude grating with a specially designed transmittance function in the uniaxial crystal can lead to the generation of the so-called elliptical carpet beams. These elliptical carpet beams retain the characteristic of the traditionally radial carpet beams in free space, such as maintaining stable patterns during propagation. On the other hand, due to the anisotropic diffraction in the uniaxial crystals, the elliptical carpet beams lose some symmetry, resulting in a kind of elliptical transverse carpet pattern during propagation. The ellipticity of the carpet pattern varies with the birefringence ratio for different uniaxial crystals. In this study, we have investigated in detail the behavior of the elliptical carpet beams by varying crystal refractive indices and derived the analytical expression of elliptical carpet beams. Particularly, we show that there exists a kind of characteristic ellipse of the elliptical carpet beams and point out the properties in relation to the birefringence ratio of the crystals.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "双折射率对单轴晶体椭圆径向地毯光束的影响",
        "abstract_cn": "<jats:p>在单轴晶体中传播由具有专门设计的透射率函数的幅度光栅调制的平面波可以产生所谓的椭圆地毯光束。这些椭圆地毯梁保留了自由空间中传统径向地毯梁的特性，例如在传播过程中保持稳定的图案。另一方面，由于单轴晶体中的各向异性衍射，椭圆地毯光束失去了一定的对称性，在传播过程中产生了一种椭圆横向地毯图案。地毯图案的椭圆率随着不同单轴晶体的双折射率而变化。在这项研究中，我们通过改变晶体折射率详细研究了椭圆地毯梁的行为，并推导了椭圆地毯梁的解析表达式。特别是，我们证明了椭圆地毯梁存在一种特征椭圆，并指出了与晶体双折射率相关的性质。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.583420",
        "title": "Entopic starburst in vision: revisiting the role of diffractive lens suture patterns",
        "link": "https://doi.org/10.1364/josaa.583420",
        "published": "2026-01-06",
        "author": "Francisco J. Ávila, Justin C. D’Antin, Rafael I. Barraquer, Rafael Navarro",
        "summary": "<jats:p>\n                    Starbursts are entoptic visual phenomena perceived as radial spikes around point light sources, frequently reported under scotopic conditions or following ocular surgeries such as LASIK or intraocular lens implantation. While starbursts have traditionally been attributed to high-order optical aberrations, in particular spherical aberration, we hypothesize that crystalline lens suture patterns may play a dominant role in the formation of starbursts through light diffraction. This study investigates the contribution of lens suture diffraction to starburst formation and compares it to the influence of high-order spherical aberrations. Using\n                    <jats:italic toggle=\"yes\">ex vivo</jats:italic>\n                    porcine crystalline lenses mounted in a mechanical expansion unit, we performed through-focus imaging and aberrometric analysis with a custom optical system. Suture patterns were segmented and used to simulate diffraction-based point-spread functions (PSFs) by convolving them with a Gaussian point source. Simulated PSFs were compared to those generated using fourth- and sixth-order spherical aberration parameters. A cross-correlation analysis and angular detection of diffraction spikes were performed to assess the similarity between experimental and simulated PSFs. Results demonstrate that diffraction by crystalline lens sutures generates starburst-like PSFs, independent of high-order aberrations. Through-focus simulations revealed asymmetric Strehl ratio distributions when suture diffraction was included, suggesting that sutures significantly degrade optical quality over a broader dioptric range. Cross-correlation coefficients (\n                    <jats:italic>τ</jats:italic>\n                    ) between experimental and simulated PSFs exceeded 0.7 in all cases, and diffraction spike orientations showed high angular consistency, confirming the predictive value of the suture diffraction model. Our findings support the hypothesis that lens sutures are the primary source of starburst formation via diffraction, with spherical aberration acting as a secondary factor.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "视觉中的内位星爆：重新审视衍射透镜缝合图案的作用",
        "abstract_cn": "<贾茨：p>\n                    星爆是一种内视视觉现象，被视为点光源周围的径向尖峰，经常在暗视条件下或在 LASIK 或人工晶状体植入等眼科手术后报告。虽然星暴传统上被归因于高阶光学像差，特别是球面像差，但我们假设晶状体缝合图案可能在通过光衍射形成星暴的过程中发挥主导作用。这项研究研究了透镜缝线衍射对星暴形成的贡献，并将其与高阶球面像差的影响进行了比较。使用\n                    <jats:italictoggle=\"yes\">离体</jats:italic>\n                    将猪晶状体安装在机械膨胀装置中，我们使用定制光学系统进行离焦成像和像差分析。缝合线图案被分段并通过将它们与高斯点源进行卷积来模拟基于衍射的点扩散函数（PSF）。将模拟的 PSF 与使用四阶和六阶球面像差参数生成的 PSF 进行比较。进行互相关分析和衍射尖峰的角度检测，以评估实验和模拟 PSF 之间的相似性。结果表明，晶状体缝线的衍射会产生星爆状的 PSF，与高阶像差无关。当包括缝线衍射时，离焦模拟揭示了不对称的斯特列尔比分布，这表明缝线在更宽的屈光范围内显着降低了光学质量。互相关系数（\n                    <jats:斜体>τ</jats:斜体>\n                    在所有情况下，实验和模拟 PSF 之间的 ) 均超过 0.7，并且衍射尖峰方向显示出较高的角度一致性，证实了缝线衍射模型的预测价值。我们的研究结果支持这样的假设：透镜缝线是通过衍射形成星爆的主要来源，球面像差是次要因素。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.580440",
        "title": "Framework for noise-type and noise-level estimation under additive and multiplicative models in color images",
        "link": "https://doi.org/10.1364/josaa.580440",
        "published": "2026-01-16",
        "author": "Zipeng Fu, Xiaoling Ge, Xuelian Yu, Weixian Qian",
        "summary": "<jats:p>Accurate estimation of the noise type and noise level in color images is crucial for tasks such as denoising, segmentation, and super-resolution. However, existing approaches often rely on the assumption that the noise type is known, and they tend to suffer from significant deviations when dealing with complex textures or strong inter-channel correlations in color images. To address these limitations, this paper proposes a quaternion-based framework for estimating noise type and noise level under two representative and widely used noise families: additive noise and multiplicative–additive noise (non-additive noise). By leveraging quaternion matrix modeling, the proposed method effectively captures cross-channel correlations, thereby enhancing the accuracy of both type discrimination between these two noise categories and noise-level estimation. On this basis, a classification model is developed by combining statistical features with logistic regression. Furthermore, differentiated noise-level estimation strategies based on weak-texture extraction are designed for the identified additive or non-additive noise models. Extensive experimental results demonstrate that the proposed method can accurately identify noise types and significantly improve the precision of noise-level estimation across diverse color image datasets and complex noise conditions, outperforming state-of-the-art techniques.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "彩色图像中加法和乘法模型下的噪声类型和噪声水平估计框架",
        "abstract_cn": "<jats:p>准确估计彩色图像中的噪声类型和噪声水平对于去噪、分割和超分辨率等任务至关重要。然而，现有的方法通常依赖于噪声类型已知的假设，并且在处理彩色图像中的复杂纹理或强通道间相关性时，它们往往会遭受显着偏差。为了解决这些限制，本文提出了一种基于四元数的框架，用于估计两个代表性且广泛使用的噪声族下的噪声类型和噪声水平：加性噪声和乘性-加性噪声（非加性噪声）。通过利用四元数矩阵建模，所提出的方法有效地捕获跨通道相关性，从而提高这两个噪声类别之间的类型区分和噪声水平估计的准确性。在此基础上，结合统计特征和逻辑回归建立了分类模型。此外，针对识别的加性或非加性噪声模型，设计了基于弱纹理提取的差异化噪声水平估计策略。大量实验结果表明，该方法能够准确识别噪声类型，并显着提高不同彩色图像数据集和复杂噪声条件下噪声水平估计的精度，优于现有技术。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.580665",
        "title": "Enhanced optical transmission tuning in a subwavelength metallic slit with internal ridges",
        "link": "https://doi.org/10.1364/josaa.580665",
        "published": "2026-01-13",
        "author": "M. A. Ortiz Ferreyro, J. Sumaya Martínez",
        "summary": "<jats:p>We present a rigorous frequency-domain finite-element formulation for modeling the diffraction of transverse magnetic polarized electromagnetic waves through metallic subwavelength slits. The approach incorporates realistic boundary conditions, open-domain truncation via perfectly matched layers, and material dispersion. Numerical simulations reproduce Fabry–Perot- like resonances for a reference slit and demonstrate that embedding symmetric internal ridges produces measurable spectral shifts, redistributes near-field hot spots, and modulates transmitted power by factors exceeding two compared to the plain slit. The proposed model provides compact, quantitative design rules for tuning resonance wavelengths and quality factors, enabling the engineering of subwavelength photonic components for filtering, sensing, and light manipulation.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "具有内部脊的亚波长金属狭缝中增强的光传输调谐",
        "abstract_cn": "<jats:p>我们提出了严格的频域有限元公式，用于模拟横向磁极化电磁波通过金属亚波长狭缝的衍射。该方法结合了现实的边界条件、通过完美匹配的层进行的开域截断以及材料分散。数值模拟重现了参考狭缝的法布里-珀罗共振，并证明嵌入对称内部脊会产生可测量的光谱偏移，重新分布近场热点，并以超过平面狭缝两倍的系数调制传输功率。所提出的模型为调谐谐振波长和品质因数提供了紧凑的定量设计规则，从而实现了用于滤波、传感和光操纵的亚波长光子元件的工程。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574632",
        "title": "Color differences in complex scenes: the role of background",
        "link": "https://doi.org/10.1364/josaa.574632",
        "published": "2026-01-09",
        "author": "Jiaying Wu, Renzo Shamey",
        "summary": "<jats:p>\n                    Color appearance is strongly shaped by surrounding context, yet color-quality assessments for multicolored materials, such as camouflage textiles, typically rely on judgments made against uniform backgrounds. This study systematically tests whether a homogeneous background matched to the average color of a multicolored surround can approximate the perceptual influence of the original patterned background when assessing small color differences in MARPAT camouflage. Using a center–background paradigm, pairs of stimuli with either identical centers or small lightness differences (ΔL\n                    <jats:sup>∗</jats:sup>\n                    =±0.5, ±1.0) were presented on three background types: a MARPAT two-color checkerboard, its corresponding CIELAB-matched average background, and a neutral gray reference. Psychophysical experiments with 20 color-normal observers revealed systematic perceptual discrepancies between patterned and color-averaged backgrounds: even physically identical centers produced mean visual differences of ∼0.2−1.2ΔE\n                    <jats:sub>00</jats:sub>\n                    (1:1:1). Background composition exerted a strong effect (p&lt;0.001), whereas checkerboard size (4×4, 8×8, 16×16) had no significant influence. The largest induced differences occurred for combinations in which the center–surround lightness ratio approached unity, consistent with enhanced chromatic induction. Sensitivity to small ΔL\n                    <jats:sup>∗</jats:sup>\n                    differences declined sharply when stimuli were spatially separated, with many observers failing to detect even ±1.0ΔL\n                    <jats:sup>∗</jats:sup>\n                    differences on gray. Substantial intra- and inter-observer variability further highlighted the difficulty of judging subtle differences in complex scenes. Baseline measurements on uniform gray backgrounds yielded near-zero visual differences, confirming that the large apparent differences observed under patterned surrounds were induction-driven rather than due to random noise. Overall, the results demonstrate that a color-averaged background does not serve as a perceptually equivalent substitute for a complex multicolored surround, underscoring the need for context-appropriate backgrounds in the visual evaluation of camouflage, textiles, displays, and other patterned materials.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "复杂场景中的色差：背景的作用",
        "abstract_cn": "<贾茨：p>\n                    颜色外观很大程度上受到周围环境的影响，但迷彩纺织品等彩色材料的颜色质量评估通常依赖于对统一背景的判断。这项研究系统地测试了在评估 MARPAT 迷彩中的微小颜色差异时，与彩色周围的平均颜色相匹配的均匀背景是否可以近似原始图案背景的感知影响。使用中心-背景范式，具有相同中心或较小亮度差异（ΔL\n                    <jats:sup>*</jats:sup>\n                    =±0.5，±1.0）在三种背景类型上呈现：MARPAT 双色棋盘、其相应的 CIELAB 匹配平均背景和中性灰色参考。对 20 名颜色正常的观察者进行的心理物理学实验揭示了图案背景和颜色平均背景之间的系统感知差异：即使物理上相同的中心也会产生~0.2−1.2ΔE 的平均视觉差异\n                    <jats:sub>00</jats:sub>\n                    （1：1：1）。背景成分发挥了很强的影响(p＜0.001)，而棋盘尺寸(4×4、8×8、16×16)没有显着影响。最大的诱导差异发生在中心-环绕亮度比接近统一的组合中，这与增强的色彩诱导一致。对小 ΔL 的敏感性\n                    <jats:sup>*</jats:sup>\n                    当刺激在空间上分离时，差异急剧下降，许多观察者甚至无法检测到±1.0ΔL\n                    <jats:sup>*</jats:sup>\n                    灰度上的差异。观察者内部和观察者之间的巨大差异进一步凸显了判断复杂场景中细微差异的难度。对均匀灰色背景的基线测量产生了接近于零的视觉差异，证实了在图案周围观察到的巨大的明显差异是感应驱动的，而不是由于随机噪声造成的。总体而言，结果表明，颜色平均背景并不能在感知上等效地替代复杂的多色环绕，这强调了在迷彩、纺织品、显示器和其他图案材料的视觉评估中需要适合上下文的背景。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.583298",
        "title": "The geometric phase of rotations and 3D coordinate transformations",
        "link": "https://doi.org/10.1364/josaa.583298",
        "published": "2025-12-22",
        "author": "Luis Garza, Nathan Hagen",
        "summary": "The wave description of geometric phase uses the superposition of light waves to explain the geometric phase's origin. While our previous work focused on a basis of linearly polarized waves, here we show that the same concepts can be applied to circularly polarized waves, and to any case in which a rotator is itself subjected to rotation. As with a linear polarization basis, we show that the addition of two vectors (rotators) with different orientations and magnitudes causes the orientation of the resulting vector to shift towards the component vector of greater magnitude, i.e. it introduces a geometric phase. We illustrate this approach with two classic examples of the geometric phase of rotations in space: a system of three fold mirrors, and the helical coiled fiber. In both cases we show that it is possible to derive the phase shift directly from the electromagnetic wave vector without needing to resort to mathematical abstractions such as differential geometry, or calculating solid angles in the space of directions.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "旋转的几何相位和 3D 坐标变换",
        "abstract_cn": "几何相位的波动描述是利用光波的叠加来解释几何相位的起源。虽然我们之前的工作集中在线性偏振波的基础上，但在这里我们表明相同的概念可以应用于圆偏振波，以及旋转器本身受到旋转的任何情况。与线性偏振基础一样，我们表明，具有不同方向和幅度的两个矢量（旋转器）的相加会导致所得矢量的方向向幅度更大的分量矢量移动，即它引入了几何相位。我们用空间旋转几何相位的两个经典例子来说明这种方法：三折镜系统和螺旋卷曲光纤。在这两种情况下，我们都表明可以直接从电磁波矢量导出相移，而无需求助于微分几何等数学抽象，或计算方向空间中的立体角。"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579923",
        "title": "Weld seam recognition algorithm based on a fast point cloud plane fitting method",
        "link": "https://doi.org/10.1364/josaa.579923",
        "published": "2026-01-13",
        "author": "Xingyu Gao, Xi Xiong, Weiming Li, Shuibiao Chen, Yu Li",
        "summary": "<jats:p>Aiming at the problems of low accuracy and slow speed of existing point cloud weld extraction algorithms in 3D vision-based robotic intelligent welding, this study proposes a novel, to our knowledge, three-stage automatic point cloud weld extraction method. In the plane segmentation stage, the random sample consensus (RANSAC) algorithm is improved: by narrowing the selection range of sampling points to the local neighborhood and optimizing neighborhood construction with dynamic curvature detection, the efficiency of plane fitting is enhanced. In the feature point extraction stage, based on plane parameters, the plane intersection line method and distance threshold method are adopted to obtain weld seam feature points. In the curve fitting stage, farthest point sampling (FPS) is used to denoise and resample the feature points, and then the weld curve is fitted to achieve high-precision contour reconstruction. Experiments show that the method exhibits high efficiency, robustness, and engineering adaptability.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于快速点云平面拟合方法的焊缝识别算法",
        "abstract_cn": "<jats:p>针对现有基于3D视觉的机器人智能焊接点云焊缝提取算法精度低、速度慢的问题，提出一种据我们所知的三阶段自动点云焊缝提取方法。在平面分割阶段，改进了随机样本一致性（RANSAC）算法：通过将采样点的选择范围缩小到局部邻域，并通过动态曲率检测优化邻域构建，提高了平面拟合的效率。在特征点提取阶段，根据平面参数，采用平面交线法和距离阈值法来获取焊缝特征点。在曲线拟合阶段，采用最远点采样（FPS）对特征点进行去噪和重采样，然后拟合焊缝曲线，实现高精度轮廓重建。实验表明，该方法具有高效性、鲁棒性和工程适应性。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581983",
        "title": "Adaptively balanced Poisson-constrained physics-informed neural networks for robust displacement integration in background-oriented Schlieren",
        "link": "https://doi.org/10.1364/josaa.581983",
        "published": "2026-01-14",
        "author": "Hao Liu, Hongzhe Wang, Yang Song, Yunjing Ji, Jiancheng Lai, Zhenhua Li",
        "summary": "<jats:p>Displacement integration in background-oriented Schlieren (BOS) is a critical step in the reconstruction of physical fields. This process typically employs high-order fitting integration or discrete Poisson solvers. This paper examines the strengths and limitations of these conventional approaches and proposes a novel physics-informed neural network (PINN) framework constrained by the Poisson equation, termed the adaptively balanced Poisson-constrained PINN (AB-PoissonPINN). The proposed method incorporates relative loss balancing with random backtracking (ReLoBRaLo) to dynamically balance the contributions of different loss components. The integration performance of AB-PoissonPINN is evaluated through both simulated and experimental numerical integration and is benchmarked against established techniques, including weighted cubic spline least squares integration (WCSLI), discrete Poisson solvers, and standard PINN. Experimental results demonstrate that AB-PoissonPINN consistently achieves higher accuracy than WCSLI, discrete Poisson solvers, and standard PINN under both noise-free conditions and various noise levels.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "自适应平衡泊松约束物理信息神经网络，用于背景导向纹影中的鲁棒位移积分",
        "abstract_cn": "<jats:p>背景导向纹影 (BOS) 中的位移积分是物理场重建的关键步骤。该过程通常采用高阶拟合积分或离散泊松求解器。本文研究了这些传统方法的优点和局限性，并提出了一种受泊松方程约束的新型物理信息神经网络 (PINN) 框架，称为自适应平衡泊松约束 PINN (AB-PoissonPINN)。所提出的方法将相对损失平衡与随机回溯（ReLoBRaLo）相结合，以动态平衡不同损失分量的贡献。 AB-PoissonPINN 的积分性能通过模拟和实验数值积分进行评估，并以现有技术为基准，包括加权三次样条最小二乘积分 (WCSLI)、离散泊松求解器和标准 PINN。实验结果表明，在无噪声条件和各种噪声水平下，AB-PoissonPINN 始终能够比 WCSLI、离散泊松求解器和标准 PINN 实现更高的精度。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.584451",
        "title": "Diagnosing quantum channel decoherence via vector coherent mode decomposition",
        "link": "https://doi.org/10.1364/josaa.584451",
        "published": "2026-01-14",
        "author": "Kenneth A. Menard",
        "summary": "<jats:p>High-dimensional quantum key distribution (HD-QKD) is limited by atmospheric turbulence, which scrambles the spatial modes used to encode information. A critical challenge is the lack of general methods to diagnose how this decoherence occurs. We apply the vector coherent mode decomposition (VCMD)—a numerical framework that represents a partially coherent beam as an incoherent superposition of its natural, orthogonal eigenmodes—as a powerful diagnostic engine for this problem. We simulate a qudit state propagating through a turbulent channel and use VCMD to quantify the decay of channel fidelity and reveal the exact spatial structure of the dominant error modes. This provides a complete modal fingerprint of the channel’s decoherence mechanism. To establish the framework’s credibility, we first validate its accuracy against a benchmark suite, including unmasking a deceptive “Masked Gaussian” beam and quantifying the purity of a decohered optical skyrmion. While the theory of vector coherence is established, this work consolidates it into a practical, basis-independent framework and applies it to discover the physical error modes of a turbulent quantum channel, a task for which conventional, basis-dependent methods are ill-suited.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "通过矢量相干模式分解诊断量子通道退相干",
        "abstract_cn": "<jats:p>高维量子密钥分发 (HD-QKD) 受到大气湍流的限制，大气湍流会扰乱用于编码信息的空间模式。一个关键的挑战是缺乏诊断这种退相干如何发生的通用方法。我们应用矢量相干模式分解 (VCMD)（一种数值框架，将部分相干光束表示为其自然正交本征模式的非相干叠加）作为解决此问题的强大诊断引擎。我们模拟通过湍流通道传播的量子状态，并使用 VCMD 来量化通道保真度的衰减并揭示主要误差模式的确切空间结构。这提供了通道退相干机制的完整模态指纹。为了建立该框架的可信度，我们首先根据基准套件验证其准确性，包括揭开欺骗性的“掩蔽高斯”光束并量化退相干光学斯格明子的纯度。虽然矢量相干理论已经建立，但这项工作将其整合为一个实用的、与基无关的框架，并将其应用于发现湍流量子通道的物理误差模式，这是传统的、与基相关的方法不适合的任务。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581822",
        "title": "Physics-informed deep learning framework for wavefront sensing via optical beam pattern analysis",
        "link": "https://doi.org/10.1364/josaa.581822",
        "published": "2026-01-14",
        "author": "Tengfei Chai, Xiaoyun Liu, Hongwei Wang, Yumeihui Jin, Jianyu Huang, Tianyu Shi, Yueqiu Jiang",
        "summary": "<jats:p>We propose PIRNet, a lightweight, physics-informed deep learning framework for non-interferometric wavefront sensing in optical beam expansion systems. The network simultaneously estimates spherical aberration, coma, and astigmatism from single-shot beam intensity patterns. A large-scale dataset is generated by simulating vortex beam propagation through combined aberrations using the ABCD transfer matrix method. To ensure physical plausibility, we introduce a physics-consistency loss that reconstructs the beam pattern from the predicted coefficients via an optical propagation model and compares it with the input, dynamically balanced with the regression loss through a learnable uncertainty weighting mechanism. A staged training strategy stabilizes convergence by first focusing on regression before introducing the physical constraint. Comparative experiments demonstrate that PIRNet outperforms ResNet18, ResNet34, ResNet50, and Xception across multiple metrics under varying noise levels and cropping ratios. The integration of physical priors enhances both accuracy and generalization, positioning PIRNet as a promising approach for model-driven wavefront characterization in adaptive optics and free-space optical communication.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于物理的深度学习框架，通过光束图案分析进行波前传感",
        "abstract_cn": "<jats:p>我们提出了 PIRNet，这是一种轻量级、基于物理的深度学习框架，用于光束扩展系统中的非干涉波前传感。该网络同时根据单次光束强度图案估计球面像差、慧差和像散。使用 ABCD 传递矩阵方法通过组合像差模拟涡旋光束传播，生成大规模数据集。为了确保物理合理性，我们引入了物理一致性损失，它通过光学传播模型根据预测系数重建光束图案，并将其与输入进行比较，通过可学习的不确定性加权机制与回归损失动态平衡。分阶段训练策略通过在引入物理约束之前首先关注回归来稳定收敛。比较实验表明，在不同的噪声水平和裁剪比率下，PIRNet 在多个指标上均优于 ResNet18、ResNet34、ResNet50 和 Xception。物理先验的集成提高了准确性和泛化性，使 PIRNet 成为自适应光学和自由空间光通信中模型驱动波前表征的一种有前景的方法。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.579780",
        "title": "Reflection in a concave spherical mirror—two contrasting conceptions of optics: retrospective",
        "link": "https://doi.org/10.1364/josaa.579780",
        "published": "2026-01-13",
        "author": "Yaakov Zik, Giora Hon",
        "summary": "<jats:p>\n                    We contrast two approaches to the optics of reflection in concave spherical mirrors. In the\n                    <jats:italic toggle=\"yes\">Optics</jats:italic>\n                    (ca. 165), Ptolemy employed the cathetus principle as a regulative rule to account qualitatively for visual appearance in concave spherical mirrors. In his\n                    <jats:italic toggle=\"yes\">Magia naturalis</jats:italic>\n                    (1589) and\n                    <jats:italic toggle=\"yes\">De refractione</jats:italic>\n                    (1593), Della Porta’s analyses rested on the assumption of a reciprocal relation between reflection in concave mirrors and refraction in glass spheres. These two historical cases highlight two fundamentally different conceptions of optics: Ptolemy explained optical phenomena based on vision, whereas Della Porta explained the same phenomena by appealing to the geometrical action of light. The shift from perception to the geometry of light rays marks an important development in the history of optics. The elimination of vision from the equation, as it were, ushered in modern optics.\n                  </jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "凹球面镜中的反射——两种截然不同的光学概念：回顾",
        "abstract_cn": "<贾茨：p>\n                    我们对比了凹球面镜反射光学的两种方法。在\n                    <jats:italictoggle=\"yes\">光学</jats:italic>\n                    （约 165），托勒密采用了内镜原理作为规范规则来定性地解释凹球面镜中的视觉外观。在他的\n                    <jats:italictoggle=\"yes\">自然魔法</jats:italic>\n                    【1589】\n                    <jats:italictoggle=\"yes\">去折射</jats:italic>\n                    (1593)，Della Porta 的分析基于凹面镜中的反射与玻璃球中的折射之间存在相互关系的假设。这两个历史案例凸显了两种根本不同的光学概念：托勒密基于视觉解释了光学现象，而德拉波塔则通过诉诸光的几何作用来解释相同的现象。从感知到光线几何的转变标志着光学史上的重要发展。可以说，从方程中消除视觉开创了现代光学。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.573810",
        "title": "Analytical transfer characteristics of axially symmetric 3D optical imaging systems",
        "link": "https://doi.org/10.1364/josaa.573810",
        "published": "2026-01-16",
        "author": "Nikolay Nikolaev, Jeremy Coupland",
        "summary": "<jats:p>This paper presents a unified analytical framework to derive the three-dimensional transfer characteristics of axially symmetric optical imaging systems operating in the far field, with emphasis on coherence scanning interferometry, confocal microscopy, and focus variation techniques. While many 3D optical instruments are treated as linear systems, in practice inconsistencies remain in how their transfer functions are derived and interpreted, particularly across forward- and back-scatter geometries. Addressing this, we develop closed-form expressions for the 3D transfer function and point spread function for generic back- and forward-scatter systems under commonly applied apodization conditions (e.g., uniform, root-cosine, and cosine). These derivations clarify the spatial frequency support and resolution trade-offs intrinsic to each geometry and validate the characteristic “bowtie” and “umbrella” structures observed experimentally in the spatial frequency domain. Our analytical results not only resolve the ambiguities of previous numerical models but provide a means to validate and understand the applicability of approximate 3D and 2D numerical models. The formalism is robust, generalizable, and appropriate to the modeling and correction of real instruments in 3D surface metrology and optical tomography.</jats:p>",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "轴对称 3D 光学成像系统的分析传输特性",
        "abstract_cn": "<jats:p>本文提出了一个统一的分析框架，用于导出在远场运行的轴对称光学成像系统的三维传输特性，重点是相干扫描干涉测量、共焦显微镜和焦点变化技术。虽然许多 3D 光学仪器被视为线性系统，但实际上，其传递函数的导出和解释方式仍然存在不一致，特别是在前向和后向散射几何形状中。为了解决这个问题，我们在常用的变迹条件（例如均匀、根余弦和余弦）下为通用反向和前向散射系统开发了 3D 传递函数和点扩散函数的闭合形式表达式。这些推导阐明了每种几何结构固有的空间频率支持和分辨率权衡，并验证了在空间频域中通过实验观察到的特征“领结”和“伞”结构。我们的分析结果不仅解决了以前数值模型的模糊性，而且提供了一种验证和理解近似 3D 和 2D 数值模型的适用性的方法。该形式具有鲁棒性、可推广性，适合 3D 表面计量和光学断层扫描中真实仪器的建模和校正。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1364/josaa.574250",
        "title": "Spectral element method for optical planar waveguide modal analysis",
        "link": "https://doi.org/10.1364/josaa.574250",
        "published": "2026-01-05",
        "author": "Gerard Granet, Malalatiana Rinah Rasoamilanto, Karyl Raniriharinosy, Kofi EDEE, Manjakavola Randriamihaja",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "光学平面波导模态分析的谱元法",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.587566",
        "title": "Hybrid broken-ray tomography",
        "link": "https://doi.org/10.1364/josaa.587566",
        "published": "2026-01-08",
        "author": "Matthew Faulkner, John Schotland, Vadim Markel, lucia Florescu",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "混合破碎射线断层扫描",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.577058",
        "title": "Median Cut Color Quantization Algorithm: Retrospective",
        "link": "https://doi.org/10.1364/josaa.577058",
        "published": "2026-01-12",
        "author": "Emre Celebi, Maria-Luisa Perez-Delgado",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "中值剪切颜色量化算法：回顾",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.580584",
        "title": "An Error Correction Method Based on Projection Transformation for Rail Wear Measurement Using Line-Structured Light",
        "link": "https://doi.org/10.1364/josaa.580584",
        "published": "2026-01-12",
        "author": "Bo Han, Yue Jin, Wenming Yang, Beiying Liu",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "基于投影变换的线结构光钢轨磨损测量误差修正方法",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.582961",
        "title": "The forward and inverse problems for symmetric starbursts",
        "link": "https://doi.org/10.1364/josaa.582961",
        "published": "2026-01-13",
        "author": "Jacob Rubinstein",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "对称星暴的正向和逆向问题",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.585628",
        "title": "Evolution of Energy Flow in the Focal Field of Azimuthally Polarized Lorentz-Gauss Beams under Helico-Conical Phase Modulation",
        "link": "https://doi.org/10.1364/josaa.585628",
        "published": "2026-01-14",
        "author": "shuo yu, jinsong li, Chundi Zheng, shuo jia, hao wang",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "螺旋锥相位调制下方位偏振洛伦兹高斯光束焦场能量流的演化",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.584693",
        "title": "Characterization of birefringent media via intrinsic and equivalent parameters",
        "link": "https://doi.org/10.1364/josaa.584693",
        "published": "2026-01-14",
        "author": "Jhon Pabón Niño, Camilo Cadena, Rafael Torres",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "通过固有参数和等效参数表征双折射介质",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.585179",
        "title": "Lightweight Neural Network for Wavefront Estimation under Long-Distance, Strong Atmospheric Turbulence",
        "link": "https://doi.org/10.1364/josaa.585179",
        "published": "2026-01-16",
        "author": "Yonghao Chen, Wuli Hu, Zheqiang Zhong, Bin Zhang",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "用于长距离、强大气湍流下波前估计的轻量级神经网络",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.581369",
        "title": "Bridging Optics and Machine Learning: Revisiting Correspondence Imaging via Linear Classification",
        "link": "https://doi.org/10.1364/josaa.581369",
        "published": "2026-01-20",
        "author": "Yu Zhou, Jianbin Liu, Huaibin Zheng, Hui Chen, Yuchen He, fuli li, Zhuo Xu",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "连接光学和机器学习：通过线性分类重新审视对应成像",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.584761",
        "title": "Generation of Ultra-Long pure longitudinal magnetization fields using Complex Phase Filters",
        "link": "https://doi.org/10.1364/josaa.584761",
        "published": "2026-01-20",
        "author": "shuo jia, jinsong li, Chundi Zheng, shuo yu, hao wang",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "使用复相滤波器生成超长纯纵向磁化场",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.583069",
        "title": "Vector diffraction of aspherical wavefronts with azimuthal polarization in microscope objectives",
        "link": "https://doi.org/10.1364/josaa.583069",
        "published": "2026-01-23",
        "author": "Rafael Gonzalez Acuña",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "显微镜物镜中具有方位角偏振的非球面波前的矢量衍射",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "http://arxiv.org/abs/2601.16287v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16287v1",
        "title": "Active learning for photonics",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Active learning for photonics"
        },
        "updated": "2026-01-22T19:42:23Z",
        "updated_parsed": [
            2026,
            1,
            22,
            19,
            42,
            23,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16287v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16287v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Active learning for photonic crystals explores the integration of analytic approximate Bayesian last layer neural networks (LL-BNNs) with uncertainty-driven sample selection to accelerate photonic band gap prediction. We employ an analytic LL-BNN formulation, corresponding to the infinite Monte Carlo sample limit, to obtain uncertainty estimates that are strongly correlated with the true predictive error on unlabeled candidate structures. These uncertainty scores drive an active learning strategy that prioritizes the most informative simulations during training. Applied to the task of predicting band gap sizes in two-dimensional, two-tone photonic crystals, our approach achieves up to a 2.6x reduction in required training data compared to a random sampling baseline while maintaining predictive accuracy. The efficiency gains arise from concentrating computational resources on high uncertainty regions of the design space rather than sampling uniformly. Given the substantial cost of full band structure simulations, especially in three dimensions, this data efficiency enables rapid and scalable surrogate modeling. Our results suggest that analytic LL-BNN based active learning can substantially accelerate topological optimization and inverse design workflows for photonic crystals, and more broadly, offers a general framework for data efficient regression across scientific machine learning domains.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Active learning for photonic crystals explores the integration of analytic approximate Bayesian last layer neural networks (LL-BNNs) with uncertainty-driven sample selection to accelerate photonic band gap prediction. We employ an analytic LL-BNN formulation, corresponding to the infinite Monte Carlo sample limit, to obtain uncertainty estimates that are strongly correlated with the true predictive error on unlabeled candidate structures. These uncertainty scores drive an active learning strategy that prioritizes the most informative simulations during training. Applied to the task of predicting band gap sizes in two-dimensional, two-tone photonic crystals, our approach achieves up to a 2.6x reduction in required training data compared to a random sampling baseline while maintaining predictive accuracy. The efficiency gains arise from concentrating computational resources on high uncertainty regions of the design space rather than sampling uniformly. Given the substantial cost of full band structure simulations, especially in three dimensions, this data efficiency enables rapid and scalable surrogate modeling. Our results suggest that analytic LL-BNN based active learning can substantially accelerate topological optimization and inverse design workflows for photonic crystals, and more broadly, offers a general framework for data efficient regression across scientific machine learning domains."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T19:42:23Z",
        "published_parsed": [
            2026,
            1,
            22,
            19,
            42,
            23,
            3,
            22,
            0
        ],
        "arxiv_comment": "6 pages, 5 figures, submitted to Optics Express",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Ryan Lopez"
            },
            {
                "name": "Charlotte Loh"
            },
            {
                "name": "Rumen Dangovski"
            },
            {
                "name": "Marin Soljačić"
            }
        ],
        "author_detail": {
            "name": "Marin Soljačić"
        },
        "author": "Marin Soljačić",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "光子学的主动学习",
        "abstract_cn": "光子晶体的主动学习探索了分析近似贝叶斯最后一层神经网络 (LL-BNN) 与不确定性驱动的样本选择的集成，以加速光子带隙预测。我们采用与无限蒙特卡罗样本限制相对应的解析 LL-BNN 公式来获得与未标记候选结构的真实预测误差密切相关的不确定性估计。这些不确定性分数推动了主动学习策略，该策略在训练期间优先考虑信息最丰富的模拟。应用于预测二维、双色调光子晶体中的带隙大小的任务时，与随机采样基线相比，我们的方法可将所需的训练数据减少多达 2.6 倍，同时保持预测准确性。效率增益来自于将计算资源集中在设计空间的高不确定性区域，而不是均匀采样。考虑到全能带结构模拟的巨大成本，尤其是在三个维度上，这种数据效率可以实现快速且可扩展的替代建模。我们的结果表明，基于分析 LL-BNN 的主动学习可以大大加速光子晶体的拓扑优化和逆向设计工作流程，更广泛地说，为跨科学机器学习领域的数据高效回归提供了通用框架。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16322v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16322v1",
        "title": "Controlled Switching of Bose-Einstein Condensation in a Mixture of Two Species of Polaritons",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Controlled Switching of Bose-Einstein Condensation in a Mixture of Two Species of Polaritons"
        },
        "updated": "2026-01-22T21:08:19Z",
        "updated_parsed": [
            2026,
            1,
            22,
            21,
            8,
            19,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16322v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16322v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We report temperature-dependent switching between lower and upper polariton condensation in a GaAs/AlGaAs microcavity when both of these species have comparable populations in a mixture. Using angle-resolved photoluminescence, we observe that at low temperatures, condensation occurs in the lower polariton branch, while at elevated temperatures, the upper polariton branch can become favored. At an intermediate temperature, we observe instability in the condensate formation, characterized by metastable correlations of the fluctuations in intensity and linewidth of the lower and upper polariton branches.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We report temperature-dependent switching between lower and upper polariton condensation in a GaAs/AlGaAs microcavity when both of these species have comparable populations in a mixture. Using angle-resolved photoluminescence, we observe that at low temperatures, condensation occurs in the lower polariton branch, while at elevated temperatures, the upper polariton branch can become favored. At an intermediate temperature, we observe instability in the condensate formation, characterized by metastable correlations of the fluctuations in intensity and linewidth of the lower and upper polariton branches."
        },
        "tags": [
            {
                "term": "cond-mat.quant-gas",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T21:08:19Z",
        "published_parsed": [
            2026,
            1,
            22,
            21,
            8,
            19,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cond-mat.quant-gas"
        },
        "authors": [
            {
                "name": "Hassan Alnatah"
            },
            {
                "name": "Shuang Liang"
            },
            {
                "name": "Qiaochu Wan"
            },
            {
                "name": "Jonathan Beaumariage"
            },
            {
                "name": "Ken West"
            },
            {
                "name": "Kirk Baldwin"
            },
            {
                "name": "Loren N. Pfeiffer"
            },
            {
                "name": "David W. Snoke"
            }
        ],
        "author_detail": {
            "name": "David W. Snoke"
        },
        "author": "David W. Snoke",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "两种极化子混合物中玻色-爱因斯坦凝聚的受控切换",
        "abstract_cn": "我们报告了当这两种物质在混合物中具有相当的群体时，GaAs/AlGaAs 微腔中的下极化子凝结和上极化子凝结之间的温度依赖性切换。使用角度分辨光致发光，我们观察到在低温下，在下部极化子分支中发生凝结，而在高温下，上部极化子分支会变得有利。在中间温度下，我们观察到冷凝物形成的不稳定性，其特征是下部和上部极化子分支的强度和线宽波动的亚稳态相关性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16330v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16330v1",
        "title": "Single-View Holographic Volumetric 3D Printing with Coupled Differentiable Wave-Optical and Photochemical Optimization",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Single-View Holographic Volumetric 3D Printing with Coupled Differentiable Wave-Optical and Photochemical Optimization"
        },
        "updated": "2026-01-22T21:27:56Z",
        "updated_parsed": [
            2026,
            1,
            22,
            21,
            27,
            56,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16330v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16330v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Volumetric additive manufacturing promises near-instantaneous fabrication of 3D objects, yet achieving high fidelity at the micro-scale remains challenging due to the complex interplay between optical diffraction and chemical effects. We present \\emph{Single-View Holographic Volumetric Additive Manufacturing} (SHVAM), a mechanically static system that shapes volumetric dose distributions using time-multiplexed, phase-only holograms projected from a single optical axis. To achieve high resolution with SHVAM, we formulate hologram synthesis as a coupled inverse problem, integrating a differentiable wave-optical forward model with a simplified photochemical model that explicitly captures inhibitor diffusion and non-linear dose response. Optimizing hologram sequences under these coupled constraints allows us to pre-compensate for chemical blur, yielding higher print fidelity than optical-only optimization. We demonstrate the efficacy of SHVAM by fabricating simple 2D and 3D structures with lateral feature sizes of approximately \\SI{10}{\\micro\\meter} within a $\\SI{0.8}{\\milli\\meter} \\times \\SI{0.8}{\\milli\\meter} \\times \\SI{3}{\\milli\\meter}$ volume in seconds.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Volumetric additive manufacturing promises near-instantaneous fabrication of 3D objects, yet achieving high fidelity at the micro-scale remains challenging due to the complex interplay between optical diffraction and chemical effects. We present \\emph{Single-View Holographic Volumetric Additive Manufacturing} (SHVAM), a mechanically static system that shapes volumetric dose distributions using time-multiplexed, phase-only holograms projected from a single optical axis. To achieve high resolution with SHVAM, we formulate hologram synthesis as a coupled inverse problem, integrating a differentiable wave-optical forward model with a simplified photochemical model that explicitly captures inhibitor diffusion and non-linear dose response. Optimizing hologram sequences under these coupled constraints allows us to pre-compensate for chemical blur, yielding higher print fidelity than optical-only optimization. We demonstrate the efficacy of SHVAM by fabricating simple 2D and 3D structures with lateral feature sizes of approximately \\SI{10}{\\micro\\meter} within a $\\SI{0.8}{\\milli\\meter} \\times \\SI{0.8}{\\milli\\meter} \\times \\SI{3}{\\milli\\meter}$ volume in seconds."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T21:27:56Z",
        "published_parsed": [
            2026,
            1,
            22,
            21,
            27,
            56,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Felix Wechsler"
            },
            {
                "name": "Riccardo Rizzo"
            },
            {
                "name": "Christophe Moser"
            }
        ],
        "author_detail": {
            "name": "Christophe Moser"
        },
        "author": "Christophe Moser",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "结合可微分波光和光化学优化的单视图全息体积 3D 打印",
        "abstract_cn": "体积增材制造有望近乎瞬时地制造 3D 物体，但由于光学衍射和化学效应之间复杂的相互作用，在微观尺度上实现高保真度仍然具有挑战性。我们提出了 \\emph{单视图全息体积增材制造} (SHVAM)，这是一种机械静态系统，它使用从单个光轴投影的时分复用、纯相位全息图来塑造体积剂量分布。为了利用 SHVAM 实现高分辨率，我们将全息图合成表述为耦合逆问题，将可微分的波光正向模型与简化的光化学模型相结合，该模型明确捕获抑制剂扩散和非线性剂量响应。在这些耦合约束下优化全息图序列使我们能够预先补偿化学模糊，从而比仅光学优化产生更高的打印保真度。我们通过在几秒内的 $\\SI{0.8}{\\milli\\meter} \\times \\SI{0.8}{\\milli\\meter} \\times \\SI{3}{\\milli\\meter}$ 体积内制造横向特征尺寸约为 \\SI{10}{\\micro\\meter} 的简单 2D 和 3D 结构来证明 SHVAM 的功效。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16465v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16465v1",
        "title": "Mode Conversion of Hyperbolic Phonon Polaritons in van der Waals terraces",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Mode Conversion of Hyperbolic Phonon Polaritons in van der Waals terraces"
        },
        "updated": "2026-01-23T05:51:18Z",
        "updated_parsed": [
            2026,
            1,
            23,
            5,
            51,
            18,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16465v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16465v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1038/s41467-025-68030-7",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Electromagnetic hyperbolicity has driven key functionalities in nanophotonics, including super-resolution imaging, efficient energy control, and extreme light manipulation. Central to these advances are hyperbolic polaritons - nanometer-scale light-matter waves - spanning multiple energy-momentum dispersion orders with distinct mode profiles and incrementally high optical momenta. In this work, we report the mode conversion of hyperbolic polaritons across different dispersion orders by breaking the structure symmetry in engineered step-shape van der Waals (vdW) terraces. The mode conversion from the fundamental to high-order hyperbolic polaritons is imaged using scattering-type scanning near-field optical microscopy (s-SNOM) on both hexagonal boron nitride (hBN) and alpha-phase molybdenum trioxide (alpha-MoO3) vdW terraces. Our s-SNOM data, augmented with electromagnetic simulations, further demonstrate the alteration of polariton mode conversion by varying the step size of vdW terraces. The mode conversion reported here offers a practical approach toward integrating previously independent different-order hyperbolic polaritons with ultra-high momenta, paving the way for promising applications in nano-optical circuits, sensing, computation, information processing, and super-resolution imaging.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Electromagnetic hyperbolicity has driven key functionalities in nanophotonics, including super-resolution imaging, efficient energy control, and extreme light manipulation. Central to these advances are hyperbolic polaritons - nanometer-scale light-matter waves - spanning multiple energy-momentum dispersion orders with distinct mode profiles and incrementally high optical momenta. In this work, we report the mode conversion of hyperbolic polaritons across different dispersion orders by breaking the structure symmetry in engineered step-shape van der Waals (vdW) terraces. The mode conversion from the fundamental to high-order hyperbolic polaritons is imaged using scattering-type scanning near-field optical microscopy (s-SNOM) on both hexagonal boron nitride (hBN) and alpha-phase molybdenum trioxide (alpha-MoO3) vdW terraces. Our s-SNOM data, augmented with electromagnetic simulations, further demonstrate the alteration of polariton mode conversion by varying the step size of vdW terraces. The mode conversion reported here offers a practical approach toward integrating previously independent different-order hyperbolic polaritons with ultra-high momenta, paving the way for promising applications in nano-optical circuits, sensing, computation, information processing, and super-resolution imaging."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T05:51:18Z",
        "published_parsed": [
            2026,
            1,
            23,
            5,
            51,
            18,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "arxiv_journal_ref": "Nature Communications (2025)",
        "authors": [
            {
                "name": "Byung-Il Noh"
            },
            {
                "name": "Sina Jafari Ghalekohneh"
            },
            {
                "name": "Mingyuan Chen"
            },
            {
                "name": "Jialiang Shen"
            },
            {
                "name": "Eli Janzen"
            },
            {
                "name": "Lang Zhou"
            },
            {
                "name": "Pengyu Chen"
            },
            {
                "name": "James Edgar"
            },
            {
                "name": "Bo Zhao"
            },
            {
                "name": "Siyuan Dai"
            }
        ],
        "author_detail": {
            "name": "Siyuan Dai"
        },
        "author": "Siyuan Dai",
        "arxiv_doi": "10.1038/s41467-025-68030-7",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "范德华台中双曲声子极化子的模式转换",
        "abstract_cn": "电磁双曲性推动了纳米光子学的关键功能，包括超分辨率成像、高效能量控制和极端光操纵。这些进步的核心是双曲极化激元（纳米级光物质波），跨越多个能量动量色散阶，具有不同的模式分布和递增的高光学动量。在这项工作中，我们通过打破工程阶梯形范德华（vdW）平台中的结构对称性，报告了双曲极化激元在不同色散级之间的模式转换。使用散射型扫描近场光学显微镜 (s-SNOM) 在六方氮化硼 (hBN) 和 α 相三氧化钼 (α-MoO3) vdW 平台上对从基本到高阶双曲极化激元的模式转换进行成像。我们的 s-SNOM 数据通过电磁模拟进行增强，进一步证明了通过改变 vdW 阶跃的步长来改变极化子模式转换。这里报道的模式转换提供了一种将以前独立的不同阶双曲极化子与超高动量集成的实用方法，为纳米光学电路、传感、计算、信息处理和超分辨率成像中的有前景的应用铺平了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16484v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16484v1",
        "title": "Integrated Photonic Quantum Computing: From Silicon to Lithium Niobate",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Integrated Photonic Quantum Computing: From Silicon to Lithium Niobate"
        },
        "updated": "2026-01-23T06:26:34Z",
        "updated_parsed": [
            2026,
            1,
            23,
            6,
            26,
            34,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16484v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16484v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Quantum technologies have surpassed classical systems by leveraging the unique properties of superposition and entanglement in photons and matter. Recent advancements in integrated quantum photonics, especially in silicon-based and lithium niobate platforms, are pushing the technology toward greater scalability and functionality. Silicon circuits have progressed from centimeter-scale, dual-photon systems to millimeter-scale, high-density devices that integrate thousands of components, enabling sophisticated programmable manipulation of multi-photon states. Meanwhile, lithium niobate, thanks to its wide optical transmission window, outstanding nonlinear and electro-optic coefficients, and chemical stability, has emerged as an optimal substrate for fully integrated photonic quantum chips. Devices made from this material exhibit high efficiency in in generating, manipulating, converting, storing, and detecting photon states, thereby establishing a basis for deterministic multi-photon generation and single-photon quantum interactions, as well as comprehensive frequency-state control. This review explores the development of integrated photonic quantum technologies based on both silicon and lithium niobate, highlighting invaluable insights gained from silicon-based systems that can assist the scaling of lithium niobate technologies. It examines the functional integration mechanisms of lithium niobate in electro-optic tuning and nonlinear energy conversion, showcasing its transformative impact throughout the photonic quantum computing process. Looking ahead, we speculate on the developmental pathways for lithium niobate platforms and their potential to revolutionize areas such as quantum communication, complex system simulation, quantum sampling, and optical quantum computing paradigms.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Quantum technologies have surpassed classical systems by leveraging the unique properties of superposition and entanglement in photons and matter. Recent advancements in integrated quantum photonics, especially in silicon-based and lithium niobate platforms, are pushing the technology toward greater scalability and functionality. Silicon circuits have progressed from centimeter-scale, dual-photon systems to millimeter-scale, high-density devices that integrate thousands of components, enabling sophisticated programmable manipulation of multi-photon states. Meanwhile, lithium niobate, thanks to its wide optical transmission window, outstanding nonlinear and electro-optic coefficients, and chemical stability, has emerged as an optimal substrate for fully integrated photonic quantum chips. Devices made from this material exhibit high efficiency in in generating, manipulating, converting, storing, and detecting photon states, thereby establishing a basis for deterministic multi-photon generation and single-photon quantum interactions, as well as comprehensive frequency-state control. This review explores the development of integrated photonic quantum technologies based on both silicon and lithium niobate, highlighting invaluable insights gained from silicon-based systems that can assist the scaling of lithium niobate technologies. It examines the functional integration mechanisms of lithium niobate in electro-optic tuning and nonlinear energy conversion, showcasing its transformative impact throughout the photonic quantum computing process. Looking ahead, we speculate on the developmental pathways for lithium niobate platforms and their potential to revolutionize areas such as quantum communication, complex system simulation, quantum sampling, and optical quantum computing paradigms."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T06:26:34Z",
        "published_parsed": [
            2026,
            1,
            23,
            6,
            26,
            34,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Hui Zhang"
            },
            {
                "name": "Yiming Ma"
            },
            {
                "name": "Di Zhu"
            },
            {
                "name": "Yuancheng Zhan"
            },
            {
                "name": "Yuzhi Shi"
            },
            {
                "name": "Zhanshan Wang"
            },
            {
                "name": "Leong Chuan Kwek"
            },
            {
                "name": "Anthony Laing"
            },
            {
                "name": "Ai Qun Liu"
            },
            {
                "name": "Marko Loncar"
            },
            {
                "name": "Xinbin Cheng"
            }
        ],
        "author_detail": {
            "name": "Xinbin Cheng"
        },
        "author": "Xinbin Cheng",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "集成光子量子计算：从硅到铌酸锂",
        "abstract_cn": "量子技术利用光子和物质的叠加和纠缠的独特性质，已经超越了经典系统。集成量子光子学的最新进展，尤其是硅基和铌酸锂平台，正在推动该技术实现更大的可扩展性和功能性。硅电路已经从厘米级双光子系统发展到集成了数千个组件的毫米级高密度器件，从而能够对多光子状态进行复杂的可编程操纵。同时，铌酸锂凭借其宽的光学传输窗口、出色的非线性和电光系数以及化学稳定性，已成为全集成光子量子芯片的最佳衬底。由这种材料制成的器件在生成、操纵、转换、存储和检测光子态方面表现出高效率，从而为确定性多光子生成和单光子量子相互作用以及全面的频率状态控制奠定了基础。这篇综述探讨了基于硅和铌酸锂的集成光子量子技术的发展，强调了从硅基系统中获得的宝贵见解，这些见解可以帮助扩大铌酸锂技术的规模。它研究了铌酸锂在电光调谐和非线性能量转换中的功能集成机制，展示了其在整个光子量子计算过程中的变革性影响。展望未来，我们推测铌酸锂平台的发展路径及其彻底改变量子通信、复杂系统模拟、量子采样和光量子计算范式等领域的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16604v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16604v1",
        "title": "Enhanced Terahertz Photoresponse via Acoustic Plasmon Cavity Resonances in Scalable Graphene",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Enhanced Terahertz Photoresponse via Acoustic Plasmon Cavity Resonances in Scalable Graphene"
        },
        "updated": "2026-01-23T10:04:59Z",
        "updated_parsed": [
            2026,
            1,
            23,
            10,
            4,
            59,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16604v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16604v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Precise control and nanoscale confinement of terahertz (THz) fields are essential requirements for emerging applications in photonics, quantum technologies, wireless communications, and sensing. Here, we demonstrate a polaritonic cavity enhanced THz photoresponse in an antenna coupled device based on chemical vapor deposited (CVD) monolayer graphene. The dipole antenna lobes simultaneously serve as two gate electrodes, concentrate the impinging THz field, and efficiently launch acoustic graphene plasmons (AGPs), which drive a strong photo-thermoelectric (PTE) signal. Between 6 and 90 K, the photovoltage exhibits pronounced peaks, modulating the PTE response by up to 40\\%, that we attribute to AGPs forming a Fabry Pérot THz cavity in the full or half graphene channel. Combined full wave and transport thermal simulations accurately reproduce the gate controlled plasmon wavelength, spatial absorption profile, and the resulting nonuniform electron heating responsible for the PTE response. The lateral and vertical maximum confinement factors of the AGP wavelength relative to the incident wavelength are 165 and 4000, respectively, for frequencies from 1.83 to 2.52 THz. These results demonstrate that wafer scalable CVD graphene, without hBN encapsulation, can host coherent AGP resonances and exhibit an efficient polaritonic enhanced photoresponse under appropriate gating, antenna coupling, and AGP cavity design, opening a route to scalable, polarization and frequency selective, liquid nitrogen cooled, and low power consumption THz detection platforms based on plasmon thermoelectric transduction.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Precise control and nanoscale confinement of terahertz (THz) fields are essential requirements for emerging applications in photonics, quantum technologies, wireless communications, and sensing. Here, we demonstrate a polaritonic cavity enhanced THz photoresponse in an antenna coupled device based on chemical vapor deposited (CVD) monolayer graphene. The dipole antenna lobes simultaneously serve as two gate electrodes, concentrate the impinging THz field, and efficiently launch acoustic graphene plasmons (AGPs), which drive a strong photo-thermoelectric (PTE) signal. Between 6 and 90 K, the photovoltage exhibits pronounced peaks, modulating the PTE response by up to 40\\%, that we attribute to AGPs forming a Fabry Pérot THz cavity in the full or half graphene channel. Combined full wave and transport thermal simulations accurately reproduce the gate controlled plasmon wavelength, spatial absorption profile, and the resulting nonuniform electron heating responsible for the PTE response. The lateral and vertical maximum confinement factors of the AGP wavelength relative to the incident wavelength are 165 and 4000, respectively, for frequencies from 1.83 to 2.52 THz. These results demonstrate that wafer scalable CVD graphene, without hBN encapsulation, can host coherent AGP resonances and exhibit an efficient polaritonic enhanced photoresponse under appropriate gating, antenna coupling, and AGP cavity design, opening a route to scalable, polarization and frequency selective, liquid nitrogen cooled, and low power consumption THz detection platforms based on plasmon thermoelectric transduction."
        },
        "tags": [
            {
                "term": "cond-mat.mes-hall",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T10:04:59Z",
        "published_parsed": [
            2026,
            1,
            23,
            10,
            4,
            59,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cond-mat.mes-hall"
        },
        "authors": [
            {
                "name": "Domenico De Fazio"
            },
            {
                "name": "Sebastián Castilla"
            },
            {
                "name": "Karuppasamy P. Soundarapandian"
            },
            {
                "name": "Tetiana Slipchenko"
            },
            {
                "name": "Ioannis Vangelidis"
            },
            {
                "name": "Simone Marconi"
            },
            {
                "name": "Riccardo Bertini"
            },
            {
                "name": "Vlad Petrica"
            },
            {
                "name": "Yang Hao"
            },
            {
                "name": "Alessandro Principi"
            },
            {
                "name": "Elefterios Lidorikis"
            },
            {
                "name": "Roshan K. Kumar"
            },
            {
                "name": "Luis Martín-Moreno"
            },
            {
                "name": "Frank H. L. Koppens"
            }
        ],
        "author_detail": {
            "name": "Frank H. L. Koppens"
        },
        "author": "Frank H. L. Koppens",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过可扩展石墨烯中的声等离子腔共振增强太赫兹光响应",
        "abstract_cn": "太赫兹 (THz) 场的精确控制和纳米级限制是光子学、量子技术、无线通信和传感领域新兴应用的基本要求。在这里，我们展示了基于化学气相沉积（CVD）单层石墨烯的天线耦合器件中的极化子腔增强的太赫兹光响应。偶极天线波瓣同时充当两个栅电极，集中撞击的太赫兹场，并有效发射声石墨烯等离子体（AGP），从而驱动强光热电（PTE）信号。在 6 到 90 K 之间，光电压表现出明显的峰值，将 PTE 响应调制高达 40%，我们将其归因于 AGP 在全石墨烯通道或半石墨烯通道中形成法布里珀罗太赫兹腔。组合的全波和传输热模拟可以准确地再现栅极控制的等离激元波长、空间吸收剖面以及导致 PTE 响应的不均匀电子加热。对于 1.83 至 2.52 THz 的频率，AGP 波长相对于入射波长的横向和垂直最大限制因子分别为 165 和 4000。这些结果表明，无需六方氮化硼封装的晶圆可扩展CVD石墨烯可以承载相干AGP谐振，并在适当的选通、天线耦合和AGP腔设计下表现出有效的极化增强光响应，为基于等离子体热电传导的可扩展、极化和频率选择性、液氮冷却和低功耗太赫兹检测平台开辟了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16666v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16666v1",
        "title": "Fast compression of pure-quartic solitons in nonlinear optical fibers via shortcuts to adiabaticity",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fast compression of pure-quartic solitons in nonlinear optical fibers via shortcuts to adiabaticity"
        },
        "updated": "2026-01-23T11:30:44Z",
        "updated_parsed": [
            2026,
            1,
            23,
            11,
            30,
            44,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16666v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16666v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1103/yzgb-y1tc",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Pure-quartic solitons (PQSs) supported by negative fourth-order dispersion have recently attracted considerable interest. In this work, we study both adiabatic and nonadiabatic compression of PQSs in nonlinear optical fibers with pure quartic dispersion in the presence of distributed gain and loss. Within a variational framework, we show that, for weak constant gain, the adiabatic compression dynamics can be mapped onto the motion of an effective particle in a slowly deformed potential, providing an intuitive physical picture. To overcome the long propagation distance required by conventional adiabatic condition, we exploit shortcuts to adiabaticity (STA) based on inverse engineering and derive analytical gain-loss profiles, with appropriate boundary conditions that realize a prescribed fast compression over a shorter propagation distance. Numerical simulations confirm the theoretical predictions and indicate a minimum propagation distance below which noticeable waveform distortion emerges. Compared with standard adiabatic references, the STA design significantly reduces the required compression distance while maintaining high-fidelity PQS evolution.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Pure-quartic solitons (PQSs) supported by negative fourth-order dispersion have recently attracted considerable interest. In this work, we study both adiabatic and nonadiabatic compression of PQSs in nonlinear optical fibers with pure quartic dispersion in the presence of distributed gain and loss. Within a variational framework, we show that, for weak constant gain, the adiabatic compression dynamics can be mapped onto the motion of an effective particle in a slowly deformed potential, providing an intuitive physical picture. To overcome the long propagation distance required by conventional adiabatic condition, we exploit shortcuts to adiabaticity (STA) based on inverse engineering and derive analytical gain-loss profiles, with appropriate boundary conditions that realize a prescribed fast compression over a shorter propagation distance. Numerical simulations confirm the theoretical predictions and indicate a minimum propagation distance below which noticeable waveform distortion emerges. Compared with standard adiabatic references, the STA design significantly reduces the required compression distance while maintaining high-fidelity PQS evolution."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T11:30:44Z",
        "published_parsed": [
            2026,
            1,
            23,
            11,
            30,
            44,
            4,
            23,
            0
        ],
        "arxiv_comment": "9 pages, 5 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "arxiv_journal_ref": "Phys. Rev. A (2006)",
        "authors": [
            {
                "name": "Chengyu Han"
            },
            {
                "name": "Qian Kong"
            },
            {
                "name": "Ming Shen"
            },
            {
                "name": "Xi Chen"
            }
        ],
        "author_detail": {
            "name": "Xi Chen"
        },
        "author": "Xi Chen",
        "arxiv_doi": "10.1103/yzgb-y1tc",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过绝热性捷径快速压缩非线性光纤中的纯四次孤子",
        "abstract_cn": "由负四阶色散支持的纯四次孤子（PQS）最近引起了相当大的兴趣。在这项工作中，我们研究了在存在分布式增益和损耗的情况下具有纯四次色散的非线性光纤中 PQS 的绝热和非绝热压缩。在变分框架内，我们表明，对于弱恒定增益，绝热压缩动力学可以映射到缓慢变形势中的有效粒子的运动，从而提供直观的物理图像。为了克服传统绝热条件所需的长传播距离，我们利用基于逆向工程的绝热（STA）捷径，并导出分析增益损失曲线，并采用适当的边界条件，在较短的传播距离上实现规定的快速压缩。数值模拟证实了理论预测，并指出了最小传播距离，低于该距离会出现明显的波形失真。与标准绝热参考相比，STA 设计显着缩短了所需的压缩距离，同时保持高保真 PQS 演化。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16732v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16732v1",
        "title": "Multi-wavelength UV Upconversion in Lanthanides assisted by Photonic Crystals",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multi-wavelength UV Upconversion in Lanthanides assisted by Photonic Crystals"
        },
        "updated": "2026-01-23T13:29:17Z",
        "updated_parsed": [
            2026,
            1,
            23,
            13,
            29,
            17,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16732v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16732v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Upconversion luminescence consists of the absorption of low-energies photons followed by the emission of a higher energy photon. The process has mainly been studied in lanthanides to upconvert monochromatic near-infrared excitation to near-infrared or visible light, and has been exploited only to a limited extent to upconvert broad excitations to ultra-violet. In addition, upconverting near-infrared and visible light to ultra-violet is crucial for applications such as solar-to-fuel conversion or environmental remediation. However, upconversion luminescence is limited by the low absorption cross-sections of lanthanides. In this work, we engineered Bloch modes in a photonic crystal to assist a multi-wavelength upconversion mechanism and demonstrated a 28-fold enhancement of ultra-violet upconversion luminescence of Yb3+-Tm3+ doped thin films. Materials were selected and optimized to design nanostructures without parasitic absorption losses. The geometric parameters of the photonic crystals were scanned to match a slow-light resonance with an excited-state transition of Tm3+ and thus enhance incident visible light absorption. Ultra-violet light extraction was also enhanced by photonic crystal Bloch modes. Each of these two contributions were quantified and the measured photonic band structures were well reproduced by electromagnetic simulations.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Upconversion luminescence consists of the absorption of low-energies photons followed by the emission of a higher energy photon. The process has mainly been studied in lanthanides to upconvert monochromatic near-infrared excitation to near-infrared or visible light, and has been exploited only to a limited extent to upconvert broad excitations to ultra-violet. In addition, upconverting near-infrared and visible light to ultra-violet is crucial for applications such as solar-to-fuel conversion or environmental remediation. However, upconversion luminescence is limited by the low absorption cross-sections of lanthanides. In this work, we engineered Bloch modes in a photonic crystal to assist a multi-wavelength upconversion mechanism and demonstrated a 28-fold enhancement of ultra-violet upconversion luminescence of Yb3+-Tm3+ doped thin films. Materials were selected and optimized to design nanostructures without parasitic absorption losses. The geometric parameters of the photonic crystals were scanned to match a slow-light resonance with an excited-state transition of Tm3+ and thus enhance incident visible light absorption. Ultra-violet light extraction was also enhanced by photonic crystal Bloch modes. Each of these two contributions were quantified and the measured photonic band structures were well reproduced by electromagnetic simulations."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T13:29:17Z",
        "published_parsed": [
            2026,
            1,
            23,
            13,
            29,
            17,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Damien Rinnert"
            },
            {
                "name": "Emmanuel Drouard"
            },
            {
                "name": "Antonio Pereira"
            },
            {
                "name": "Celine Chevalier"
            },
            {
                "name": "Aziz Benamrouche"
            },
            {
                "name": "Benjamin Fornacciari"
            },
            {
                "name": "Hai Son Nguyen"
            },
            {
                "name": "Gilles Ledoux"
            },
            {
                "name": "Christian Seassal"
            }
        ],
        "author_detail": {
            "name": "Christian Seassal"
        },
        "author": "Christian Seassal",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "光子晶体辅助的镧系元素多波长紫外上转换",
        "abstract_cn": "上转换发光包括吸收低能量光子，然后发射更高能量的光子。该过程主要在镧系元素中进行研究，将单色近红外激发上转换为近红外或可见光，并且仅在有限程度上利用将宽激发上转换为紫外线。此外，将近红外和可见光上转换为紫外线对于太阳能到燃料转换或环境修复等应用至关重要。然而，上转换发光受到镧系元素低吸收截面的限制。在这项工作中，我们在光子晶体中设计了布洛赫模式以辅助多波长上转换机制，并证明了 Yb3+-Tm3+ 掺杂薄膜的紫外上转换发光增强了 28 倍。选择并优化材料以设计没有寄生吸收损失的纳米结构。扫描光子晶体的几何参数，以匹配慢光共振与 Tm3+ 的激发态跃迁，从而增强入射可见光吸收。光子晶体布洛赫模式也增强了紫外光提取。这两个贡献均被量化，并且通过电磁模拟很好地再现了测量的光子能带结构。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16747v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16747v1",
        "title": "Moderate-terahertz-induced plateau expansion of high-order harmonic generation to soft X-ray region",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Moderate-terahertz-induced plateau expansion of high-order harmonic generation to soft X-ray region"
        },
        "updated": "2026-01-23T13:52:02Z",
        "updated_parsed": [
            2026,
            1,
            23,
            13,
            52,
            2,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16747v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16747v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Extending the high-harmonic cutoff with experimentally accessible fields is essential for advancing tabletop coherent extreme ultraviolet (EUV) and soft X-ray sources. Although terahertz (THz) assistance offers a promising route, cutoff extension at weak, laboratory-accessible THz strengths remain poorly understood. In this report, we comprehensively investigate THz-assisted high-order harmonic generation (HHG) using time-dependent Schrödinger equation simulations supported by classical trajectory analysis and Bohmian-based quantum dynamics. By mapping the plateau evolution versus THz strength, we show that even weak THz fields can extend the cutoff, producing a pronounced ``fish-fin'' structure whose prominent rays saturate near $I_p + 8 U_p$. We trace this extension to long electron excursions spanning several optical cycles before recombination, and provide a fully consistent explanation using both classical analysis and Bohmian trajectories flow. Our findings reveal that this cutoff-extension mechanism is remarkably robust, persisting across different atomic species and remaining insensitive to variations in the driving parameters. These results demonstrate that cutoff control is achievable with laboratory-scale THz fields, offering practical guidelines for engineering coherent high-energy HHG, and providing a robust pathway for tracking ultrafast electron motion in real time.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Extending the high-harmonic cutoff with experimentally accessible fields is essential for advancing tabletop coherent extreme ultraviolet (EUV) and soft X-ray sources. Although terahertz (THz) assistance offers a promising route, cutoff extension at weak, laboratory-accessible THz strengths remain poorly understood. In this report, we comprehensively investigate THz-assisted high-order harmonic generation (HHG) using time-dependent Schrödinger equation simulations supported by classical trajectory analysis and Bohmian-based quantum dynamics. By mapping the plateau evolution versus THz strength, we show that even weak THz fields can extend the cutoff, producing a pronounced ``fish-fin'' structure whose prominent rays saturate near $I_p + 8 U_p$. We trace this extension to long electron excursions spanning several optical cycles before recombination, and provide a fully consistent explanation using both classical analysis and Bohmian trajectories flow. Our findings reveal that this cutoff-extension mechanism is remarkably robust, persisting across different atomic species and remaining insensitive to variations in the driving parameters. These results demonstrate that cutoff control is achievable with laboratory-scale THz fields, offering practical guidelines for engineering coherent high-energy HHG, and providing a robust pathway for tracking ultrafast electron motion in real time."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T13:52:02Z",
        "published_parsed": [
            2026,
            1,
            23,
            13,
            52,
            2,
            4,
            23,
            0
        ],
        "arxiv_comment": "10 pages, 3 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Doan-An Trieu"
            },
            {
                "name": "Duong D. Hoang-Trong"
            },
            {
                "name": "Cam-Tu Le"
            },
            {
                "name": "Sang Ha"
            },
            {
                "name": "Ngoc-Hung Phan"
            },
            {
                "name": "F. V. Potemkin"
            },
            {
                "name": "Van-Hoang Le"
            },
            {
                "name": "Ngoc-Loan Phan"
            }
        ],
        "author_detail": {
            "name": "Ngoc-Loan Phan"
        },
        "author": "Ngoc-Loan Phan",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "中太赫兹引起的高次谐波产生平台扩展至软 X 射线区域",
        "abstract_cn": "通过实验可达到的场来扩展高次谐波截止对于推进桌面相干极紫外 (EUV) 和软 X 射线源至关重要。尽管太赫兹 (THz) 辅助提供了一条很有前途的途径，但对弱的、实验室可达到的太赫兹强度的截止扩展仍然知之甚少。在本报告中，我们使用经典轨迹分析和基于波姆量子动力学支持的瞬态薛定谔方程模拟来全面研究太赫兹辅助高阶谐波产生（HHG）。通过绘制高原演化与太赫兹强度的关系图，我们发现即使是弱太赫兹场也可以延长截止范围，产生明显的“鱼鳍”结构，其突出的射线在 $I_p + 8 U_p$ 附近饱和。我们将这种延伸追溯到复合之前跨越几个光学周期的长电子偏移，并使用经典分析和玻姆轨迹流提供了完全一致的解释。我们的研究结果表明，这种截止延伸机制非常稳健，在不同的原子种类中持续存在，并且对驱动参数的变化不敏感。这些结果表明，通过实验室规模的太赫兹场可以实现截止控制，为工程相干高能 HHG 提供实用指南，并为实时跟踪超快电子运动提供可靠的途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16790v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16790v1",
        "title": "Observation of polaritonic flat-band bound states in the continuum in a 2D magnet",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Observation of polaritonic flat-band bound states in the continuum in a 2D magnet"
        },
        "updated": "2026-01-23T14:36:53Z",
        "updated_parsed": [
            2026,
            1,
            23,
            14,
            36,
            53,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16790v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16790v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Flat-band bound states in the continuum (BICs) are topological states with suppressed group velocity and robustness against radiation loss, offering a powerful platform for the exploration of non-Hermitian, nonlinear, topological phenomena and device applications. Van der Waals (vdW) metasurfaces have recently emerged as promising candidates for sustaining BICs and hybridizing with material transitions. However, the realization of flat-band BICs remains elusive. Here, we experimentally demonstrate polaritonic high-order BICs on a wide-angle flat band utilizing a subwavelength metasurface made of a vdW magnet CrSBr. The large oscillator strength of direct excitons in CrSBr enables near ultrastrong coupling with BICs, leading to strongly suppressed polaritonic angular dispersions. Remarkably, second-order polaritonic BICs become flat-band across a wide angular range, with corresponding Q factors exceeding 1500. Additionally, we find that these polaritonic BICs vanish in the transverse magnetic configuration, while leading to fascinating surface hyperbolic exciton-polaritons within the Reststrahlen band. Our findings underscore CrSBr as an exceptional platform for exploring flat-band photonics and polaritonics, paving the new avenue for advances in next-generation optical and quantum technologies.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Flat-band bound states in the continuum (BICs) are topological states with suppressed group velocity and robustness against radiation loss, offering a powerful platform for the exploration of non-Hermitian, nonlinear, topological phenomena and device applications. Van der Waals (vdW) metasurfaces have recently emerged as promising candidates for sustaining BICs and hybridizing with material transitions. However, the realization of flat-band BICs remains elusive. Here, we experimentally demonstrate polaritonic high-order BICs on a wide-angle flat band utilizing a subwavelength metasurface made of a vdW magnet CrSBr. The large oscillator strength of direct excitons in CrSBr enables near ultrastrong coupling with BICs, leading to strongly suppressed polaritonic angular dispersions. Remarkably, second-order polaritonic BICs become flat-band across a wide angular range, with corresponding Q factors exceeding 1500. Additionally, we find that these polaritonic BICs vanish in the transverse magnetic configuration, while leading to fascinating surface hyperbolic exciton-polaritons within the Reststrahlen band. Our findings underscore CrSBr as an exceptional platform for exploring flat-band photonics and polaritonics, paving the new avenue for advances in next-generation optical and quantum technologies."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mes-hall",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T14:36:53Z",
        "published_parsed": [
            2026,
            1,
            23,
            14,
            36,
            53,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Fuhuan Shen"
            },
            {
                "name": "Jiahao Ren"
            },
            {
                "name": "Zhiyi Yuan"
            },
            {
                "name": "Kai Wu"
            },
            {
                "name": "Sai Yan"
            },
            {
                "name": "Kunal Parasad"
            },
            {
                "name": "Hai Son Nguyen"
            },
            {
                "name": "Rui Su"
            }
        ],
        "author_detail": {
            "name": "Rui Su"
        },
        "author": "Rui Su",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "二维磁体连续体中极化子平带束缚态的观察",
        "abstract_cn": "连续体中的平带束缚态 (BIC) 是具有抑制群速度和抗辐射损耗鲁棒性的拓扑态，为探索非厄米特、非线性、拓扑现象和器件应用提供了强大的平台。范德华 (vdW) 超表面最近成为维持 BIC 和与材料转变混合的有希望的候选者。然而，平带 BIC 的实现仍然难以实现。在这里，我们利用 vdW 磁体 CrSBr 制成的亚波长超表面，在广角平带上实验演示了极化高阶 BIC。 CrSBr 中直接激子的大振荡强度可实现与 BIC 的近超强耦合，从而强烈抑制极化子角色散。值得注意的是，二阶极化子 BIC 在很宽的角度范围内变成平带，相应的 Q 因子超过 1500。此外，我们发现这些极化子 BIC 在横向磁配置中消失，同时在 Reststrahlen 能带内产生令人着迷的表面双曲激子极化子。我们的研究结果强调了 CrSBr 作为探索平带光子学和极化子学的特殊平台，为下一代光学和量子技术的进步铺平了新途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16797v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16797v1",
        "title": "Ultrafast Dipolar Electrostatic Modeling of Plasmonic Nanoparticles with Arbitrary Geometry",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Ultrafast Dipolar Electrostatic Modeling of Plasmonic Nanoparticles with Arbitrary Geometry"
        },
        "updated": "2026-01-23T14:50:51Z",
        "updated_parsed": [
            2026,
            1,
            23,
            14,
            50,
            51,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16797v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16797v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate and fast calculations of localized surface plasmon resonances (LSPR) in metallic nanoparticles is essential for applications in sensing, nano-optics, and energy harvesting. Although full-wave numerical techniques such as the boundary element method (BEM) or the discrete dipole approximation (DDA) provide high accuracy, their computational cost often hinders rapid parametric studies. Here it is presented an ultrafast method that avoids solving large eigenproblems. Instead, only the dipolar component of the induced surface charge density \\((σ_{dipolar})\\) is retained through a expansion into Cartesion dipole basis, yielding a compact $3\\times3$ geometric formulation that avoids full boundary-integral solves. The spectral response is obtained in a similar way, by projecting the Neumann--Poincaré surface operator onto the dipole subspace and evaluating a Rayleigh quotient, giving geometry-only eigenvalues again without an $N\\times N$ eigenproblem. A major advantage of this method is that all geometry-dependent quantities are computed once per nanoparticle, while material dispersion and environmental changes enter only through simple algebraic expressions for the polarizability, enabling rapid evaluation across wavelengths. Retardation effects are incorporated through the modified long-wavelength approximation (MLWA), extending accuracy into the weakly retarded regime. The resulting framework provides a valuable tool for fast modelling and optimization of plasmonic nanoparticles at a significant lesser computational cost than BEM, DDA, and other standard tools.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate and fast calculations of localized surface plasmon resonances (LSPR) in metallic nanoparticles is essential for applications in sensing, nano-optics, and energy harvesting. Although full-wave numerical techniques such as the boundary element method (BEM) or the discrete dipole approximation (DDA) provide high accuracy, their computational cost often hinders rapid parametric studies. Here it is presented an ultrafast method that avoids solving large eigenproblems. Instead, only the dipolar component of the induced surface charge density \\((σ_{dipolar})\\) is retained through a expansion into Cartesion dipole basis, yielding a compact $3\\times3$ geometric formulation that avoids full boundary-integral solves. The spectral response is obtained in a similar way, by projecting the Neumann--Poincaré surface operator onto the dipole subspace and evaluating a Rayleigh quotient, giving geometry-only eigenvalues again without an $N\\times N$ eigenproblem. A major advantage of this method is that all geometry-dependent quantities are computed once per nanoparticle, while material dispersion and environmental changes enter only through simple algebraic expressions for the polarizability, enabling rapid evaluation across wavelengths. Retardation effects are incorporated through the modified long-wavelength approximation (MLWA), extending accuracy into the weakly retarded regime. The resulting framework provides a valuable tool for fast modelling and optimization of plasmonic nanoparticles at a significant lesser computational cost than BEM, DDA, and other standard tools."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "math-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.comp-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T14:50:51Z",
        "published_parsed": [
            2026,
            1,
            23,
            14,
            50,
            51,
            4,
            23,
            0
        ],
        "arxiv_comment": "14 pages, 9 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Paulo S. S. dos Santos"
            },
            {
                "name": "João P. Mendes"
            },
            {
                "name": "José M. M M. de Almeida"
            },
            {
                "name": "Luís C. C. Coelho"
            }
        ],
        "author_detail": {
            "name": "Luís C. C. Coelho"
        },
        "author": "Luís C. C. Coelho",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "任意几何形状的等离子体纳米颗粒的超快偶极静电建模",
        "abstract_cn": "金属纳米颗粒中局域表面等离子体共振 (LSPR) 的准确快速计算对于传感、纳米光学和能量收集等应用至关重要。尽管边界元法 (BEM) 或离散偶极近似 (DDA) 等全波数值技术可提供高精度，但其计算成本往往会阻碍快速参数化研究。这里提出了一种避免求解大特征值问题的超快方法。相反，通过展开到笛卡尔偶极子基础，仅保留感应表面电荷密度的偶极分量 \\((σ_{偶极})\\)，从而产生紧凑的 $3\\times3$ 几何公式，避免完全边界积分求解。光谱响应以类似的方式获得，通过将 Neumann--Poincaré 曲面算子投影到偶极子空间并评估瑞利商，再次给出纯几何特征值，而无需 $N\\times N$ 特征值问题。该方法的一个主要优点是，所有与几何相关的量都对每个纳米粒子计算一次，而材料色散和环境变化仅通过极化率的简单代数表达式进入，从而能够跨波长进行快速评估。通过改进的长波长近似 (MLWA) 纳入延迟效应，将精度扩展到弱延迟区域。由此产生的框架为等离子体纳米粒子的快速建模和优化提供了一个有价值的工具，其计算成本比 BEM、DDA 和其他标准工具要低得多。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16904v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16904v1",
        "title": "Clinical Feasibility of Label-Free Digital Staining Using Mid-Infrared Microscopy at Subcellular Resolution",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Clinical Feasibility of Label-Free Digital Staining Using Mid-Infrared Microscopy at Subcellular Resolution"
        },
        "updated": "2026-01-23T17:13:30Z",
        "updated_parsed": [
            2026,
            1,
            23,
            17,
            13,
            30,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16904v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16904v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We present a rapid, large-field bimodal imaging platform that integrates conventional brightfield microscopy with a lensless IR imaging scanner, enabling whole-slide IR image stack acquisition in minutes. Using a dedicated deep learning model, we implement an optical HE staining strategy based on subcellular morpho-spectral fingerprinting.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We present a rapid, large-field bimodal imaging platform that integrates conventional brightfield microscopy with a lensless IR imaging scanner, enabling whole-slide IR image stack acquisition in minutes. Using a dedicated deep learning model, we implement an optical HE staining strategy based on subcellular morpho-spectral fingerprinting."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.bio-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T17:13:30Z",
        "published_parsed": [
            2026,
            1,
            23,
            17,
            13,
            30,
            4,
            23,
            0
        ],
        "arxiv_comment": "33 pages, 15 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "L. Duraffourg"
            },
            {
                "name": "H. Borges"
            },
            {
                "name": "M. Fernandes"
            },
            {
                "name": "M. Beurrier-Bousquet"
            },
            {
                "name": "J. Baraillon"
            },
            {
                "name": "B. Taurel"
            },
            {
                "name": "J. Le Galudec"
            },
            {
                "name": "K. Vianey"
            },
            {
                "name": "C. Maisin"
            },
            {
                "name": "L. Samaison"
            },
            {
                "name": "F. Staroz"
            },
            {
                "name": "M. Dupoy"
            }
        ],
        "author_detail": {
            "name": "M. Dupoy"
        },
        "author": "M. Dupoy",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "使用亚细胞分辨率的中红外显微镜进行无标记数字染色的临床可行性",
        "abstract_cn": "我们推出了一种快速、大视场双模成像平台，该平台将传统的明视场显微镜与无透镜红外成像扫描仪集成在一起，可在几分钟内采集整个载玻片红外图像堆栈。使用专用的深度学习模型，我们实施了基于亚细胞形态光谱指纹识别的光学 HE 染色策略。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16941v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16941v1",
        "title": "Quantum Fisher information analysis for absorption measurements with undetected photons",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Quantum Fisher information analysis for absorption measurements with undetected photons"
        },
        "updated": "2026-01-23T17:57:52Z",
        "updated_parsed": [
            2026,
            1,
            23,
            17,
            57,
            52,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16941v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16941v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We theoretically compare the quantum Fisher information (QFI) for three configurations of absorption spectroscopy with undetected idler photons: an SU(1,1) interferometer with inter-source idler loss, an induced-coherence (IC) setup in which the idler partially seeds a second squeezer together with a vacuum ancilla, and a distributed-loss (DL) scheme with in-medium attenuation. We calculate the QFI as a function of parametric gain for both full and signal-only detection access. For losses below 99% and low to moderate gain, the SU(1,1) configuration provides the largest QFI. At high gain and intermediate loss, the IC scheme performs best, while under extreme attenuation (transmission $<$ 1%) the DL model becomes optimal. These results delineate the measurement regimes in which each architecture is optimal in terms of information theory.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We theoretically compare the quantum Fisher information (QFI) for three configurations of absorption spectroscopy with undetected idler photons: an SU(1,1) interferometer with inter-source idler loss, an induced-coherence (IC) setup in which the idler partially seeds a second squeezer together with a vacuum ancilla, and a distributed-loss (DL) scheme with in-medium attenuation. We calculate the QFI as a function of parametric gain for both full and signal-only detection access. For losses below 99% and low to moderate gain, the SU(1,1) configuration provides the largest QFI. At high gain and intermediate loss, the IC scheme performs best, while under extreme attenuation (transmission $<$ 1%) the DL model becomes optimal. These results delineate the measurement regimes in which each architecture is optimal in terms of information theory."
        },
        "tags": [
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T17:57:52Z",
        "published_parsed": [
            2026,
            1,
            23,
            17,
            57,
            52,
            4,
            23,
            0
        ],
        "arxiv_comment": "15 pages, 4 figures, 5 appendices",
        "arxiv_primary_category": {
            "term": "quant-ph"
        },
        "authors": [
            {
                "name": "Martin Houde"
            },
            {
                "name": "Franz Roeder"
            },
            {
                "name": "Christine Silberhorn"
            },
            {
                "name": "Benjamin Brecht"
            },
            {
                "name": "Nicolás Quesada"
            }
        ],
        "author_detail": {
            "name": "Nicolás Quesada"
        },
        "author": "Nicolás Quesada",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于未检测到光子吸收测量的量子费希尔信息分析",
        "abstract_cn": "我们从理论上比较了三种吸收光谱配置与未检测到的闲频光子的量子费希尔信息（QFI）：具有源间闲频损耗的 SU(1,1) 干涉仪、闲频光源与真空辅助设备一起部分播种第二个压缩器的诱导相干 (IC) 设置，以及具有介质内衰减的分布式损耗 (DL) 方案。我们将 QFI 计算为完整检测访问和仅信号检测访问的参数增益的函数。对于低于 99% 的损耗和低至中等增益，SU(1,1) 配置提供最大的 QFI。在高增益和中等损耗时，IC 方案表现最佳，而在极端衰减（传输 $<$ 1%）下，DL 模型变得最优。这些结果描绘了每种架构在信息论方面都是最佳的测量机制。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16359v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16359v1",
        "title": "Experience with Single Domain Generalization in Real World Medical Imaging Deployments",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Experience with Single Domain Generalization in Real World Medical Imaging Deployments"
        },
        "updated": "2026-01-22T23:11:48Z",
        "updated_parsed": [
            2026,
            1,
            22,
            23,
            11,
            48,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16359v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16359v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "A desirable property of any deployed artificial intelligence is generalization across domains, i.e. data generation distribution under a specific acquisition condition. In medical imagining applications the most coveted property for effective deployment is Single Domain Generalization (SDG), which addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. In multi-center studies, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare class characteristics. This paper presents our experience on SDG in real life deployment for two exemplary medical imaging case studies on seizure onset zone detection using fMRI data, and stress electrocardiogram based coronary artery detection. Utilizing the commonly used application of diabetic retinopathy, we first demonstrate that state-of-the-art SDG techniques fail to achieve generalized performance across data domains. We then develop a generic expert knowledge integrated deep learning technique DL+EKE and instantiate it for the DR application and show that DL+EKE outperforms SOTA SDG methods on DR. We then deploy instances of DL+EKE technique on the two real world examples of stress ECG and resting state (rs)-fMRI and discuss issues faced with SDG techniques.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A desirable property of any deployed artificial intelligence is generalization across domains, i.e. data generation distribution under a specific acquisition condition. In medical imagining applications the most coveted property for effective deployment is Single Domain Generalization (SDG), which addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. In multi-center studies, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare class characteristics. This paper presents our experience on SDG in real life deployment for two exemplary medical imaging case studies on seizure onset zone detection using fMRI data, and stress electrocardiogram based coronary artery detection. Utilizing the commonly used application of diabetic retinopathy, we first demonstrate that state-of-the-art SDG techniques fail to achieve generalized performance across data domains. We then develop a generic expert knowledge integrated deep learning technique DL+EKE and instantiate it for the DR application and show that DL+EKE outperforms SOTA SDG methods on DR. We then deploy instances of DL+EKE technique on the two real world examples of stress ECG and resting state (rs)-fMRI and discuss issues faced with SDG techniques."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T23:11:48Z",
        "published_parsed": [
            2026,
            1,
            22,
            23,
            11,
            48,
            3,
            22,
            0
        ],
        "arxiv_comment": "Accepted at AAAI 2026 Innovative Applications of Artificial Intelligence",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Ayan Banerjee"
            },
            {
                "name": "Komandoor Srivathsan"
            },
            {
                "name": "Sandeep K. S. Gupta"
            }
        ],
        "author_detail": {
            "name": "Sandeep K. S. Gupta"
        },
        "author": "Sandeep K. S. Gupta",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "现实世界医学成像部署中单域泛化的经验",
        "abstract_cn": "任何部署的人工智能的一个理想特性是跨领域的泛化，即特定采集条件下的数据生成分布。在医学成像应用中，有效部署最令人垂涎​​的属性是单域泛化（SDG），它解决了在单个域上训练模型以确保其能够很好地泛化到看不见的目标域的挑战。在多中心研究中，扫描仪和成像协议的差异引入了领域转移，加剧了稀有类别特征的变异性。本文介绍了我们在现实生活中部署 SDG 的经验，包括使用 fMRI 数据检测癫痫发作区和基于冠状动脉检测的负荷心电图的两个示例性医学成像案例研究。利用糖尿病视网膜病变的常用应用，我们首先证明最先进的 SDG 技术无法实现跨数据域的通用性能。然后，我们开发了一种集成深度学习技术 DL+EKE 的通用专家知识，并将其实例化用于 DR 应用，并表明 DL+EKE 在 DR 上优于 SOTA SDG 方法。然后，我们在压力心电图和静息状态 (rs)-fMRI 的两个现实示例上部署 DL+EKE 技术实例，并讨论 SDG 技术面临的问题。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16383v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16383v1",
        "title": "On The Robustness of Foundational 3D Medical Image Segmentation Models Against Imprecise Visual Prompts",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "On The Robustness of Foundational 3D Medical Image Segmentation Models Against Imprecise Visual Prompts"
        },
        "updated": "2026-01-23T00:55:02Z",
        "updated_parsed": [
            2026,
            1,
            23,
            0,
            55,
            2,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16383v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16383v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "While 3D foundational models have shown promise for promptable segmentation of medical volumes, their robustness to imprecise prompts remains under-explored. In this work, we aim to address this gap by systematically studying the effect of various controlled perturbations of dense visual prompts, that closely mimic real-world imprecision. By conducting experiments with two recent foundational models on a multi-organ abdominal segmentation task, we reveal several facets of promptable medical segmentation, especially pertaining to reliance on visual shape and spatial cues, and the extent of resilience of models towards certain perturbations. Codes are available at: https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "While 3D foundational models have shown promise for promptable segmentation of medical volumes, their robustness to imprecise prompts remains under-explored. In this work, we aim to address this gap by systematically studying the effect of various controlled perturbations of dense visual prompts, that closely mimic real-world imprecision. By conducting experiments with two recent foundational models on a multi-organ abdominal segmentation task, we reveal several facets of promptable medical segmentation, especially pertaining to reliance on visual shape and spatial cues, and the extent of resilience of models towards certain perturbations. Codes are available at: https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs"
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T00:55:02Z",
        "published_parsed": [
            2026,
            1,
            23,
            0,
            55,
            2,
            4,
            23,
            0
        ],
        "arxiv_comment": "Accepted at ISBI 2026",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Soumitri Chattopadhyay"
            },
            {
                "name": "Basar Demir"
            },
            {
                "name": "Marc Niethammer"
            }
        ],
        "author_detail": {
            "name": "Marc Niethammer"
        },
        "author": "Marc Niethammer",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "基础 3D 医学图像分割模型针对不精确视觉提示的鲁棒性",
        "abstract_cn": "虽然 3D 基础模型已显示出快速分割医疗体积的希望，但它们对不精确提示的鲁棒性仍有待探索。在这项工作中，我们的目标是通过系统地研究密集视觉提示的各种受控扰动的影响来解决这一差距，这些扰动密切模仿现实世界的不精确性。通过对两个最近的多器官腹部分割任务的基础模型进行实验，我们揭示了快速医学分割的几个方面，特别是与视觉形状和空间线索的依赖，以及模型对某些扰动的弹性程度有关。代码位于：https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs"
    },
    {
        "id": "http://arxiv.org/abs/2601.16602v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16602v1",
        "title": "Unsupervised Super-Resolution of Hyperspectral Remote Sensing Images Using Fully Synthetic Training",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Unsupervised Super-Resolution of Hyperspectral Remote Sensing Images Using Fully Synthetic Training"
        },
        "updated": "2026-01-23T10:04:09Z",
        "updated_parsed": [
            2026,
            1,
            23,
            10,
            4,
            9,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16602v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16602v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Considerable work has been dedicated to hyperspectral single image super-resolution to improve the spatial resolution of hyperspectral images and fully exploit their potential. However, most of these methods are supervised and require some data with ground truth for training, which is often non-available. To overcome this problem, we propose a new unsupervised training strategy for the super-resolution of hyperspectral remote sensing images, based on the use of synthetic abundance data. Its first step decomposes the hyperspectral image into abundances and endmembers by unmixing. Then, an abundance super-resolution neural network is trained using synthetic abundances, which are generated using the dead leaves model in such a way as to faithfully mimic real abundance statistics. Next, the spatial resolution of the considered hyperspectral image abundances is increased using this trained network, and the high resolution hyperspectral image is finally obtained by recombination with the endmembers. Experimental results show the training potential of the synthetic images, and demonstrate the method effectiveness.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Considerable work has been dedicated to hyperspectral single image super-resolution to improve the spatial resolution of hyperspectral images and fully exploit their potential. However, most of these methods are supervised and require some data with ground truth for training, which is often non-available. To overcome this problem, we propose a new unsupervised training strategy for the super-resolution of hyperspectral remote sensing images, based on the use of synthetic abundance data. Its first step decomposes the hyperspectral image into abundances and endmembers by unmixing. Then, an abundance super-resolution neural network is trained using synthetic abundances, which are generated using the dead leaves model in such a way as to faithfully mimic real abundance statistics. Next, the spatial resolution of the considered hyperspectral image abundances is increased using this trained network, and the high resolution hyperspectral image is finally obtained by recombination with the endmembers. Experimental results show the training potential of the synthetic images, and demonstrate the method effectiveness."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T10:04:09Z",
        "published_parsed": [
            2026,
            1,
            23,
            10,
            4,
            9,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "arxiv_journal_ref": "2024 14th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS), Dec 2024, Helsinki, France. pp.1-5",
        "authors": [
            {
                "name": "Xinxin Xu"
            },
            {
                "name": "Yann Gousseau"
            },
            {
                "name": "Christophe Kervazo"
            },
            {
                "name": "Saïd Ladjal"
            }
        ],
        "author_detail": {
            "name": "Saïd Ladjal"
        },
        "arxiv_affiliation": "IMAGES, LTCI",
        "author": "Saïd Ladjal",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "使用全合成训练的无监督高光谱遥感图像超分辨率",
        "abstract_cn": "大量工作致力于高光谱单图像超分辨率，以提高高光谱图像的空间分辨率并充分发挥其潜力。然而，这些方法大多数都是有监督的，需要一些具有基本事实的数据进行训练，而这些数据通常是不可用的。为了克服这个问题，我们基于合成丰度数据的使用，提出了一种新的无监督训练策略，用于高光谱遥感图像的超分辨率。其第一步通过分解将高光谱图像分解为丰度和端元。然后，使用合成丰度来训练丰度超分辨率神经网络，合成丰度是使用死叶模型生成的，以忠实地模拟真实丰度统计数据。接下来，使用该训练网络提高所考虑的高光谱图像丰度的空间分辨率，并最终通过与端元重组获得高分辨率高光谱图像。实验结果显示了合成图像的训练潜力，并证明了该方法的有效性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16631v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16631v1",
        "title": "PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation"
        },
        "updated": "2026-01-23T10:33:15Z",
        "updated_parsed": [
            2026,
            1,
            23,
            10,
            33,
            15,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16631v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16631v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling. We design a multiscale Mamba backbone and a State Space Model (SSM)-based fusion network to enable efficient long-range perception in pyramid features, thereby extending the pure encoder-decoder framework while facilitating information sharing across multiscale features of nuclei. The proposed SSM-based feature-enhanced fusion integrates pyramid feature networks and dynamic feature enhancement across different spatial scales, enhancing the feature representation of densely overlapping nuclei in both semantic and spatial dimensions. To the best of our knowledge, this is the first Mamba-based approach for panoptic segmentation. Additionally, we introduce alternative evaluation metrics, including image-level Panoptic Quality ($i$PQ), boundary-weighted PQ ($w$PQ), and frequency-weighted PQ ($fw$PQ), which are specifically designed to address the unique challenges of nuclei segmentation and thereby mitigate the potential bias inherent in vanilla PQ. Experimental evaluations on two multiclass nuclei segmentation benchmark datasets, MoNuSAC2020 and NuInsSeg, demonstrate the superiority of PanopMamba for nuclei panoptic segmentation over state-of-the-art methods. Consequently, the robustness of PanopMamba is validated across various metrics, while the distinctiveness of PQ variants is also demonstrated. Code is available at https://github.com/mkang315/PanopMamba.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling. We design a multiscale Mamba backbone and a State Space Model (SSM)-based fusion network to enable efficient long-range perception in pyramid features, thereby extending the pure encoder-decoder framework while facilitating information sharing across multiscale features of nuclei. The proposed SSM-based feature-enhanced fusion integrates pyramid feature networks and dynamic feature enhancement across different spatial scales, enhancing the feature representation of densely overlapping nuclei in both semantic and spatial dimensions. To the best of our knowledge, this is the first Mamba-based approach for panoptic segmentation. Additionally, we introduce alternative evaluation metrics, including image-level Panoptic Quality ($i$PQ), boundary-weighted PQ ($w$PQ), and frequency-weighted PQ ($fw$PQ), which are specifically designed to address the unique challenges of nuclei segmentation and thereby mitigate the potential bias inherent in vanilla PQ. Experimental evaluations on two multiclass nuclei segmentation benchmark datasets, MoNuSAC2020 and NuInsSeg, demonstrate the superiority of PanopMamba for nuclei panoptic segmentation over state-of-the-art methods. Consequently, the robustness of PanopMamba is validated across various metrics, while the distinctiveness of PQ variants is also demonstrated. Code is available at https://github.com/mkang315/PanopMamba."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "stat.AP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T10:33:15Z",
        "published_parsed": [
            2026,
            1,
            23,
            10,
            33,
            15,
            4,
            23,
            0
        ],
        "arxiv_comment": "10 pages, 3 figures",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Ming Kang"
            },
            {
                "name": "Fung Fung Ting"
            },
            {
                "name": "Raphaël C. -W. Phan"
            },
            {
                "name": "Zongyuan Ge"
            },
            {
                "name": "Chee-Ming Ting"
            }
        ],
        "author_detail": {
            "name": "Chee-Ming Ting"
        },
        "author": "Chee-Ming Ting",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "PanopMamba：核全景分割的视觉状态空间建模",
        "abstract_cn": "细胞核全景分割通过集成不同细胞类型的语义和实例分割来分析组织病理学图像中的整体组织结构和单个细胞核，从而支持癌症诊断。主要挑战包括检测小物体、处理模糊边界以及解决类不平衡问题。为了解决这些问题，我们提出了 PanopMamba，这是一种新颖的混合编码器-解码器架构，它通过状态空间建模将 Mamba 和 Transformer 与附加功能增强融合相集成。我们设计了多尺度 Mamba 主干和基于状态空间模型（SSM）的融合网络，以实现金字塔特征的高效远程感知，从而扩展纯编码器-解码器框架，同时促进跨核多尺度特征的信息共享。所提出的基于SSM的特征增强融合集成了金字塔特征网络和跨不同空间尺度的动态特征增强，增强了语义和空间维度上密集重叠核的特征表示。据我们所知，这是第一个基于 Mamba 的全景分割方法。此外，我们还引入了替代评估指标，包括图像级全景质量 ($i$PQ)、边界加权 PQ ($w$PQ) 和频率加权 PQ ($fw$PQ)，这些指标专门设计用于解决细胞核分割的独特挑战，从而减轻普通 PQ 固有的潜在偏差。对两个多类细胞核分割基准数据集 MoNuSAC2020 和 NuInsSeg 的实验评估证明了 PanopMamba 在细胞核全景分割方面优于最先进的方法。因此，PanopMamba 的稳健性在各种指标上得到了验证，同时也证明了 PQ 变体的独特性。代码可在 https://github.com/mkang315/PanopMamba 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16660v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16660v1",
        "title": "Fast, faithful and photorealistic diffusion-based image super-resolution with enhanced Flow Map models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fast, faithful and photorealistic diffusion-based image super-resolution with enhanced Flow Map models"
        },
        "updated": "2026-01-23T11:25:04Z",
        "updated_parsed": [
            2026,
            1,
            23,
            11,
            25,
            4,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16660v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16660v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Diffusion-based image super-resolution (SR) has recently attracted significant attention by leveraging the expressive power of large pre-trained text-to-image diffusion models (DMs). A central practical challenge is resolving the trade-off between reconstruction faithfulness and photorealism. To address inference efficiency, many recent works have explored knowledge distillation strategies specifically tailored to SR, enabling one-step diffusion-based approaches. However, these teacher-student formulations are inherently constrained by information compression, which can degrade perceptual cues such as lifelike textures and depth of field, even with high overall perceptual quality. In parallel, self-distillation DMs, known as Flow Map models, have emerged as a promising alternative for image generation tasks, enabling fast inference while preserving the expressivity and training stability of standard DMs. Building on these developments, we propose FlowMapSR, a novel diffusion-based framework for image super-resolution explicitly designed for efficient inference. Beyond adapting Flow Map models to SR, we introduce two complementary enhancements: (i) positive-negative prompting guidance, based on a generalization of classifier free-guidance paradigm to Flow Map models, and (ii) adversarial fine-tuning using Low-Rank Adaptation (LoRA). Among the considered Flow Map formulations (Eulerian, Lagrangian, and Shortcut), we find that the Shortcut variant consistently achieves the best performance when combined with these enhancements. Extensive experiments show that FlowMapSR achieves a better balance between reconstruction faithfulness and photorealism than recent state-of-the-art methods for both x4 and x8 upscaling, while maintaining competitive inference time. Notably, a single model is used for both upscaling factors, without any scale-specific conditioning or degradation-guided mechanisms.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Diffusion-based image super-resolution (SR) has recently attracted significant attention by leveraging the expressive power of large pre-trained text-to-image diffusion models (DMs). A central practical challenge is resolving the trade-off between reconstruction faithfulness and photorealism. To address inference efficiency, many recent works have explored knowledge distillation strategies specifically tailored to SR, enabling one-step diffusion-based approaches. However, these teacher-student formulations are inherently constrained by information compression, which can degrade perceptual cues such as lifelike textures and depth of field, even with high overall perceptual quality. In parallel, self-distillation DMs, known as Flow Map models, have emerged as a promising alternative for image generation tasks, enabling fast inference while preserving the expressivity and training stability of standard DMs. Building on these developments, we propose FlowMapSR, a novel diffusion-based framework for image super-resolution explicitly designed for efficient inference. Beyond adapting Flow Map models to SR, we introduce two complementary enhancements: (i) positive-negative prompting guidance, based on a generalization of classifier free-guidance paradigm to Flow Map models, and (ii) adversarial fine-tuning using Low-Rank Adaptation (LoRA). Among the considered Flow Map formulations (Eulerian, Lagrangian, and Shortcut), we find that the Shortcut variant consistently achieves the best performance when combined with these enhancements. Extensive experiments show that FlowMapSR achieves a better balance between reconstruction faithfulness and photorealism than recent state-of-the-art methods for both x4 and x8 upscaling, while maintaining competitive inference time. Notably, a single model is used for both upscaling factors, without any scale-specific conditioning or degradation-guided mechanisms."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T11:25:04Z",
        "published_parsed": [
            2026,
            1,
            23,
            11,
            25,
            4,
            4,
            23,
            0
        ],
        "arxiv_comment": "Technical report",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Maxence Noble"
            },
            {
                "name": "Gonzalo Iñaki Quintana"
            },
            {
                "name": "Benjamin Aubin"
            },
            {
                "name": "Clément Chadebec"
            }
        ],
        "author_detail": {
            "name": "Clément Chadebec"
        },
        "author": "Clément Chadebec",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "快速、忠实且逼真的基于扩散的图像超分辨率，具有增强的 Flow Map 模型",
        "abstract_cn": "基于扩散的图像超分辨率（SR）最近通过利用大型预训练文本到图像扩散模型（DM）的表达能力而引起了极大的关注。一个主要的实际挑战是解决重建忠实度和真实感之间的权衡。为了解决推理效率问题，最近的许多工作都探索了专门针对 SR 的知识蒸馏策略，从而实现了基于扩散的一步式方法。然而，这些师生表述本质上受到信息压缩的限制，即使整体感知质量很高，信息压缩也会降低感知线索，例如逼真的纹理和景深。与此同时，自蒸馏 DM（称为 Flow Map 模型）已成为图像生成任务的一种有前景的替代方案，可以实现快速推理，同时保留标准 DM 的表达能力和训练稳定性。基于这些进展，我们提出了 FlowMapSR，这是一种新型的基于扩散的图像超分辨率框架，专门为高效推理而设计。除了使 Flow Map 模型适应 SR 之外，我们还引入了两个互补的增强功能：（i）基于分类器自由指导范例对 Flow Map 模型的泛化的正负提示指导，以及（ii）使用低秩适应（LoRA）进行对抗性微调。在考虑的流图公式（欧拉、拉格朗日和快捷方式）中，我们发现快捷方式变体在与这些增强功能相结合时始终能实现最佳性能。大量实验表明，与最新的 x4 和 x8 放大方法相比，FlowMapSR 在重建忠实度和真实感之间实现了更好的平衡，同时保持了有竞争力的推理时间。值得注意的是，单个模型用于两个放大因子，没有任何特定于尺度的调节或降级引导机制。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16664v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16664v1",
        "title": "OFDM-Based ISAC Imaging of Extended Targets via Inverse Virtual Aperture Processing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "OFDM-Based ISAC Imaging of Extended Targets via Inverse Virtual Aperture Processing"
        },
        "updated": "2026-01-23T11:28:37Z",
        "updated_parsed": [
            2026,
            1,
            23,
            11,
            28,
            37,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16664v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16664v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This work investigates the performance of an integrated sensing and communication (ISAC) system exploiting inverse virtual aperture (IVA) for imaging moving extended targets in vehicular scenarios. A base station (BS) operates as a monostatic sensor using MIMO-OFDM waveforms. Echoes reflected by the target are processed through motion-compensation techniques to form an IVA range-Doppler (cross-range) image. A case study considers a 5G NR waveform in the upper mid-band, with the target model defined in 3GPP Release 19, representing a vehicle as a set of spatially distributed scatterers. Performance is evaluated in terms of image contrast (IC) and the root mean squared error (RMSE) of the estimated target-centroid range. Finally, the trade-off between sensing accuracy and communication efficiency is examined by varying the subcarrier allocation for IVA imaging. The results provide insights for designing effective sensing strategies in next-generation radio networks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This work investigates the performance of an integrated sensing and communication (ISAC) system exploiting inverse virtual aperture (IVA) for imaging moving extended targets in vehicular scenarios. A base station (BS) operates as a monostatic sensor using MIMO-OFDM waveforms. Echoes reflected by the target are processed through motion-compensation techniques to form an IVA range-Doppler (cross-range) image. A case study considers a 5G NR waveform in the upper mid-band, with the target model defined in 3GPP Release 19, representing a vehicle as a set of spatially distributed scatterers. Performance is evaluated in terms of image contrast (IC) and the root mean squared error (RMSE) of the estimated target-centroid range. Finally, the trade-off between sensing accuracy and communication efficiency is examined by varying the subcarrier allocation for IVA imaging. The results provide insights for designing effective sensing strategies in next-generation radio networks."
        },
        "tags": [
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T11:28:37Z",
        "published_parsed": [
            2026,
            1,
            23,
            11,
            28,
            37,
            4,
            23,
            0
        ],
        "arxiv_comment": "6 pages; This paper was presented at the IEEE JC&S Symposium 2026",
        "arxiv_primary_category": {
            "term": "eess.SP"
        },
        "authors": [
            {
                "name": "Michael Negosanti"
            },
            {
                "name": "Lorenzo Pucci"
            },
            {
                "name": "Andrea Giorgetti"
            }
        ],
        "author_detail": {
            "name": "Andrea Giorgetti"
        },
        "author": "Andrea Giorgetti",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "通过逆虚拟孔径处理对扩展目标进行基于 OFDM 的 ISAC 成像",
        "abstract_cn": "这项工作研究了集成传感和通信 (ISAC) 系统的性能，该系统利用逆虚拟孔径 (IVA) 对车辆场景中的移动扩展目标进行成像。基站 (BS) 作为使用 MIMO-OFDM 波形的单基地传感器运行。目标反射的回波通过运动补偿技术进行处理，形成 IVA 距离多普勒（跨距离）图像。案例研究考虑了中上频段的 5G NR 波形，其目标模型在 3GPP 第 19 版中定义，将车辆表示为一组空间分布的散射体。根据图像对比度 (IC) 和估计目标质心范围的均方根误差 (RMSE) 来评估性能。最后，通过改变 IVA 成像的子载波分配来检查传感精度和通信效率之间的权衡。研究结果为设计下一代无线电网络中的有效传感策略提供了见解。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16780v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16780v1",
        "title": "PocketDVDNet: Realtime Video Denoising for Real Camera Noise",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PocketDVDNet: Realtime Video Denoising for Real Camera Noise"
        },
        "updated": "2026-01-23T14:27:03Z",
        "updated_parsed": [
            2026,
            1,
            23,
            14,
            27,
            3,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16780v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16780v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Live video denoising under realistic, multi-component sensor noise remains challenging for applications such as autofocus, autonomous driving, and surveillance. We propose PocketDVDNet, a lightweight video denoiser developed using our model compression framework that combines sparsity-guided structured pruning, a physics-informed noise model, and knowledge distillation to achieve high-quality restoration with reduced resource demands. Starting from a reference model, we induce sparsity, apply targeted channel pruning, and retrain a teacher on realistic multi-component noise. The student network learns implicit noise handling, eliminating the need for explicit noise-map inputs. PocketDVDNet reduces the original model size by 74% while improving denoising quality and processing 5-frame patches in real-time. These results demonstrate that aggressive compression, combined with domain-adapted distillation, can reconcile performance and efficiency for practical, real-time video denoising.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Live video denoising under realistic, multi-component sensor noise remains challenging for applications such as autofocus, autonomous driving, and surveillance. We propose PocketDVDNet, a lightweight video denoiser developed using our model compression framework that combines sparsity-guided structured pruning, a physics-informed noise model, and knowledge distillation to achieve high-quality restoration with reduced resource demands. Starting from a reference model, we induce sparsity, apply targeted channel pruning, and retrain a teacher on realistic multi-component noise. The student network learns implicit noise handling, eliminating the need for explicit noise-map inputs. PocketDVDNet reduces the original model size by 74% while improving denoising quality and processing 5-frame patches in real-time. These results demonstrate that aggressive compression, combined with domain-adapted distillation, can reconcile performance and efficiency for practical, real-time video denoising."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T14:27:03Z",
        "published_parsed": [
            2026,
            1,
            23,
            14,
            27,
            3,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Crispian Morris"
            },
            {
                "name": "Imogen Dexter"
            },
            {
                "name": "Fan Zhang"
            },
            {
                "name": "David R. Bull"
            },
            {
                "name": "Nantheera Anantrasirichai"
            }
        ],
        "author_detail": {
            "name": "Nantheera Anantrasirichai"
        },
        "author": "Nantheera Anantrasirichai",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "PocketDVDNet：针对真实相机噪声的实时视频降噪",
        "abstract_cn": "对于自动对焦、自动​​驾驶和监控等应用来说，真实的多分量传感器噪声下的实时视频去噪仍然具有挑战性。我们提出了 PocketDVDNet，这是一种使用我们的模型压缩框架开发的轻量级视频降噪器，该框架结合了稀疏引导的结构化修剪、物理信息噪声模型和知识蒸馏，以在减少资源需求的情况下实现高质量恢复。从参考模型开始，我们引入稀疏性，应用有针对性的通道修剪，并对教师进行现实多分量噪声的重新培训。学生网络学习隐式噪声处理，无需显式噪声图输入。 PocketDVDNet 将原始模型大小减少了 74%，同时提高了去噪质量并实时处理 5 帧补丁。这些结果表明，积极的压缩与域适应的蒸馏相结合，可以协调实用的实时视频去噪的性能和效率。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16812v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16812v1",
        "title": "Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing"
        },
        "updated": "2026-01-23T15:06:02Z",
        "updated_parsed": [
            2026,
            1,
            23,
            15,
            6,
            2,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16812v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16812v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice."
        },
        "tags": [
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "math.OC",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T15:06:02Z",
        "published_parsed": [
            2026,
            1,
            23,
            15,
            6,
            2,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.LG"
        },
        "authors": [
            {
                "name": "Francesca Lanzillotta"
            },
            {
                "name": "Chiara Albisani"
            },
            {
                "name": "Davide Pucci"
            },
            {
                "name": "Daniele Baracchi"
            },
            {
                "name": "Alessandro Piva"
            },
            {
                "name": "Matteo Lapucci"
            }
        ],
        "author_detail": {
            "name": "Matteo Lapucci"
        },
        "author": "Matteo Lapucci",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "通过顺序惩罚方法进行样本约束学习及其在图像处理中的应用",
        "abstract_cn": "在许多学习任务中，对单个数据样本处理的某些要求可以说应该被形式化为底层优化问题的严格约束，而不是通过任意惩罚的方式。我们表明，在这些场景中，可以利用允许正确处理约束的顺序惩罚方法来进行学习。所提出的算法在深度学习场景中合理的假设下具有收敛保证。此外，图像处理任务的实验结果表明该方法在实际中确实可行。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16950v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16950v1",
        "title": "Evaluating Wi-Fi Performance for VR Streaming: A Study on Realistic HEVC Video Traffic",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Evaluating Wi-Fi Performance for VR Streaming: A Study on Realistic HEVC Video Traffic"
        },
        "updated": "2026-01-23T18:15:25Z",
        "updated_parsed": [
            2026,
            1,
            23,
            18,
            15,
            25,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16950v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16950v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Cloud-based Virtual Reality (VR) streaming presents significant challenges for 802.11 networks due to its high throughput and low latency requirements. When multiple VR users share a Wi-Fi network, the resulting uplink and downlink traffic can quickly saturate the channel. This paper investigates the capacity of 802.11 networks for supporting realistic VR streaming workloads across varying frame rates, bitrates, codec settings, and numbers of users. We develop an emulation framework that reproduces Air Light VR (ALVR) operation, where real HEVC video traffic is fed into an 802.11 simulation model. Our findings explore Wi-Fi's performance anomaly and demonstrate that Intra-refresh (IR) coding effectively reduces latency variability and improves QoS, supporting up to 4 concurrent VR users with Constant Bitrate (CBR) 100 Mbps before the channel is saturated.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Cloud-based Virtual Reality (VR) streaming presents significant challenges for 802.11 networks due to its high throughput and low latency requirements. When multiple VR users share a Wi-Fi network, the resulting uplink and downlink traffic can quickly saturate the channel. This paper investigates the capacity of 802.11 networks for supporting realistic VR streaming workloads across varying frame rates, bitrates, codec settings, and numbers of users. We develop an emulation framework that reproduces Air Light VR (ALVR) operation, where real HEVC video traffic is fed into an 802.11 simulation model. Our findings explore Wi-Fi's performance anomaly and demonstrate that Intra-refresh (IR) coding effectively reduces latency variability and improves QoS, supporting up to 4 concurrent VR users with Constant Bitrate (CBR) 100 Mbps before the channel is saturated."
        },
        "tags": [
            {
                "term": "cs.NI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.MM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T18:15:25Z",
        "published_parsed": [
            2026,
            1,
            23,
            18,
            15,
            25,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.NI"
        },
        "authors": [
            {
                "name": "Ferran Maura"
            },
            {
                "name": "Francesc Wilhelmi"
            },
            {
                "name": "Boris Bellalta"
            }
        ],
        "author_detail": {
            "name": "Boris Bellalta"
        },
        "author": "Boris Bellalta",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "评估 VR 流媒体的 Wi-Fi 性能：真实 HEVC 视频流量的研究",
        "abstract_cn": "基于云的虚拟现实 (VR) 流媒体由于其高吞吐量和低延迟要求，给 802.11 网络带来了重大挑战。当多个VR用户共享Wi-Fi网络时，产生的上下行流量会很快使信道饱和。本文研究了 802.11 网络在不同帧速率、比特率、编解码器设置和用户数量上支持真实 VR 流媒体工作负载的能力。我们开发了一个模拟框架，可以重现 Air Light VR (ALVR) 操作，其中真实的 HEVC 视频流量被输入到 802.11 模拟模型中。我们的研究结果探讨了 Wi-Fi 的性能异常，并证明刷新内 (IR) 编码可有效降低延迟变化并提高 QoS，在通道饱和之前以 100 Mbps 的恒定比特率 (CBR) 支持最多 4 个并发 VR 用户。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16272v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16272v1",
        "title": "GR3EN: Generative Relighting for 3D Environments",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "GR3EN: Generative Relighting for 3D Environments"
        },
        "updated": "2026-01-22T19:10:05Z",
        "updated_parsed": [
            2026,
            1,
            22,
            19,
            10,
            5,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16272v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16272v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T19:10:05Z",
        "published_parsed": [
            2026,
            1,
            22,
            19,
            10,
            5,
            3,
            22,
            0
        ],
        "arxiv_comment": "project page: https://gr3en-relight.github.io/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xiaoyan Xing"
            },
            {
                "name": "Philipp Henzler"
            },
            {
                "name": "Junhwa Hur"
            },
            {
                "name": "Runze Li"
            },
            {
                "name": "Jonathan T. Barron"
            },
            {
                "name": "Pratul P. Srinivasan"
            },
            {
                "name": "Dor Verbin"
            }
        ],
        "author_detail": {
            "name": "Dor Verbin"
        },
        "author": "Dor Verbin",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "GR3EN：3D 环境的生成式重新照明",
        "abstract_cn": "我们提出了一种重新照亮大型房间规模环境 3D 重建的方法。现有的 3D 场景重新照明解决方案通常需要解决不确定或病态的逆渲染问题，因此无法在复杂的现实场景上产生高质量的结果。尽管最近在使用生成图像和视频扩散模型进行重新照明方面取得了很大的进展，但这些技术要么仅限于 2D 图像和视频重新照明，要么仅限于单个对象的 3D 重新照明。我们的方法通过将视频到视频重新照明扩散模型的输出提炼为 3D 重建，实现房间规模场景的可控 3D 重新照明。这回避了解决困难的逆渲染问题的需要，并产生了一个灵活的系统，可以重新照亮复杂现实世界场景的 3D 重建。我们在合成数据集和真实数据集上验证了我们的方法，以表明它可以在新的照明条件下忠实地渲染场景的新颖视图。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16296v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16296v1",
        "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory"
        },
        "updated": "2026-01-22T19:59:17Z",
        "updated_parsed": [
            2026,
            1,
            22,
            19,
            59,
            17,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16296v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16296v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T19:59:17Z",
        "published_parsed": [
            2026,
            1,
            22,
            19,
            59,
            17,
            3,
            22,
            0
        ],
        "arxiv_comment": "Project page: https://dohunlee1.github.io/MemoryV2V",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Dohun Lee"
            },
            {
                "name": "Chun-Hao Paul Huang"
            },
            {
                "name": "Xuelin Chen"
            },
            {
                "name": "Jong Chul Ye"
            },
            {
                "name": "Duygu Ceylan"
            },
            {
                "name": "Hyeonho Jeong"
            }
        ],
        "author_detail": {
            "name": "Hyeonho Jeong"
        },
        "author": "Hyeonho Jeong",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Memory-V2V：用内存增强视频到视频的扩散模型",
        "abstract_cn": "最近的基础视频到视频扩散模型在通过修改外观、运动或相机运动来编辑用户提供的视频方面取得了令人印象深刻的结果。然而，现实世界的视频编辑通常是一个迭代过程，用户通过多轮交互来完善结果。在这种多轮设置中，当前的视频编辑器很难在顺序编辑之间保持交叉一致性。在这项工作中，我们首次解决了多轮视频编辑中的交叉一致性问题，并引入了 Memory-V2V，这是一个简单而有效的框架，可以通过显式内存增强现有的视频到视频模型。给定先前编辑的视频的外部缓存，Memory-V2V 采用准确的检索和动态标记化策略来根据先前的结果调整当前的编辑步骤。为了进一步减少冗余和计算开销，我们在 DiT 主干中提出了一个可学习的令牌压缩器，它可以压缩冗余的条件令牌，同时保留基本的视觉线索，从而实现 30% 的整体加速。我们在具有挑战性的任务上验证 Memory-V2V，包括视频小说视图合成和文本调节的长视频编辑。大量实验表明，Memory-V2V 生成的视频在最小计算开销的情况下显着提高了交叉一致性，同时在最先进的基线上保持甚至提高了特定任务的性能。项目页面：https://dohunlee1.github.io/MemoryV2V"
    },
    {
        "id": "http://arxiv.org/abs/2601.16302v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16302v1",
        "title": "FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging"
        },
        "updated": "2026-01-22T20:14:45Z",
        "updated_parsed": [
            2026,
            1,
            22,
            20,
            14,
            45,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16302v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16302v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T20:14:45Z",
        "published_parsed": [
            2026,
            1,
            22,
            20,
            14,
            45,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Abhijeet Parida"
            },
            {
                "name": "Antonia Alomar"
            },
            {
                "name": "Zhifan Jiang"
            },
            {
                "name": "Pooneh Roshanitabrizi"
            },
            {
                "name": "Austin Tapp"
            },
            {
                "name": "Ziyue Xu"
            },
            {
                "name": "Syed Muhammad Anwar"
            },
            {
                "name": "Maria J. Ledesma-Carbayo"
            },
            {
                "name": "Holger R. Roth"
            },
            {
                "name": "Marius George Linguraru"
            }
        ],
        "author_detail": {
            "name": "Marius George Linguraru"
        },
        "author": "Marius George Linguraru",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "FeTTL：多机构医学成像的联合模板和任务学习",
        "abstract_cn": "联邦学习可以跨地理分布的医疗中心进行协作模型训练，同时保护数据隐私。然而，数据的域转移和异质性通常会导致模型性能下降。医学成像应用尤其受到采集协议、扫描仪类型和患者群体变化的影响。为了解决这些问题，我们引入了联合模板和任务学习（FeTTL），这是一种新颖的框架，旨在协调联合环境中的多机构医学成像数据。 FeTTL 学习全局模板和任务模型，以协调客户端之间的数据分布。我们在两项具有挑战性且多样化的多机构医学成像任务上评估了 FeTTL：视网膜眼底光盘分割和组织病理学转移分类。实验结果表明，在光盘分割和多机构数据转移分类方面，FeTTL 显着优于最先进的联邦学习基线（p 值 <0.002）。我们的实验进一步强调了共同学习模板和任务的重要性。这些发现表明，FeTTL 提供了一种有原则且可扩展的解决方案，用于减轻联邦学习中的分布变化，支持现实世界、多机构环境中的稳健模型部署。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16348v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16348v1",
        "title": "Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures"
        },
        "updated": "2026-01-22T22:19:02Z",
        "updated_parsed": [
            2026,
            1,
            22,
            22,
            19,
            2,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16348v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16348v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T22:19:02Z",
        "published_parsed": [
            2026,
            1,
            22,
            22,
            19,
            2,
            3,
            22,
            0
        ],
        "arxiv_comment": "Preprint, submitted for review",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Aline Sindel"
            },
            {
                "name": "Andreas Maier"
            },
            {
                "name": "Vincent Christlein"
            }
        ],
        "author_detail": {
            "name": "Vincent Christlein"
        },
        "author": "Vincent Christlein",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "基于裂纹结构的历史面板画从粗到细的非刚性多模态图像配准",
        "abstract_cn": "历史面板画的艺术技术研究依赖于获取多模态图像数据，包括可见光摄影、红外反射摄影、紫外荧光摄影、X射线摄影和微距摄影。为了进行全面分析，多模态图像需要逐像素对齐，这仍然经常手动执行。多模态图像配准可以减少这种繁重的手动工作，速度大大加快，并且精度更高。由于图像分辨率不同、图像尺寸巨大、非刚性扭曲以及依赖于模态的图像内容，配准具有挑战性。因此，我们提出了一种有效依赖稀疏关键点和薄板样条的从粗到细的非刚性多模态配准方法。历史绘画在油漆层上呈现出细小的裂纹图案，称为裂纹，所有图像系统都会捕获该图案，非常适合作为配准特征。在我们的一阶段非刚性配准方法中，我们采用卷积神经网络基于裂纹进行联合关键点检测和描述，并采用图神经网络以基于补丁的方式进行描述符匹配，并基于局部区域的单应性重投影误差进行过滤匹配。对于从粗到细的配准，我们引入了一种新颖的多级关键点细化方法，将混合分辨率图像配准到最高分辨率。我们创建了一个具有大量关键点注释的面板绘画的多模态数据集，以及一个包含五个多模态域和不同图像分辨率的大型测试集。消融研究证明了我们的细化方法所有模块的有效性。与竞争关键点、密集匹配方法和细化方法相比，我们提出的方法实现了最佳配准结果。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16381v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16381v1",
        "title": "VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection"
        },
        "updated": "2026-01-23T00:30:24Z",
        "updated_parsed": [
            2026,
            1,
            23,
            0,
            30,
            24,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16381v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16381v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T00:30:24Z",
        "published_parsed": [
            2026,
            1,
            23,
            0,
            30,
            24,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yuxin Jiang"
            },
            {
                "name": "Yunkang Cao"
            },
            {
                "name": "Yuqi Cheng"
            },
            {
                "name": "Yiheng Zhang"
            },
            {
                "name": "Weiming Shen"
            }
        ],
        "author_detail": {
            "name": "Weiming Shen"
        },
        "author": "Weiming Shen",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "VTFusion：用于少样本异常检测的视觉文本多模态融合网络",
        "abstract_cn": "少样本异常检测 (FSAD) 已成为使用稀缺正常参考识别异常情况的关键范例。虽然最近的方法集成了文本语义来补充视觉数据，但它们主要依赖于在自然场景上预先训练的特征，从而忽略了工业检查所必需的细粒度、特定领域的语义。此外，流行的融合策略通常诉诸于表面连接，无法解决视觉和文本模态之间固有的语义不一致问题，从而损害了针对跨模态干扰的鲁棒性。为了弥补这些差距，本研究提出了 VTFusion，这是一种专为 FSAD 量身定制的视觉文本多模态融合框架。该框架基于两个核心设计。首先，引入图像和文本模态的自适应特征提取器来学习特定于任务的表示，从而弥合预训练模型和工业数据之间的领域差距；通过生成不同的合成异常来增强特征辨别力，这一点得到了进一步增强。其次，开发了专用的多模态预测融合模块，包括促进丰富的跨模态信息交换的融合块和在多模态指导下生成精细的像素级异常图的分割网络。 VTFusion 显着提高了 FSAD 性能，在 MVTec AD 和 VisA 数据集上的 2-shot 场景中分别实现了 96.8% 和 86.2% 的图像级 AUROC。此外，VTFusion 在本文介绍的工业汽车塑料零部件的真实数据集上实现了 93.5% 的 AUPRO，进一步证明了其在苛刻工业场景中的实际适用性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16413v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16413v1",
        "title": "A Cosine Network for Image Super-Resolution",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Cosine Network for Image Super-Resolution"
        },
        "updated": "2026-01-23T02:58:57Z",
        "updated_parsed": [
            2026,
            1,
            23,
            2,
            58,
            57,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16413v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16413v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1109/TIP.2025.3645630",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T02:58:57Z",
        "published_parsed": [
            2026,
            1,
            23,
            2,
            58,
            57,
            4,
            23,
            0
        ],
        "arxiv_comment": "in IEEE Transactions on Image Processing (2025)",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chunwei Tian"
            },
            {
                "name": "Chengyuan Zhang"
            },
            {
                "name": "Bob Zhang"
            },
            {
                "name": "Zhiwu Li"
            },
            {
                "name": "C. L. Philip Chen"
            },
            {
                "name": "David Zhang"
            }
        ],
        "author_detail": {
            "name": "David Zhang"
        },
        "author": "David Zhang",
        "arxiv_doi": "10.1109/TIP.2025.3645630",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于图像超分辨率的余弦网络",
        "abstract_cn": "深度卷积神经网络可以利用层次信息逐步提取结构信息来恢复高质量图像。然而，保持所获得的结构信息的有效性在图像超分辨率中非常重要。在本文中，我们通过改进网络架构和优化训练策略，提出了一种用于图像超分辨率的余弦网络（CSRNet）。为了提取互补的同源结构信息，设计了奇数和偶数异质块以放大结构差异并提高图像超分辨率的性能。将线性和非线性结构信息结合起来可以克服同源信息的缺点，增强所获得的结构信息在图像超分辨率中的鲁棒性。考虑到梯度下降的局部最小值，余弦退火机制用于通过执行热重启和调整学习率来优化训练过程。实验结果表明，所提出的 CSRNet 在图像超分辨率方面与最先进的方法具有竞争力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16419v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16419v1",
        "title": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning"
        },
        "updated": "2026-01-23T03:10:08Z",
        "updated_parsed": [
            2026,
            1,
            23,
            3,
            10,
            8,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16419v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16419v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities in multimodal perception and understanding tasks. However, their effectiveness in specialized domains, such as remote sensing and medical imaging, remains limited. A natural approach to domain adaptation is to inject domain knowledge through textual instructions, prompts, or auxiliary captions. Surprisingly, we find that such input-level domain knowledge injection yields little to no improvement on scientific multimodal tasks, even when the domain knowledge is explicitly provided. This observation suggests that current MLLMs fail to internalize domain-specific priors through language alone, and that domain knowledge must be integrated at the optimization level. Motivated by this insight, we propose a reinforcement fine-tuning framework that incorporates domain knowledge directly into the learning objective. Instead of treating domain knowledge as descriptive information, we encode it as domain-informed constraints and reward signals, shaping the model's behavior in the output space. Extensive experiments across multiple datasets in remote sensing and medical domains consistently demonstrate good performance gains, achieving state-of-the-art results on multimodal domain tasks. Our results highlight the necessity of optimization-level domain knowledge integration and reveal a fundamental limitation of textual domain conditioning in current MLLMs.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multimodal large language models (MLLMs) have shown remarkable capabilities in multimodal perception and understanding tasks. However, their effectiveness in specialized domains, such as remote sensing and medical imaging, remains limited. A natural approach to domain adaptation is to inject domain knowledge through textual instructions, prompts, or auxiliary captions. Surprisingly, we find that such input-level domain knowledge injection yields little to no improvement on scientific multimodal tasks, even when the domain knowledge is explicitly provided. This observation suggests that current MLLMs fail to internalize domain-specific priors through language alone, and that domain knowledge must be integrated at the optimization level. Motivated by this insight, we propose a reinforcement fine-tuning framework that incorporates domain knowledge directly into the learning objective. Instead of treating domain knowledge as descriptive information, we encode it as domain-informed constraints and reward signals, shaping the model's behavior in the output space. Extensive experiments across multiple datasets in remote sensing and medical domains consistently demonstrate good performance gains, achieving state-of-the-art results on multimodal domain tasks. Our results highlight the necessity of optimization-level domain knowledge integration and reveal a fundamental limitation of textual domain conditioning in current MLLMs."
        },
        "tags": [
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T03:10:08Z",
        "published_parsed": [
            2026,
            1,
            23,
            3,
            10,
            8,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CL"
        },
        "authors": [
            {
                "name": "Qinglong Cao"
            },
            {
                "name": "Yuntian Chen"
            },
            {
                "name": "Chao Ma"
            },
            {
                "name": "Xiaokang Yang"
            }
        ],
        "author_detail": {
            "name": "Xiaokang Yang"
        },
        "author": "Xiaokang Yang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过强化微调学习多模态大语言模型中的领域知识",
        "abstract_cn": "多模态大语言模型（MLLM）在多模态感知和理解任务中表现出了卓越的能力。然而，它们在遥感和医学成像等专业领域的有效性仍然有限。领域适应的一种自然方法是通过文本指令、提示或辅助标题注入领域知识。令人惊讶的是，我们发现这种输入级领域知识注入对科学多模态任务几乎没有任何改进，即使明确提供了领域知识。这一观察结果表明，当前的 MLLM 无法仅通过语言内化特定于领域的先验，并且必须在优化级别集成领域知识。受这种洞察力的启发，我们提出了一个强化微调框架，将领域知识直接纳入学习目标。我们没有将领域知识视为描述性信息，而是将其编码为领域知情的约束和奖励信号，从而塑造模型在输出空间中的行为。在遥感和医学领域的多个数据集上进行的广泛实验一致证明了良好的性能增益，在多模态域任务上取得了最先进的结果。我们的结果强调了优化级领域知识集成的必要性，并揭示了当前 MLLM 中文本域调节的基本局限性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16429v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16429v1",
        "title": "AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose"
        },
        "updated": "2026-01-23T04:01:49Z",
        "updated_parsed": [
            2026,
            1,
            23,
            4,
            1,
            49,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16429v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16429v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git'.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git'."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T04:01:49Z",
        "published_parsed": [
            2026,
            1,
            23,
            4,
            1,
            49,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jongmin Yu"
            },
            {
                "name": "Hyeontaek Oh"
            },
            {
                "name": "Zhongtian Sun"
            },
            {
                "name": "Angelica I Aviles-Rivero"
            },
            {
                "name": "Moongu Jeon"
            },
            {
                "name": "Jinhong Yang"
            }
        ],
        "author_detail": {
            "name": "Jinhong Yang"
        },
        "author": "Jinhong Yang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "AlphaFace：高保真、实时换脸器，对面部姿势具有鲁棒性",
        "abstract_cn": "现有的换脸方法通常在有限的环境中提供有竞争力的结果，但在处理极端的面部姿势时却表现出严重的质量下降。为了提高面部姿势的鲁棒性，应用了显式几何特征，但这种方法仍然存在问题，因为它引入了额外的依赖性并增加了计算成本。基于扩散的方法取得了显着的效果；然而，它们对于实时处理来说是不切实际的。我们介绍 AlphaFace，它利用开源视觉语言模型以及 CLIP 图像和文本嵌入来应用新颖的视觉和文本语义对比损失。 AlphaFace 能够实现更强大的身份表示和更精确的属性保存，同时保持实时性能。 FF++、MPIE 和 LPFF 的综合实验表明，AlphaFace 在姿势挑战的情况下超越了最先进的方法。该项目已在“https://github.com/andrewyu90/Alphaface_Official.git”上公开发布。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16451v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16451v1",
        "title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology"
        },
        "updated": "2026-01-23T05:06:57Z",
        "updated_parsed": [
            2026,
            1,
            23,
            5,
            6,
            57,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16451v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16451v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T05:06:57Z",
        "published_parsed": [
            2026,
            1,
            23,
            5,
            6,
            57,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Peixian Liang"
            },
            {
                "name": "Songhao Li"
            },
            {
                "name": "Shunsuke Koga"
            },
            {
                "name": "Yutong Li"
            },
            {
                "name": "Zahra Alipour"
            },
            {
                "name": "Yucheng Tang"
            },
            {
                "name": "Daguang Xu"
            },
            {
                "name": "Zhi Huang"
            }
        ],
        "author_detail": {
            "name": "Zhi Huang"
        },
        "author": "Zhi Huang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "VISTA-PATH：计算病理学中病理图像分割和定量分析的交互式基础模型",
        "abstract_cn": "组织病理学图像的准确语义分割对于定量组织分析和下游临床建模至关重要。最近的分割基础模型通过大规模预训练提高了泛化能力，但与病理学的一致性仍然较差，因为它们将分割视为静态视觉预测任务。在这里，我们介绍 VISTA-PATH，这是一种交互式、类感知病理学分割基础模型，旨在解决异质结构、纳入专家反馈并生成对临床解释有直接意义的像素级分割。 VISTA-PATH 联合对视觉上下文、语义组织描述和可选的专家提供的空间提示进行分割，从而实现跨异构病理图像的精确多类分割。为了支持这一范式，我们策划了 VISTA-PATH 数据，这是一个大规模病理分割语料库，包含超过 160 万个图像-掩模-文本三元组，涵盖 9 个器官和 93 个组织类别。在广泛的保留和外部基准测试中，VISTA-PATH 始终优于现有的细分基础模型。重要的是，VISTA-PATH 通过将稀疏的、补丁级边界框注释反馈传播到整个幻灯片分割中来支持动态人机交互细化。最后，我们表明 VISTA-PATH 产生的高保真、类别感知分割是计算病理学的首选模型。它通过提出的肿瘤相互作用评分 (TIS) 改善组织微环境分析，该评分与患者生存具有强烈且显着的关联。这些结果共同确立了 VISTA-PATH 作为基础模型，将病理图像分割从静态预测提升为数字病理学的交互式且基于临床的表示。源代码和demo可以在https://github.com/zhihuanglab/VISTA-PATH找到。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16473v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16473v1",
        "title": "DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses"
        },
        "updated": "2026-01-23T06:04:43Z",
        "updated_parsed": [
            2026,
            1,
            23,
            6,
            4,
            43,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16473v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16473v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The rapid proliferation of realistic deepfakes has raised urgent concerns over their misuse, motivating the use of defensive watermarks in synthetic images for reliable detection and provenance tracking. However, this defense paradigm assumes such watermarks are inherently resistant to removal. We challenge this assumption with DeMark, a query-free black-box attack framework that targets defensive image watermarking schemes for deepfakes. DeMark exploits latent-space vulnerabilities in encoder-decoder watermarking models through a compressive sensing based sparsification process, suppressing watermark signals while preserving perceptual and structural realism appropriate for deepfakes. Across eight state-of-the-art watermarking schemes, DeMark reduces watermark detection accuracy from 100% to 32.9% on average while maintaining natural visual quality, outperforming existing attacks. We further evaluate three defense strategies, including image super resolution, sparse watermarking, and adversarial training, and find them largely ineffective. These results demonstrate that current encoder decoder watermarking schemes remain vulnerable to latent-space manipulations, underscoring the need for more robust watermarking methods to safeguard against deepfakes.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The rapid proliferation of realistic deepfakes has raised urgent concerns over their misuse, motivating the use of defensive watermarks in synthetic images for reliable detection and provenance tracking. However, this defense paradigm assumes such watermarks are inherently resistant to removal. We challenge this assumption with DeMark, a query-free black-box attack framework that targets defensive image watermarking schemes for deepfakes. DeMark exploits latent-space vulnerabilities in encoder-decoder watermarking models through a compressive sensing based sparsification process, suppressing watermark signals while preserving perceptual and structural realism appropriate for deepfakes. Across eight state-of-the-art watermarking schemes, DeMark reduces watermark detection accuracy from 100% to 32.9% on average while maintaining natural visual quality, outperforming existing attacks. We further evaluate three defense strategies, including image super resolution, sparse watermarking, and adversarial training, and find them largely ineffective. These results demonstrate that current encoder decoder watermarking schemes remain vulnerable to latent-space manipulations, underscoring the need for more robust watermarking methods to safeguard against deepfakes."
        },
        "tags": [
            {
                "term": "cs.CR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T06:04:43Z",
        "published_parsed": [
            2026,
            1,
            23,
            6,
            4,
            43,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CR"
        },
        "authors": [
            {
                "name": "Wei Song"
            },
            {
                "name": "Zhenchang Xing"
            },
            {
                "name": "Liming Zhu"
            },
            {
                "name": "Yulei Sui"
            },
            {
                "name": "Jingling Xue"
            }
        ],
        "author_detail": {
            "name": "Jingling Xue"
        },
        "author": "Jingling Xue",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DeMark：针对 Deepfake 水印防御的免查询黑盒攻击",
        "abstract_cn": "逼真的深度赝品的快速扩散引发了对其滥用的紧迫担忧，促使人们在合成图像中使用防御性水印以进行可靠的检测和来源跟踪。然而，这种防御范式假设此类水印本质上难以去除。我们用 DeMark 来挑战这个假设，DeMark 是一个免查询的黑盒攻击框架，针对深度伪造品的防御性图像水印方案。 DeMark 通过基于压缩感知的稀疏化过程，利用编码器-解码器水印模型中的潜在空间漏洞，抑制水印信号，同时保留适合深度伪造的感知和结构真实性。在八种最先进的水印方案中，DeMark 将水印检测准确率平均从 100% 降低到 32.9%，同时保持自然的视觉质量，优于现有的攻击。我们进一步评估了三种防御策略，包括图像超分辨率、稀疏水印和对抗性训练，发现它们基本上无效。这些结果表明，当前的编码器解码器水印方案仍然容易受到潜在空间操纵的影响，这强调了需要更强大的水印方法来防范深度伪造。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16487v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16487v1",
        "title": "Multi-View Consistent Wound Segmentation With Neural Fields",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multi-View Consistent Wound Segmentation With Neural Fields"
        },
        "updated": "2026-01-23T06:28:56Z",
        "updated_parsed": [
            2026,
            1,
            23,
            6,
            28,
            56,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16487v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16487v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide. In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms. In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images. Some approaches have extended segmentation to 3D, enabling more complete and precise healing progress tracking. However, inferring multi-view consistent 3D structures from 2D images remains a challenge. In this paper, we evaluate WoundNeRF, a NeRF SDF-based method for estimating robust wound segmentations from automatically generated annotations. We demonstrate the potential of this paradigm in recovering accurate segmentations by comparing it against state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms. The code will be released to facilitate further development in this promising paradigm.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide. In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms. In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images. Some approaches have extended segmentation to 3D, enabling more complete and precise healing progress tracking. However, inferring multi-view consistent 3D structures from 2D images remains a challenge. In this paper, we evaluate WoundNeRF, a NeRF SDF-based method for estimating robust wound segmentations from automatically generated annotations. We demonstrate the potential of this paradigm in recovering accurate segmentations by comparing it against state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms. The code will be released to facilitate further development in this promising paradigm."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T06:28:56Z",
        "published_parsed": [
            2026,
            1,
            23,
            6,
            28,
            56,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Remi Chierchia"
            },
            {
                "name": "Léo Lebrat"
            },
            {
                "name": "David Ahmedt-Aristizabal"
            },
            {
                "name": "Yulia Arzhaeva"
            },
            {
                "name": "Olivier Salvado"
            },
            {
                "name": "Clinton Fookes"
            },
            {
                "name": "Rodrigo Santa Cruz"
            }
        ],
        "author_detail": {
            "name": "Rodrigo Santa Cruz"
        },
        "author": "Rodrigo Santa Cruz",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用神经场进行多视图一致伤口分割",
        "abstract_cn": "伤口护理经常面临经济和后勤负担的挑战，这些负担一直困扰着世界各地的患者和医院。近几十年来，医疗保健专业人员一直在寻求计算机视觉和机器学习算法的支持。特别是，伤口分割因其能够为专业人员提供标准 RGB 图像的快速、自动组织评估而引起了人们的兴趣。一些方法已将分割扩展到 3D，从而实现更完整、更精确的愈合进度跟踪。然而，从 2D 图像推断多视图一致的 3D 结构仍然是一个挑战。在本文中，我们评估了 WoundNeRF，这是一种基于 NeRF SDF 的方法，用于根据自动生成的注释估计稳健的伤口分割。我们通过将其与最先进的 Vision Transformer 网络和传统的基于光栅化的算法进行比较，展示了这种范例在恢复准确分割方面的潜力。该代码将被发布，以促进这一有希望的范例的进一步开发。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16532v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16532v1",
        "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding"
        },
        "updated": "2026-01-23T08:08:12Z",
        "updated_parsed": [
            2026,
            1,
            23,
            8,
            8,
            12,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16532v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16532v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T08:08:12Z",
        "published_parsed": [
            2026,
            1,
            23,
            8,
            8,
            12,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Runmao Yao"
            },
            {
                "name": "Junsheng Zhou"
            },
            {
                "name": "Zhen Dong"
            },
            {
                "name": "Yu-Shen Liu"
            }
        ],
        "author_detail": {
            "name": "Yu-Shen Liu"
        },
        "author": "Yu-Shen Liu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "AnchoredDream：通过几何接地从单一视图生成零镜头 360° 室内场景",
        "abstract_cn": "单视图室内场景生成在一系列现实应用中发挥着至关重要的作用。然而，从单个图像生成完整的 360° 场景仍然是一个非常不适定且具有挑战性的问题。最近的方法通过利用扩散模型和深度估计网络取得了进展，但它们仍然难以在大视点变化下保持外观一致性和几何合理性，限制了它们在全场景生成中的有效性。为了解决这个问题，我们提出了 AnchoredDream，这是一种新颖的零镜头管道，通过外观几何相互增强机制将 360° 场景生成锚定在高保真几何上。给定单视图图像，我们的方法首先执行外观引导的几何生成，以构建可靠的 3D 场景布局。然后，我们通过一系列模块逐步生成完整的场景：扭曲和修复、扭曲和细化、后优化和新颖的灌浆块，确保输入视图和生成区域之间的无缝过渡。大量实验表明，AnchoredDream 在外观一致性和几何合理性方面都大幅优于现有方法，而且全部都是零样本方式。我们的结果凸显了几何基础在高质量、零镜头单视图场景生成方面的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16573v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16573v1",
        "title": "HA2F: Dual-module Collaboration-Guided Hierarchical Adaptive Aggregation Framework for Remote Sensing Change Detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "HA2F: Dual-module Collaboration-Guided Hierarchical Adaptive Aggregation Framework for Remote Sensing Change Detection"
        },
        "updated": "2026-01-23T09:21:11Z",
        "updated_parsed": [
            2026,
            1,
            23,
            9,
            21,
            11,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16573v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16573v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1109/TGRS.2025.3650706",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Remote sensing change detection (RSCD) aims to identify the spatio-temporal changes of land cover, providing critical support for multi-disciplinary applications (e.g., environmental monitoring, disaster assessment, and climate change studies). Existing methods focus either on extracting features from localized patches, or pursue processing entire images holistically, which leads to the cross temporal feature matching deviation and exhibiting sensitivity to radiometric and geometric noise. Following the above issues, we propose a dual-module collaboration guided hierarchical adaptive aggregation framework, namely HA2F, which consists of dynamic hierarchical feature calibration module (DHFCM) and noise-adaptive feature refinement module (NAFRM). The former dynamically fuses adjacent-level features through perceptual feature selection, suppressing irrelevant discrepancies to address multi-temporal feature alignment deviations. The NAFRM utilizes the dual feature selection mechanism to highlight the change sensitive regions and generate spatial masks, suppressing the interference of irrelevant regions or shadows. Extensive experiments verify the effectiveness of the proposed HA2F, which achieves state-of-the-art performance on LEVIR-CD, WHU-CD, and SYSU-CD datasets, surpassing existing comparative methods in terms of both precision metrics and computational efficiency. In addition, ablation experiments show that DHFCM and NAFRM are effective. \\href{https://huggingface.co/InPeerReview/RemoteSensingChangeDetection-RSCD.HA2F}{HA2F Official Code is Available Here!}",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Remote sensing change detection (RSCD) aims to identify the spatio-temporal changes of land cover, providing critical support for multi-disciplinary applications (e.g., environmental monitoring, disaster assessment, and climate change studies). Existing methods focus either on extracting features from localized patches, or pursue processing entire images holistically, which leads to the cross temporal feature matching deviation and exhibiting sensitivity to radiometric and geometric noise. Following the above issues, we propose a dual-module collaboration guided hierarchical adaptive aggregation framework, namely HA2F, which consists of dynamic hierarchical feature calibration module (DHFCM) and noise-adaptive feature refinement module (NAFRM). The former dynamically fuses adjacent-level features through perceptual feature selection, suppressing irrelevant discrepancies to address multi-temporal feature alignment deviations. The NAFRM utilizes the dual feature selection mechanism to highlight the change sensitive regions and generate spatial masks, suppressing the interference of irrelevant regions or shadows. Extensive experiments verify the effectiveness of the proposed HA2F, which achieves state-of-the-art performance on LEVIR-CD, WHU-CD, and SYSU-CD datasets, surpassing existing comparative methods in terms of both precision metrics and computational efficiency. In addition, ablation experiments show that DHFCM and NAFRM are effective. \\href{https://huggingface.co/InPeerReview/RemoteSensingChangeDetection-RSCD.HA2F}{HA2F Official Code is Available Here!}"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T09:21:11Z",
        "published_parsed": [
            2026,
            1,
            23,
            9,
            21,
            11,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Shuying Li"
            },
            {
                "name": "Yuchen Wang"
            },
            {
                "name": "San Zhang"
            },
            {
                "name": "Chuang Yang"
            }
        ],
        "author_detail": {
            "name": "Chuang Yang"
        },
        "author": "Chuang Yang",
        "arxiv_doi": "10.1109/TGRS.2025.3650706",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "HA2F：用于遥感变化检测的双模块协作引导的分层自适应聚合框架",
        "abstract_cn": "遥感变化检测（RSCD）旨在识别土地覆盖的时空变化，为多学科应用（例如环境监测、灾害评估和气候变化研究）提供关键支持。现有的方法要么侧重于从局部斑块中提取特征，要么追求整体处理整个图像，这导致跨时间特征匹配偏差并对辐射和几何噪声表现出敏感性。针对上述问题，我们提出了一种双模块协作引导的分层自适应聚合框架，即HA2F，它由动态分层特征校准模块（DHFCM）和噪声自适应特征细化模块（NAFRM）组成。前者通过感知特征选择动态融合相邻级特征，抑制不相关的差异以解决多时态特征对齐偏差。 NAFRM利用双特征选择机制突出变化敏感区域并生成空间掩模，抑制不相关区域或阴影的干扰。大量实验验证了所提出的 HA2F 的有效性，它在 LEVIR-CD、WHU-CD 和 SYSU-CD 数据集上实现了最先进的性能，在精度指标和计算效率方面超越了现有的比较方法。此外，消融实验表明DHFCM和NAFRM是有效的。 \\href{https://huggingface.co/InPeerReview/RemoteSensingChangeDetection-RSCD.HA2F}{HA2F 官方代码可在此处获取！}"
    },
    {
        "id": "http://arxiv.org/abs/2601.16582v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16582v1",
        "title": "X-Aligner: Composed Visual Retrieval without the Bells and Whistles",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "X-Aligner: Composed Visual Retrieval without the Bells and Whistles"
        },
        "updated": "2026-01-23T09:33:38Z",
        "updated_parsed": [
            2026,
            1,
            23,
            9,
            33,
            38,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16582v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16582v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T09:33:38Z",
        "published_parsed": [
            2026,
            1,
            23,
            9,
            33,
            38,
            4,
            23,
            0
        ],
        "arxiv_comment": "8 pages",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yuqian Zheng"
            },
            {
                "name": "Mariana-Iuliana Georgescu"
            }
        ],
        "author_detail": {
            "name": "Mariana-Iuliana Georgescu"
        },
        "author": "Mariana-Iuliana Georgescu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "X-Aligner：没有花哨的组合视觉检索",
        "abstract_cn": "组合视频检索 (CoVR) 通过结合视觉和文本查询来促进视频检索。然而，现有的 CoVR 框架通常在一个阶段融合多模式输入，仅实现比初始基线更高的边际收益。为了解决这个问题，我们提出了一种新颖的 CoVR 框架，该框架利用视觉语言模型 (VLM) 的表征能力。我们的框架采用了一种新颖的交叉注意模块 X-Aligner，该模块由交叉注意层组成，逐步融合视觉和文本输入，并将其多模态表示与目标视频的多模态表示对齐。为了进一步增强多模式查询的表示，我们将视觉查询的标题合并为附加输入。该框架分两个阶段进行训练，以保留预训练的 VLM 表示。在第一阶段，仅训练新引入的模块，而在第二阶段，文本查询编码器也进行微调。我们在 BLIP 系列架构（即 BLIP 和 BLIP-2）之上实现我们的框架，并在 Webvid-CoVR 数据集上对其进行训练。除了对 Webvid-CoVR-Test 进行域内评估之外，我们还对合成图像检索 (CIR) 数据集 CIRCO 和 Fashion-IQ 进行零样本评估。我们的框架在 CoVR 上实现了最先进的性能，在 Webvid-CoVR-Test 上获得了 63.93% 的 Recall@1，并在 CIR 任务上展示了强大的零样本泛化能力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16608v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16608v1",
        "title": "A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling"
        },
        "updated": "2026-01-23T10:08:37Z",
        "updated_parsed": [
            2026,
            1,
            23,
            10,
            8,
            37,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16608v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16608v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T10:08:37Z",
        "published_parsed": [
            2026,
            1,
            23,
            10,
            8,
            37,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jingsong Xia"
            },
            {
                "name": "Siqi Wang"
            }
        ],
        "author_detail": {
            "name": "Siqi Wang"
        },
        "author": "Siqi Wang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过自监督对比学习和量子增强特征建模的轻量级医学图像分类框架",
        "abstract_cn": "智能医学图像分析对于临床决策支持至关重要，但往往受到注释稀缺、计算资源有限和模型泛化欠佳的限制。为了应对这些挑战，我们提出了一种轻量级医学图像分类框架，它将自监督对比学习与量子增强特征建模相结合。 MobileNetV2 被用作紧凑的主干网，并使用 SimCLR 式的自监督范例在未标记的图像上进行预训练。嵌入轻量级参数化量子电路（PQC）作为量子特征增强模块，形成混合经典量子架构，随后在有限的标记数据上进行微调。实验结果表明，仅用大约 2-3 百万个参数和较低的计算成本，所提出的方法在准确性、AUC 和 F1 分数方面始终优于没有自监督学习或量子增强的经典基线。特征可视化进一步表明可辨别性和表示稳定性得到提高。总体而言，这项工作为资源受限环境下的高性能医疗人工智能提供了实用且具有前瞻性的解决方案。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16617v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16617v1",
        "title": "Boundary and Position Information Mining for Aerial Small Object Detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Boundary and Position Information Mining for Aerial Small Object Detection"
        },
        "updated": "2026-01-23T10:15:12Z",
        "updated_parsed": [
            2026,
            1,
            23,
            10,
            15,
            12,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16617v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16617v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T10:15:12Z",
        "published_parsed": [
            2026,
            1,
            23,
            10,
            15,
            12,
            4,
            23,
            0
        ],
        "arxiv_comment": "12 pages, 10 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Rongxin Huang"
            },
            {
                "name": "Guangfeng Lin"
            },
            {
                "name": "Wenbo Zhou"
            },
            {
                "name": "Zhirong Li"
            },
            {
                "name": "Wenhuan Wu"
            }
        ],
        "author_detail": {
            "name": "Wenhuan Wu"
        },
        "author": "Wenhuan Wu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "空中小物体检测的边界和位置信息挖掘",
        "abstract_cn": "无人机（UAV）在航空摄影和物体识别领域的应用变得越来越普遍。然而，由于尺度不平衡和边缘模糊，在目标检测中准确捕获小目标存在重大挑战。为了解决这些问题，提出了边界和位置信息挖掘（BPIM）框架来捕获对象边缘和位置线索。所提出的BPIM包括用于获取位置信息的位置信息引导（PIG）模块、用于提取物体边缘的边界信息引导（BIG）模块、用于逐步组装浅层图像特征的跨尺度融合（CSF）模块、用于逐步组合位置和边界信息的三特征融合（TFF）模块以及用于灵活合并深层语义特征的自适应权重融合（AWF）模块。因此，BPIM可以利用注意力机制和跨尺度特征融合策略整合图像中的边界、位置和尺度信息来进行小物体检测。此外，BPIM不仅通过与边界的自适应权重融合提高了上下文特征的辨别力，而且通过跨尺度位置融合增强了小物体的感知。在 VisDrone2021、DOTA1.0 和 WiderPerson 数据集上，实验结果表明，与基线 Yolov5-P2 相比，BPIM 具有更好的性能，并且在具有可比计算负载的最先进方法中获得了有希望的性能。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16627v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16627v1",
        "title": "SCHIGAND: A Synthetic Facial Generation Mode Pipeline",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SCHIGAND: A Synthetic Facial Generation Mode Pipeline"
        },
        "updated": "2026-01-23T10:30:58Z",
        "updated_parsed": [
            2026,
            1,
            23,
            10,
            30,
            58,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16627v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16627v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CY",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T10:30:58Z",
        "published_parsed": [
            2026,
            1,
            23,
            10,
            30,
            58,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ananya Kadali"
            },
            {
                "name": "Sunnie Jehan-Morrison"
            },
            {
                "name": "Orasiki Wellington"
            },
            {
                "name": "Barney Evans"
            },
            {
                "name": "Precious Durojaiye"
            },
            {
                "name": "Richard Guest"
            }
        ],
        "author_detail": {
            "name": "Richard Guest"
        },
        "author": "Richard Guest",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SCHIGAND：合成面部生成模式管道",
        "abstract_cn": "对用于训练和测试生物识别系统的多样化和高质量面部数据集的需求不断增长，但受到隐私法规、数据稀缺和道德问题的挑战。合成面部图像提供了一种潜在的解决方案，但现有的生成模型往往难以平衡现实性、多样性和身份保存。本文提出了 SCHIGAND，一种新颖的合成面部生成管道，集成了 StyleCLIP、HyperStyle、InterfaceGAN 和 Diffusion 模型，可生成高度真实且可控的面部数据集。 SCHIGAND 增强了身份保存，同时生成真实的类内差异并保持类间独特性，使其适合生物识别测试。使用领先的面部验证模型 ArcFace 对生成的数据集进行评估，以评估其与现实世界面部数据集相比的有效性。实验结果表明，SCHIGAND 实现了图像质量和多样性之间的平衡，解决了先前生成模型的关键局限性。这项研究凸显了 SCHIGAND 补充并在某些情况下取代面部生物识别应用的真实数据的潜力，为合成数据集生成中符合隐私且可扩展的解决方案铺平了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16645v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16645v1",
        "title": "Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss"
        },
        "updated": "2026-01-23T11:06:51Z",
        "updated_parsed": [
            2026,
            1,
            23,
            11,
            6,
            51,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16645v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16645v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T11:06:51Z",
        "published_parsed": [
            2026,
            1,
            23,
            11,
            6,
            51,
            4,
            23,
            0
        ],
        "arxiv_comment": "Accepted to WACV 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Minsu Gong"
            },
            {
                "name": "Nuri Ryu"
            },
            {
                "name": "Jungseul Ok"
            },
            {
                "name": "Sunghyun Cho"
            }
        ],
        "author_detail": {
            "name": "Sunghyun Cho"
        },
        "author": "Sunghyun Cho",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过具有新颖结构保留损失的扩散模型进行边缘感知图像处理",
        "abstract_cn": "图像编辑领域的最新进展利用潜在扩散模型 (LDM) 来实现跨不同任务的多功能、文本提示驱动的编辑。然而，维护像素级边缘结构（对于照片级真实风格转换或图像色调调整等任务至关重要）仍然是基于潜在扩散的编辑的挑战。为了克服这一限制，我们提出了一种新颖的结构保留损失（SPL），它利用局部线性模型来量化输入图像和编辑图像之间的结构差异。我们的免训练方法将 SPL 直接集成到扩散模型的生成过程中，以确保结构保真度。这一核心机制得到了后处理步骤的补充，以减轻 LDM 解码失真、用于精确编辑本地化的掩蔽策略，以及用于保留未编辑区域色调的颜色保留损失。实验证实 SPL 增强了结构保真度，在基于潜在扩散的图像编辑中提供了最先进的性能。我们的代码将在 https://github.com/gongms00/SPL 公开发布。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16652v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16652v1",
        "title": "Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training"
        },
        "updated": "2026-01-23T11:16:34Z",
        "updated_parsed": [
            2026,
            1,
            23,
            11,
            16,
            34,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16652v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16652v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.NE",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T11:16:34Z",
        "published_parsed": [
            2026,
            1,
            23,
            11,
            16,
            34,
            4,
            23,
            0
        ],
        "arxiv_comment": "Accepted at ISBI 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Aurora Pia Ghiardelli"
            },
            {
                "name": "Guangzhi Tang"
            },
            {
                "name": "Tao Sun"
            }
        ],
        "author_detail": {
            "name": "Tao Sun"
        },
        "author": "Tao Sun",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "基于有效训练的尖峰神经网络的可靠脑肿瘤分割",
        "abstract_cn": "我们提出了一种使用尖峰神经网络 (SNN) 进行 3D 脑肿瘤分割的可靠且节能的框架。矢状、冠状和轴向 SNN 模型的多视图集成提供了体素方面的不确定性估计并增强了分割的鲁棒性。为了解决训练用于语义图像分割的 SNN 模型的高计算成本问题，我们采用了时间前向传播 (FPTT)，它在保持时间学习效率的同时显着降低了计算成本。多模态脑肿瘤分割挑战（BraTS 2017 和 BraTS 2023）的实验证明了具有竞争力的准确性、经过良好校准的不确定性以及 FLOP 减少 87%，这凸显了 SNN 在可靠、低功耗医疗物联网和护理点系统中的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16672v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16672v1",
        "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction"
        },
        "updated": "2026-01-23T11:42:02Z",
        "updated_parsed": [
            2026,
            1,
            23,
            11,
            42,
            2,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16672v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16672v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T11:42:02Z",
        "published_parsed": [
            2026,
            1,
            23,
            11,
            42,
            2,
            4,
            23,
            0
        ],
        "arxiv_comment": "15 pages, 8 figures, Submitted to CVPR 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ming Li"
            },
            {
                "name": "Hui Shan"
            },
            {
                "name": "Kai Zheng"
            },
            {
                "name": "Chentao Shen"
            },
            {
                "name": "Siyu Liu"
            },
            {
                "name": "Yanwei Fu"
            },
            {
                "name": "Zhen Chen"
            },
            {
                "name": "Xiangru Huang"
            }
        ],
        "author_detail": {
            "name": "Xiangru Huang"
        },
        "author": "Xiangru Huang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ReWeaver：迈向模拟就绪且拓扑精确的服装重建",
        "abstract_cn": "高质量 3D 服装重建在缩小数字化身、虚拟试穿和机器人操作等应用中的模拟与真实差距方面发挥着至关重要的作用。然而，现有的服装重建方法通常依赖于非结构化表示，例如 3D 高斯 Splats，难以提供服装拓扑和缝纫结构的准确重建。因此，重建的输出通常不适合高保真物理模拟。我们提出了 ReWeaver，这是一种新颖的框架，用于从稀疏多视图 RGB 图像中进行拓扑精确的 3D 服装和缝纫图案重建。只需四个输入视图，ReWeaver 即可预测接缝和面板以及它们在 2D UV 空间和 3D 空间中的连接性。预测的接缝和面板与多视图图像精确对齐，产生适合 3D 感知、高保真物理模拟和机器人操作的结构化 2D--3D 服装表示。为了实现有效的训练，我们构建了一个大规模数据集 GCD-TS，包括多视图 RGB 图像、3D 服装几何形状、纹理人体网格和带注释的缝纫图案。该数据集包含超过 100,000 个合成样本，涵盖各种复杂的几何形状和拓扑。大量实验表明，ReWeaver 在拓扑精度、几何对齐和接缝板一致性方面始终优于现有方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16733v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16733v1",
        "title": "Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis"
        },
        "updated": "2026-01-23T13:31:01Z",
        "updated_parsed": [
            2026,
            1,
            23,
            13,
            31,
            1,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16733v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16733v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Circular Synthetic Aperture Sonar (CSAS) provides a 360° azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Circular Synthetic Aperture Sonar (CSAS) provides a 360° azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T13:31:01Z",
        "published_parsed": [
            2026,
            1,
            23,
            13,
            31,
            1,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "arxiv_journal_ref": "Synthetic Aperture in Sonar and Radar 2023",
        "authors": [
            {
                "name": "Yann Le Gall"
            },
            {
                "name": "Nicolas Burlet"
            },
            {
                "name": "Mathieu Simon"
            },
            {
                "name": "Fabien Novella"
            },
            {
                "name": "Samantha Dugelay"
            },
            {
                "name": "Jean-Philippe Malkasse"
            }
        ],
        "author_detail": {
            "name": "Jean-Philippe Malkasse"
        },
        "author": "Jean-Philippe Malkasse",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "利用圆形合成孔径声纳成像中的阴影进行目标分析",
        "abstract_cn": "圆形合成孔径声纳 (CSAS) 提供海底 360° 方位角视图，超越了传统侧扫 SAS 的有限孔径和单视图图像。这使得 CSAS 成为水雷战中目标识别的宝贵工具，在水雷战中，观点的多样性对于减少误报至关重要。 CSAS 处理通常会生成非常高分辨率的二维图像。然而，照明器的圆形位移引入的视差填充了阴影区域，并且海底物体投射的阴影会丢失，有利于方位角覆盖和分辨率。然而，阴影提供了对目标识别有用的目标形状的补充信息。在本文中，我们探索了一种从 CSAS 数据中检索阴影信息的方法，以改进目标分析并进行 3D 重建。使用子孔径滤波来获取沿圆形轨迹的各个视点的图像集合，并应用定焦阴影增强（FFSE）来获得清晰的阴影。还提出了一个交互式界面，允许操作人员沿着圆形轨迹可视化这些阴影。应用空间雕刻重建方法从分段阴影推断物体的 3D 形状。结果证明了圆形 SAS 中的阴影在改进目标分析和 3D 重建方面的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16759v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16759v1",
        "title": "Curated endoscopic retrograde cholangiopancreatography images dataset",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Curated endoscopic retrograde cholangiopancreatography images dataset"
        },
        "updated": "2026-01-23T14:05:42Z",
        "updated_parsed": [
            2026,
            1,
            23,
            14,
            5,
            42,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16759v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16759v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Endoscopic Retrograde Cholangiopancreatography (ERCP) is a key procedure in the diagnosis and treatment of biliary and pancreatic diseases. Artificial intelligence has been pointed as one solution to automatize diagnosis. However, public ERCP datasets are scarce, which limits the use of such approach. Therefore, this study aims to help fill this gap by providing a large and curated dataset. The collection is composed of 19.018 raw images and 19.317 processed from 1.602 patients. 5.519 images are labeled, which provides a ready to use dataset. All images were manually inspected and annotated by two gastroenterologist with more than 5 years of experience and reviewed by another gastroenterologist with more than 20 years of experience, all with more than 400 ERCP procedures annually. The utility and validity of the dataset is proven by a classification experiment. This collection aims to provide or contribute for a benchmark in automatic ERCP analysis and diagnosis of biliary and pancreatic diseases.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Endoscopic Retrograde Cholangiopancreatography (ERCP) is a key procedure in the diagnosis and treatment of biliary and pancreatic diseases. Artificial intelligence has been pointed as one solution to automatize diagnosis. However, public ERCP datasets are scarce, which limits the use of such approach. Therefore, this study aims to help fill this gap by providing a large and curated dataset. The collection is composed of 19.018 raw images and 19.317 processed from 1.602 patients. 5.519 images are labeled, which provides a ready to use dataset. All images were manually inspected and annotated by two gastroenterologist with more than 5 years of experience and reviewed by another gastroenterologist with more than 20 years of experience, all with more than 400 ERCP procedures annually. The utility and validity of the dataset is proven by a classification experiment. This collection aims to provide or contribute for a benchmark in automatic ERCP analysis and diagnosis of biliary and pancreatic diseases."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T14:05:42Z",
        "published_parsed": [
            2026,
            1,
            23,
            14,
            5,
            42,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Alda João Andrade"
            },
            {
                "name": "Mónica Martins"
            },
            {
                "name": "André Ferreira"
            },
            {
                "name": "Tarcísio Araújo"
            },
            {
                "name": "Luís Lopes"
            },
            {
                "name": "Victor Alves"
            }
        ],
        "author_detail": {
            "name": "Victor Alves"
        },
        "author": "Victor Alves",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "精心策划的内窥镜逆行胰胆管造影图像数据集",
        "abstract_cn": "内镜逆行胰胆管造影（ERCP）是诊断和治疗胆胰疾病的关键手术。人工智能被认为是自动化诊断的一种解决方案。然而，公共 ERCP 数据集稀缺，这限制了这种方法的使用。因此，本研究旨在通过提供大型且精心策划的数据集来帮助填补这一空白。该集合由 19.018 张原始图像和 1.602 名患者的 19.317 张处理图像组成。 5.519 个图像被标记，这提供了一个随时可用的数据集。所有图像均由两名具有 5 年以上经验的胃肠病专家进行手动检查和注释，并由另一名具有 20 年以上经验的胃肠病专家进行审核，每年均进行 400 多例 ERCP 手术。通过分类实验证明了数据集的实用性和有效性。该集合旨在提供或贡献自动 ERCP 分析和胆胰疾病诊断的基准。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16763v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16763v1",
        "title": "Flow Matching for Probabilistic Monocular 3D Human Pose Estimation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Flow Matching for Probabilistic Monocular 3D Human Pose Estimation"
        },
        "updated": "2026-01-23T14:09:33Z",
        "updated_parsed": [
            2026,
            1,
            23,
            14,
            9,
            33,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16763v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16763v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T14:09:33Z",
        "published_parsed": [
            2026,
            1,
            23,
            14,
            9,
            33,
            4,
            23,
            0
        ],
        "arxiv_comment": "8 pages, 2 figures, 7 tables, under submission",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Cuong Le"
            },
            {
                "name": "Pavló Melnyk"
            },
            {
                "name": "Bastian Wandt"
            },
            {
                "name": "Mårten Wadenbäck"
            }
        ],
        "author_detail": {
            "name": "Mårten Wadenbäck"
        },
        "author": "Mårten Wadenbäck",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于概率单目 3D 人体姿势估计的流匹配",
        "abstract_cn": "由于深度模糊，从单目相机视图恢复 3D 人体姿势是一个非常不适定的问题。早期关于从 2D 提升 3D 人体姿势的研究通常包含不正确但过于自信的 3D 估计。为了缓解这个问题，新兴的概率方法将 3D 估计视为分布，同时考虑姿势的不确定性测量。属于类似类别，我们提出了 FMPose，一种基于流匹配生成方法的概率 3D 人体姿势估计方法。以 2D 线索为条件，流匹配方案通过连续归一化流学习从简单源分布到合理的 3D 人体姿势分布的最佳传输。二维提升条件通过图卷积网络进行建模，利用人体关节之间的可学习连接作为特征聚合的图结构。与基于扩散的方法相比，具有最佳传输的 FMPose 可以生成更快、更准确的 3D 姿态生成。实验结果表明，在 3D 人体姿态估计的三个常见基准（即 Human3.6M、MPI-INF-3DHP 和 3DPW）上，我们的 FMPose 相对于当前最先进的方法有了重大改进。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16788v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16788v1",
        "title": "REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion"
        },
        "updated": "2026-01-23T14:33:49Z",
        "updated_parsed": [
            2026,
            1,
            23,
            14,
            33,
            49,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16788v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16788v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T14:33:49Z",
        "published_parsed": [
            2026,
            1,
            23,
            14,
            33,
            49,
            4,
            23,
            0
        ],
        "arxiv_comment": "submitted to CVPR 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xuewei Li"
            },
            {
                "name": "Xinghan Bao"
            },
            {
                "name": "Zhimin Chen"
            },
            {
                "name": "Xi Li"
            }
        ],
        "author_detail": {
            "name": "Xi Li"
        },
        "author": "Xi Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "REL-SF4PASS：具有 REL 深度表示和球形融合的全景语义分割",
        "abstract_cn": "全景语义分割（PASS）作为计算机视觉中一个重要且具有挑战性的问题，旨在基于超广角视角给出完整的场景感知。大多数PASS方法通常侧重于具有RGB输入的球面几何形状或使用原始或HHA格式的深度信息，这没有充分利用全景图像几何形状。为了解决这些缺点，我们提出了 REL-SF4PASS 以及基于柱坐标和球动态多模态融合 SMMF 的 REL 深度表示。 REL由校正深度、高程增益垂直倾角和横向方位角组成，以柱坐标方式和表面法线方向完整地表示3D空间。 SMMF旨在保证全景图像不同区域融合的多样性，减少ERP投影中圆柱侧面膨胀的破损，采用不同的融合策略来匹配全景图像中的不同区域。实验结果表明，REL-SF4PASS 显着提高了流行基准斯坦福2D3D 全景数据集的性能和稳健性。它在所有 3 倍上获得了 2.35% 的平均 mIoU 改进，并且在面对 3D 干扰时将性能方差降低了约 70%。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16836v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16836v1",
        "title": "ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models"
        },
        "updated": "2026-01-23T15:36:02Z",
        "updated_parsed": [
            2026,
            1,
            23,
            15,
            36,
            2,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16836v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16836v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T15:36:02Z",
        "published_parsed": [
            2026,
            1,
            23,
            15,
            36,
            2,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chenxi Ruan"
            },
            {
                "name": "Yu Xiao"
            },
            {
                "name": "Yihan Hou"
            },
            {
                "name": "Guosheng Hu"
            },
            {
                "name": "Wei Zeng"
            }
        ],
        "author_detail": {
            "name": "Wei Zeng"
        },
        "author": "Wei Zeng",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ColorConceptBench：文本到图像模型中概率颜色概念理解的基准",
        "abstract_cn": "虽然文本到图像（T2I）模型已经取得了相当大的进步，但它们将颜色与隐含概念相关联的能力仍未得到充分开发。为了解决这一差距，我们引入了 ColorConceptBench，这是一种新的人工注释基准，可通过概率颜色分布的角度系统地评估颜色概念关联。 ColorConceptBench 超越了明确的颜色名称或代码，探索模型如何使用 6,369 个人工注释的基础来转换 1,281 个隐式颜色概念。我们对七个领先的 T2I 模型的评估表明，当前模型缺乏对抽象语义的敏感性，而且至关重要的是，这种限制似乎对标准干预措施（例如，缩放和指导）具有抵抗力。这表明，实现类人颜色语义需要的不仅仅是更大的模型，而且需要模型学习和表示隐含含义的方式发生根本性转变。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16885v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16885v1",
        "title": "GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss"
        },
        "updated": "2026-01-23T16:46:59Z",
        "updated_parsed": [
            2026,
            1,
            23,
            16,
            46,
            59,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16885v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16885v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.RO",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T16:46:59Z",
        "published_parsed": [
            2026,
            1,
            23,
            16,
            46,
            59,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yangfan Xu"
            },
            {
                "name": "Lilian Zhang"
            },
            {
                "name": "Xiaofeng He"
            },
            {
                "name": "Pengdong Wu"
            },
            {
                "name": "Wenqi Wu"
            },
            {
                "name": "Jun Mao"
            }
        ],
        "author_detail": {
            "name": "Jun Mao"
        },
        "author": "Jun Mao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "GPA-VGGT：通过几何和物理感知损失的自监督学习使 VGGT 适应大规模定位",
        "abstract_cn": "基于 Transformer 的通用视觉几何框架在相机姿态估计和 3D 场景理解方面表现出了良好的性能。视觉几何接地变压器 (VGGT) 模型的最新进展在相机姿态估计和 3D 重建方面显示出了巨大的前景。然而，这些模型通常依赖于真实标签进行训练，在适应未标记和未见过的场景时提出了挑战。在本文中，我们提出了一种自监督框架，用未标记数据训练 VGGT，从而增强其在大规模环境中的定位能力。为了实现这一目标，我们将传统的成对关系扩展到用于自监督学习的序列几何约束。具体来说，在每个序列中，我们对多个源帧进行采样，并将它们几何投影到不同的目标帧上，从而提高了时间特征的一致性。我们将物理光度一致性和几何约束制定为联合优化损失，以避免硬标签的要求。通过使用该方法训练模型，不仅局部和全局交叉视图注意层，而且相机和深度头都可以有效地捕获底层的多视图几何形状。实验表明，该模型在数百次迭代内收敛，并在大规模定位方面取得了显着改进。我们的代码将发布在https://github.com/X-yangfan/GPA-VGGT。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16954v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16954v1",
        "title": "Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment"
        },
        "updated": "2026-01-23T18:23:03Z",
        "updated_parsed": [
            2026,
            1,
            23,
            18,
            23,
            3,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16954v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16954v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T18:23:03Z",
        "published_parsed": [
            2026,
            1,
            23,
            18,
            23,
            3,
            4,
            23,
            0
        ],
        "arxiv_comment": "accepted in ICASSP 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ba-Thinh Lam"
            },
            {
                "name": "Thanh-Huy Nguyen"
            },
            {
                "name": "Hoang-Thien Nguyen"
            },
            {
                "name": "Quang-Khai Bui-Tran"
            },
            {
                "name": "Nguyen Lan Vi Vu"
            },
            {
                "name": "Phat K. Huynh"
            },
            {
                "name": "Ulas Bagci"
            },
            {
                "name": "Min Xu"
            }
        ],
        "author_detail": {
            "name": "Min Xu"
        },
        "author": "Min Xu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "具有聚类最大平均差异对齐的域不变混合域半监督医学图像分割",
        "abstract_cn": "深度学习在医学图像语义分割方面取得了显着进展，但其成功很大程度上取决于大规模专家注释和一致的数据分布。在实践中，注释很少，并且图像是从多个扫描仪或中心收集的，导致混合域设置具有未知的域标签和严重的域间隙。现有的半监督或域适应方法通常假设单个域转移或访问显式域索引，这在现实世界的部署中很少成立。在本文中，我们提出了一种域不变的混合域半监督分割框架，该框架共同增强数据多样性并减轻域偏差。复制粘贴机制 (CPM) 通过跨域传输信息区域来增强训练集，而集群最大平均差异 (CMMD) 块则对未标记的特征进行聚类，并通过 MMD 目标将它们与标记的锚点对齐，从而鼓励域不变表示。我们的方法集成在师生框架内，即使在标记示例很少和多个未知领域差异的情况下也能实现稳健且精确的分割。 Fundus 和 M&Ms 基准的实验表明，我们的方法始终超越半监督和域适应方法，为混合域半监督医学图像分割建立了潜在的解决方案。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16973v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16973v1",
        "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents"
        },
        "updated": "2026-01-23T18:43:34Z",
        "updated_parsed": [
            2026,
            1,
            23,
            18,
            43,
            34,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16973v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16973v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T18:43:34Z",
        "published_parsed": [
            2026,
            1,
            23,
            18,
            43,
            34,
            4,
            23,
            0
        ],
        "arxiv_comment": "Project page: https://visgym.github.io/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zirui Wang"
            },
            {
                "name": "Junyi Zhang"
            },
            {
                "name": "Jiaxin Ge"
            },
            {
                "name": "Long Lian"
            },
            {
                "name": "Letian Fu"
            },
            {
                "name": "Lisa Dunlap"
            },
            {
                "name": "Ken Goldberg"
            },
            {
                "name": "XuDong Wang"
            },
            {
                "name": "Ion Stoica"
            },
            {
                "name": "David M. Chan"
            },
            {
                "name": "Sewon Min"
            },
            {
                "name": "Joseph E. Gonzalez"
            }
        ],
        "author_detail": {
            "name": "Joseph E. Gonzalez"
        },
        "author": "Joseph E. Gonzalez",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "VisGym：多模式代理的多样化、可定制、可扩展环境",
        "abstract_cn": "现代视觉语言模型（VLM）在多步骤视觉交互中的特征仍然很差，特别是在它们如何整合长期视野中的感知、记忆和行动方面。我们推出 VisGym，这是一个拥有 17 个环境的体育馆，用于评估和训练 VLM。该套件涵盖符号谜题、真实图像理解、导航和操作，并提供对难度、输入表示、规划范围和反馈的灵活控制。我们还提供多步求解器来生成结构化演示，从而实现监督微调。我们的评估表明，所有前沿模型都在交互设置中陷入困境，在简单（46.6％）和困难（26.0％）配置中的成功率都很低。我们的实验揭示了显着的局限性：模型难以有效地利用长上下文，在无限历史记录中的表现比在截断窗口中的表现更差。此外，我们发现一些基于文本的符号任务一旦以视觉方式呈现就会变得更加困难。然而，明确的目标观察、文本反馈和在部分可观察或未知动态设置中进行监督微调的探索性演示会产生一致的收益，突出显示具体的故障模式和改进多步骤视觉决策的途径。代码、数据和模型可以在：https://visgym.github.io/ 找到。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16981v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16981v1",
        "title": "SyncLight: Controllable and Consistent Multi-View Relighting",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SyncLight: Controllable and Consistent Multi-View Relighting"
        },
        "updated": "2026-01-23T18:59:57Z",
        "updated_parsed": [
            2026,
            1,
            23,
            18,
            59,
            57,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16981v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16981v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T18:59:57Z",
        "published_parsed": [
            2026,
            1,
            23,
            18,
            59,
            57,
            4,
            23,
            0
        ],
        "arxiv_comment": "Project page: http://sync-light.github.io",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "David Serrano-Lozano"
            },
            {
                "name": "Anand Bhattad"
            },
            {
                "name": "Luis Herranz"
            },
            {
                "name": "Jean-François Lalonde"
            },
            {
                "name": "Javier Vazquez-Corral"
            }
        ],
        "author_detail": {
            "name": "Javier Vazquez-Corral"
        },
        "author": "Javier Vazquez-Corral",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SyncLight：可控且一致的多视图重新照明",
        "abstract_cn": "我们提出了 SyncLight，这是第一种在静态场景的多个未校准视图中实现一致的参数重新照明的方法。虽然单视图重新照明取得了显着进步，但现有的生成方法很难保持多机位广播、立体电影和虚拟制作所必需的严格照明一致性。 SyncLight 通过在场景的多视图捕获中精确控制光强度和颜色来解决这个问题，并以单个参考编辑为条件。我们的方法利用使用潜在桥匹配公式训练的多视图扩散变换器，在单个推理步骤中实现整个图像集的高保真重新照明。为了促进训练，我们引入了一个大规模混合数据集，其中包含不同的合成环境（根据现有来源和新设计的场景精心策划）以及校准照明下的高保真、真实世界多视图捕获。令人惊讶的是，虽然仅在图像对上进行训练，但 SyncLight 将零镜头推广到任意数量的视点，有效地在所有视图中传播照明变化，而不需要相机姿势信息。 SyncLight 为多视图捕捉系统提供实用的重新照明工作流程。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16982v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16982v1",
        "title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "AnyView: Synthesizing Any Novel View in Dynamic Scenes"
        },
        "updated": "2026-01-23T18:59:58Z",
        "updated_parsed": [
            2026,
            1,
            23,
            18,
            59,
            58,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16982v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16982v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.RO",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T18:59:58Z",
        "published_parsed": [
            2026,
            1,
            23,
            18,
            59,
            58,
            4,
            23,
            0
        ],
        "arxiv_comment": "Project webpage: https://tri-ml.github.io/AnyView/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Basile Van Hoorick"
            },
            {
                "name": "Dian Chen"
            },
            {
                "name": "Shun Iwase"
            },
            {
                "name": "Pavel Tokmakov"
            },
            {
                "name": "Muhammad Zubair Irshad"
            },
            {
                "name": "Igor Vasiljevic"
            },
            {
                "name": "Swati Gupta"
            },
            {
                "name": "Fangzhou Cheng"
            },
            {
                "name": "Sergey Zakharov"
            },
            {
                "name": "Vitor Campagnolo Guizilini"
            }
        ],
        "author_detail": {
            "name": "Vitor Campagnolo Guizilini"
        },
        "author": "Vitor Campagnolo Guizilini",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "AnyView：在动态场景中合成任何新颖的视图",
        "abstract_cn": "现代生成视频模型擅长产生令人信服的高质量输出，但难以在高度动态的现实环境中保持多视图和时空一致性。在这项工作中，我们介绍了 \\textbf{AnyView}，一种基于扩散的视频生成框架，用于具有最小归纳偏差或几何假设的 \\emph{动态视图合成}。我们利用具有不同监督级别的多个数据源，包括单目（2D）、多视图静态（3D）和多视图动态（4D）数据集，来训练通用时空隐式表示，能够从任意相机位置和轨迹生成零样本新颖视频。我们在标准基准上评估 AnyView，显示出与当前最先进水平具有竞争力的结果，并提出 \\textbf{AnyViewBench}，这是一个具有挑战性的新基准，专为各种现实场景中的 \\emph{extreme} 动态视图合成而定制。在这种更戏剧性的设置中，我们发现大多数基线的性能都会急剧下降，因为它们需要视点之间存在显着重叠，而 AnyView 在从 \\emph{any} 视点提示时保持了生成真实、合理且时空一致的视频的能力。结果、数据、代码和模型可以在以下位置查看：https://tri-ml.github.io/AnyView/"
    }
]