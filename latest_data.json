[
    {
        "id": "https://doi.org/10.1364/oe.585108",
        "title": "High-Resolution Confocal Raman Microscopy Spectrometer Design Based on Echelle Gratings",
        "link": "https://doi.org/10.1364/oe.585108",
        "published": "2026-01-25",
        "author": "jian zhang, xiaotian li, Jiri Jirigalantu, xiaotao mi, yuqi sun, Jian Zhang, geng wang, Guangdong Yu, Fuguan Li, heshig bayan",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "基于阶梯光栅的高分辨率共焦拉曼显微光谱仪设计",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.581308",
        "title": "Particle types and energy influence on radiation response of charge coupled devices used for space optical imaging",
        "link": "https://doi.org/10.1364/oe.581308",
        "published": "2026-01-25",
        "author": "Bingkai Liu, Lin Wen, Zitao Zhao, Yihao Cui, Qi Guo, Yudong Li",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "粒子类型和能量对空间光学成像电荷耦合器件辐射响应的影响",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.583342",
        "title": "Power Beaming for Space Exploration: Electrical and Mechanical Study of a Single Junction Thin Photonic Power Converter (PPC) in a Lunar Environment",
        "link": "https://doi.org/10.1364/oe.583342",
        "published": "2026-01-25",
        "author": "Milad Nouri, Valentin Daniel, John Cook, Mandy Lewis, Guillaume Blanchette, Paolo Pino, Karin Hinzer",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "用于太空探索的功率发射：月球环境中单结薄光子功率转换器 (PPC) 的电气和机械研究",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.580311",
        "title": "Varactor-Diode-Controlled Tunable Metasurface for Dynamic Transmission Tuning with Band-Matched High Absorption",
        "link": "https://doi.org/10.1364/oe.580311",
        "published": "2026-01-25",
        "author": "Hao Yang, Yongfeng li, Lixin Jiang, Zhe Qin, Lei Wang, Jiechu Liu, Zhihao Guo, Lin Zheng, Hongya Chen, Wenjie Wang, Jiafu Wang",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "变容二极管控制的可调谐超表面，用于带匹配高吸收的动态传输调谐",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.587323",
        "title": "Highly birefringent low-loss broadband polarization-maintaining hollow-core antiresonant fiber based on eccentric tubes",
        "link": "https://doi.org/10.1364/oe.587323",
        "published": "2026-01-26",
        "author": "Yunhao Zhu, Shu Liu, Limin Tian, Ying Ma, Xuan Zhang, Chuanfei Yao, pingxue Li",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "基于偏心管的高双折射低损耗宽带保偏空芯反谐振光纤",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1002/lpor.202502758",
        "title": "Inverse Synthetic Aperture Macroscopic Fourier Ptychographic Imaging of Moving Targets",
        "link": "https://doi.org/10.1002/lpor.202502758",
        "published": "2026-01-25",
        "author": "Sheng Li, Bowen Wang, Haitao Guan, Jiasong Sun, Kunyao Liang, Zihao Pei, Min Zeng, Shang Bao, Chunting Xu, Yuzhen Zhang, Qian Chen, Chao Zuo",
        "summary": "<jats:title>ABSTRACT</jats:title>\n                  <jats:p>High‐resolution imaging of distant, dynamic targets is fundamentally limited by finite‐aperture diffraction. Unlike radar, optical detectors record only intensity and lack the temporal resolution for direct phase measurement, hindering straightforward extension of synthetic aperture techniques to the optical domain. While Macroscopic Fourier ptychography (FP) offers a promising computational alternative, its reliance on mechanical scanning or engineered illumination restricts its use to cooperative, static scenes. In this paper, we present Inverse Synthetic Aperture Macroscopic Fourier ptychography (ISAMFP), which, to the best of our knowledge, is the first practical realization of inverse synthetic aperture imaging for diffusely reflecting targets within the Fourier ptychography framework. By transforming the natural target motion from a detrimental factor into an effective source of angular diversity, ISAMFP eliminates the need for mechanical scanning or illumination modulation and achieves resolution enhancement far beyond the aperture limit using intensity‐only measurements. Experimental validations on USAF resolution targets, unmanned aerial vehicles, and complex multi‐target scenes highlight both the robustness and versatility of ISAMFP, establishing it as a powerful tool for non‐interferometric synthetic aperture imaging of moving, non‐cooperative targets, with broad implications for surveillance, defense, and remote sensing.</jats:p>",
        "journal": "Laser & Photonics Reviews",
        "title_cn": "运动目标的逆合成孔径宏观傅里叶叠层成像",
        "abstract_cn": "<jats:title>摘要</jats:title>\n                  <jats:p>远距离动态目标的高分辨率成像从根本上受到有限孔径衍射的限制。与雷达不同，光学探测器仅记录强度，缺乏直接相位测量的时间分辨率，阻碍了合成孔径技术直接扩展到光学领域。虽然宏观傅里叶叠层成像 (FP) 提供了一种有前途的计算替代方案，但它对机械扫描或工程照明的依赖限制了其在协作、静态场景中的使用。在本文中，我们提出了逆合成孔径宏观傅里叶叠层成像（ISAMFP），据我们所知，这是傅里叶叠层成像框架内漫反射目标的逆合成孔径成像的第一个实际实现。通过将自然目标运动从有害因素转变为角度多样性的有效来源，ISAMFP 消除了机械扫描或照明调制的需要，并使用仅强度测量实现了远远超出孔径限制的分辨率增强。对美国空军分辨率目标、无人机和复杂多目标场景的实验验证凸显了 ISAMFP 的稳健性和多功能性，使其成为对移动、非合作目标进行非干涉合成孔径成像的强大工具，对监视、防御和遥感具有广泛的影响。</jats:p>"
    },
    {
        "id": "https://doi.org/10.1038/s41377-025-02162-9",
        "title": "A monolithic microcavity laser with simultaneous upconversion and frequency-doubled lasing via crystal-in-glass engineering",
        "link": "https://doi.org/10.1038/s41377-025-02162-9",
        "published": "2026-01-26",
        "author": "Shengda Ye, Jianhao Chen, Jiayue He, Weiwei Chen, Xiongjian Huang, Xiaofeng Liu, Jianrong Qiu, Zhongmin Yang, Guoping Dong",
        "summary": "<jats:title>Abstract</jats:title>\n                  <jats:p>\n                    This work demonstrates a novel crystal-in-glass composite structure for multifunctional micro-nano light sources in integrated photonics. Based on a Er\n                    <jats:sup>3+</jats:sup>\n                    /Yb\n                    <jats:sup>3+</jats:sup>\n                    -codoped glass-ceramic (GC) whispering gallery mode (WGM) microcavity incorporating Ba\n                    <jats:sub>2</jats:sub>\n                    TiGe\n                    <jats:sub>2</jats:sub>\n                    O\n                    <jats:sub>8</jats:sub>\n                    (BTG) crystals, this microcavity enables dual-mode responses combining upconversion (UC) and frequency-doubled lasing. Made from a low-phonon-energy germanate glass matrix codoped with Er\n                    <jats:sup>3+</jats:sup>\n                    /Yb\n                    <jats:sup>3+</jats:sup>\n                    for UC gain, the microcavity is crystallized to form BTG microcrystals for second harmonic generation (SHG). By leveraging the high-quality factor (Q ≈ 5.7 × 10\n                    <jats:sup>4</jats:sup>\n                    ) and small mode volume, we achieve green (550 nm) and red (660 nm) UC lasing in a 30-μm-diameter microcavity with low thresholds of 13.31 μW and 12.97 μW, respectively. Benefitted from the random quasi-phase-matching (RQPM) mechanism in BTG GC, the microcavity also demonstrates an ultrabroadband frequency-doubling response from 900 to 1200 nm. By combining tapered fiber near-field coupling and femtosecond free-space pumping, we achieve simultaneous output of green/red UC lasing and frequency-doubled lasing within a single microcavity. We believe this work offers insights into hybrid material design and cooperative optical field manipulation for tunable lasers and on-chip nonlinear photonic systems.\n                  </jats:p>",
        "journal": "Light: Science & Applications",
        "title_cn": "通过玻璃晶体工程实现同步上转换和倍频激光的单片微腔激光器",
        "abstract_cn": "<jats:title>摘要</jats:title>\n                  <贾茨：p>\n                    这项工作展示了一种用于集成光子学中多功能微纳光源的新型玻璃晶体复合结构。基于Er\n                    <贾茨：sup>3+</贾茨：sup>\n                    /镱\n                    <贾茨：sup>3+</贾茨：sup>\n                    - 掺有 Ba 的共掺杂玻璃陶瓷 (GC) 回音壁模式 (WGM) 微腔\n                    <贾茨：子>2</贾茨：子>\n                    钛锗\n                    <贾茨：子>2</贾茨：子>\n                    氧\n                    <贾茨：子>8</贾茨：子>\n                    (BTG) 晶体，这种微腔可实现结合上转换 (UC) 和倍频激光的双模响应。由掺铒的低声子能锗酸盐玻璃基体制成\n                    <贾茨：sup>3+</贾茨：sup>\n                    /镱\n                    <贾茨：sup>3+</贾茨：sup>\n                    为了获得 UC 增益，微腔被结晶以形成用于二次谐波产生 (SHG) 的 BTG 微晶体。通过利用高品质因数 (Q ≈ 5.7 × 10\n                    <贾茨：sup>4</贾茨：sup>\n                    ）和小模式体积，我们在直径 30 μm 的微腔中实现了绿色（550 nm）和红色（660 nm）UC 激光，阈值分别为 13.31 μW 和 12.97 μW。受益于 BTG GC 中的随机准相位匹配 (RQPM) 机制，微腔还表现出 900 至 1200 nm 的超宽带倍频响应。通过将锥形光纤近场耦合和飞秒自由空间泵浦相结合，我们在单个微腔内实现了绿/红UC激光和倍频激光的同时输出。我们相信这项工作为可调谐激光器和片上非线性光子系统的混合材料设计和协同光场操纵提供了见解。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/optica.586220",
        "title": "Terahertz Fourier Ptychographic Imaging",
        "link": "https://doi.org/10.1364/optica.586220",
        "published": "2026-01-23",
        "author": "Pitambar Mukherjee, Vivek Kumar, Frederic Fauquet, Amaury Badon, Damien Bigourd, Kedar Khare, Sylvain Gigan, Patrick Mounaix",
        "summary": "High-resolution imaging in the terahertz (THz) spectral range remains fundamentally constrained by the limited numerical apertures of currently existing state-of-the-art imagers, which restricts its applicability across many fields, such as imaging in complex media or nondestructive testing. To address this challenge, we introduce a proof-of-concept implementation of THz Fourier Ptychographic imaging to enhance spatial resolution without requiring extensive hardware modifications. Our method employs a motorized kinematic mirror to generate a sequence of controlled, multi-angle plane-wave illuminations, with each resulting oblique-illumination intensity image encoding a limited portion of the spatial-frequency content of the target imaging sample. These measurements are combined in the Fourier domain using an aberration-corrected iterative phase-retrieval algorithm integrated with an efficient illumination calibration scheme, which enables the reconstruction of resolution-enhanced amplitude and phase images through the synthetic expansion of the effective numerical aperture. Our work establishes a robust framework for high-resolution THz imaging and paves the way for a wide array of applications in materials characterization, spectroscopy, and non-destructive evaluation.",
        "journal": "Optica",
        "title_cn": "太赫兹傅里叶叠层成像",
        "abstract_cn": "太赫兹（THz）光谱范围内的高分辨率成像仍然从根本上受到当前最先进成像仪有限数值孔径的限制，这限制了其在许多领域的适用性，例如复杂介质中的成像或无损检测。为了应对这一挑战，我们引入了太赫兹傅里叶叠层成像的概念验证实现，以增强空间分辨率，而无需进行大量的硬件修改。我们的方法采用电动运动镜来生成一系列受控的多角度平面波照明，每个产生的倾斜照明强度图像对目标成像样本的空间频率内容的有限部分进行编码。使用像差校正迭代相位检索算法与高效照明校准方案相结合，将这些测量结果在傅立叶域中组合起来，从而能够通过有效数值孔径的综合扩展来重建分辨率增强的幅度和相位图像。我们的工作为高分辨率太赫兹成像建立了一个强大的框架，并为材料表征、光谱学和无损评估方面的广泛应用铺平了道路。"
    },
    {
        "id": "https://doi.org/10.1364/oe.586734",
        "title": "Beyond-line-of-sight physical-layer chaos communication based on troposcatter",
        "link": "https://doi.org/10.1364/oe.586734",
        "published": "2026-01-26",
        "author": "Longsheng Wang, Siqi Zhang, Siyan Wu, Zhiwei Jia, Pengfa Chang, Yuanyuan Guo, Anbang Wang, Anni Liu, Yu Liang, Yuncai Wang",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "基于对流层散射的超视距物理层混沌通信",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.584740",
        "title": "Image-free few-target tracking via single-pixel sensing",
        "link": "https://doi.org/10.1364/oe.584740",
        "published": "2026-01-26",
        "author": "Xuan Zhang, Jiahao Xiong, Ai Fu, Chit Hun Mok, Zhiyuan Ye, Hongchao LIU",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "通过单像素传感进行无图像少目标跟踪",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.583262",
        "title": "Quantitative model and experimental validation of speckle decorrelation in digital speckle pattern interferometry",
        "link": "https://doi.org/10.1364/oe.583262",
        "published": "2026-01-26",
        "author": "Peizheng Yan, Xiangwei Liu, Yonghong Wang, Taozhang Ren, hongzhen jiang, jie li",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "数字散斑干涉测量中散斑去相关的定量模型和实验验证",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.573156",
        "title": "Snapshot thickness profile cross-sectioning for a warped substrate with non-uniform sub-100nm ultra-thin film",
        "link": "https://doi.org/10.1364/oe.573156",
        "published": "2026-01-26",
        "author": "Saeid Kheiryzadehkhanghah, GUKHYEON HWANG, Inho Choi, CHEONGSONG KIM, JinHwan An, Yatana Adolphe Gbogbo, Chih-Jen Yu, Robert Magnusson, Daesuk Kim",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "具有非均匀亚 100nm 超薄膜的翘曲基板的快照厚度剖面横截面",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/oe.580902",
        "title": "Kerr Soliton Dynamics and Deterministic Generation in Microresonators: Joint Effects of Chirped Pulse Parameters, Higher-Order Dispersion and Desynchronization",
        "link": "https://doi.org/10.1364/oe.580902",
        "published": "2026-01-26",
        "author": "Elham Barati, Seyed Masoud Jazayeri, Mahdi Shayganmanesh, Maryam Jandaghi",
        "summary": "Abstract not available.",
        "journal": "Optics Express",
        "title_cn": "微谐振器中的克尔孤子动力学和确定性生成：啁啾脉冲参数、高阶色散和去同步的联合效应",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1038/s41377-025-02152-x",
        "title": "Highly luminescent organic-inorganic hybrid antimony halide scintillators for real-time dynamic and 3D X-ray imaging",
        "link": "https://doi.org/10.1038/s41377-025-02152-x",
        "published": "2026-01-26",
        "author": "Haixia Cui, Wanjiao Li, Qianxi Li, Shaolong Wang, Mingye Zhu, Yongjing Deng, Shujuan Liu, Qiang Zhao",
        "summary": "<jats:title>Abstract</jats:title>\n                  <jats:p>\n                    Real-time dynamic and three-dimensional (3D) X-ray imaging are the most challenging types of X-ray imaging technology, placing more rigorous standards on scintillators. Lead-based (Pb\n                    <jats:sup>2+</jats:sup>\n                    ) organic-inorganic hybrid halide (OIHH) scintillators with high X-ray absorption coefficients have been demonstrated to exhibit excellent scintillation performance. However, their toxicity and instability hindered further development, and it is necessary to explore novel low-toxic metal-based OIHHs possessing excellent scintillation performance. Antimony-based (Sb\n                    <jats:sup>3+</jats:sup>\n                    ) OIHHs are not only environmentally friendly, but also show good stability compared to Pb\n                    <jats:sup>2+</jats:sup>\n                    -based OIHHs, which make them promising candidates as excellent scintillators. Currently, the understanding of Sb\n                    <jats:sup>3+</jats:sup>\n                    -based OIHH scintillators for X-ray detection and imaging is still in infancy and requires further exploration. Herein, we designed two Sb\n                    <jats:sup>3+</jats:sup>\n                    -based OIHH crystals of (BPP)\n                    <jats:sub>2</jats:sub>\n                    SbCl\n                    <jats:sub>5</jats:sub>\n                    (CP1) and (BPP)\n                    <jats:sub>2</jats:sub>\n                    SbCl\n                    <jats:sub>5</jats:sub>\n                    0.5 H\n                    <jats:sub>2</jats:sub>\n                    O (CP2), which have very similar crystal structures except the introduction of water molecules in CP2. Experimental and theoretical results reveal that CP2 has larger lattice distortion and smaller freedom of motion, which can promote the self-trapped excitons emissions. A flexible scintillator screen based on CP2 crystals was prepared and applied for real-time dynamic and 3D X-ray imaging, which is the first time for Sb\n                    <jats:sup>3+</jats:sup>\n                    -based OIHH scintillators and significantly broadens the potential of Sb\n                    <jats:sup>3+</jats:sup>\n                    -based OIHH scintillators.\n                  </jats:p>",
        "journal": "Light: Science & Applications",
        "title_cn": "用于实时动态和 3D X 射线成像的高发光有机-无机混合卤化锑闪烁体",
        "abstract_cn": "<jats:title>摘要</jats:title>\n                  <贾茨：p>\n                    实时动态和三维 (3D) X 射线成像是 X 射线成像技术中最具挑战性的类型，对闪烁体提出了更严格的标准。铅基（Pb\n                    <贾茨：sup>2+</贾茨：sup>\n                    ）具有高X射线吸收系数的有机-无机杂化卤化物（OIHH）闪烁体已被证明具有优异的闪烁性能。然而，它们的毒性和不稳定性阻碍了进一步的发展，有必要探索具有优异闪烁性能的新型低毒金属基OIHH。锑基（Sb\n                    <贾茨：sup>3+</贾茨：sup>\n                    ）OIHHs不仅环保，而且与Pb相比表现出良好的稳定性\n                    <贾茨：sup>2+</贾茨：sup>\n                    基于 OIHH，这使它们成为优秀闪烁体的有希望的候选者。目前，对Sb的认识\n                    <贾茨：sup>3+</贾茨：sup>\n                    用于 X 射线探测和成像的基于 OIHH 闪烁体仍处于起步阶段，需要进一步探索。在此，我们设计了两个Sb\n                    <贾茨：sup>3+</贾茨：sup>\n                    基于 (BPP) 的 OIHH 晶体\n                    <贾茨：子>2</贾茨：子>\n                    氯化锑\n                    <贾茨：子>5</贾茨：子>\n                    (CP1) 和 (BPP)\n                    <贾茨：子>2</贾茨：子>\n                    氯化锑\n                    <贾茨：子>5</贾茨：子>\n                    0.5小时\n                    <贾茨：子>2</贾茨：子>\n                    O(CP2)，除了CP2中引入了水分子之外，它们具有非常相似的晶体结构。实验和理论结果表明，CP2具有较大的晶格畸变和较小的运动自由度，可以促进自陷激子发射。制备了基于CP2晶体的柔性闪烁体屏并应用于实时动态和3D X射线成像，这在Sb领域尚属首次\n                    <贾茨：sup>3+</贾茨：sup>\n                    基于 OIHH 闪烁体并显着拓宽了 Sb 的潜力\n                    <贾茨：sup>3+</贾茨：sup>\n                    基于 OIHH 闪烁体。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1038/s41377-025-02159-4",
        "title": "Sensitizing effect of lanthanide luminescence by Mo4+/Ag+ in double perovskites: great enhancement of near-infrared emission via wide range of excitation (250–850 nm)",
        "link": "https://doi.org/10.1038/s41377-025-02159-4",
        "published": "2026-01-26",
        "author": "Yingsheng Wang, Peipei Dang, Zixun Zeng, Dongjie Liu, Guodong Zhang, Long Tian, Kai Li, Ping’an Ma, Yi Wei, Hongzhou Lian, Zhiyao Hou, Guogang Li, Jun Lin",
        "summary": "<jats:title>Abstract</jats:title>\n                  <jats:p>\n                    Lead-free halide double perovskites (LFHDPs) have gained prominence as eco-friendly optoelectronic materials due to their structural stability and flexible tunability. Lanthanide (Ln\n                    <jats:sup>3+</jats:sup>\n                    ) ions have rich energy levels, which can endow LFHDP materials with emissions ranging from visible to near-infrared (NIR) region through the ion doping strategy. However, their NIR applications remain limited by narrowband emission and low photoluminescence quantum yield (PLQY) due to weak absorption cross-section. Herein, Cs\n                    <jats:sub>2</jats:sub>\n                    NaInCl\n                    <jats:sub>6</jats:sub>\n                    :Ln\n                    <jats:sup>3+</jats:sup>\n                    were successfully synthesized, and the problem of low absorption of Ln\n                    <jats:sup>3+</jats:sup>\n                    ions is effectively solved. Incorporating Mo\n                    <jats:sup>4+</jats:sup>\n                    /Ag\n                    <jats:sup>+</jats:sup>\n                    ions achieves a near-unity PLQY and expands the excitation spectrum across the full visible range and a small part of NIR region (250–850 nm). Mechanism analysis revealed synergistic energy transfer pathways involving self-trapping excitons and intermediate energy states of Mo\n                    <jats:sup>4+</jats:sup>\n                    ion, enhancing both photon absorption and PLQY. The universal applicability of this approach has been validated across Bi-based and multiple lanthanide ions (Ln: Ho, Er, Tm, Yb). These optimized materials demonstrate exceptional broadband emission characteristics suitable for multi-scenario NIR applications, including light-emitting-diodes (LEDs), night vision, imaging, anti-counterfeiting technologies. This co-doping methodology establishes a versatile framework for overcoming inherent limitations in Ln\n                    <jats:sup>3+</jats:sup>\n                    -activated materials, offering new possibilities for efficient NIR optoelectronic devices.\n                  </jats:p>",
        "journal": "Light: Science & Applications",
        "title_cn": "双钙钛矿中Mo4+/Ag+对镧系元素发光的敏化效应：通过宽激发范围（250–850 nm）极大增强近红外发射",
        "abstract_cn": "<jats:title>摘要</jats:title>\n                  <贾茨：p>\n                    无铅卤化物双钙钛矿（LFHDP）由于其结构稳定性和灵活的可调性而作为环保光电材料而受到关注。镧系元素（Ln\n                    <贾茨：sup>3+</贾茨：sup>\n                    ）离子具有丰富的能级，通过离子掺杂策略可以赋予LFHDP材料从可见光到近红外（NIR）区域的发射范围。然而，由于吸收截面较弱，它们的近红外应用仍然受到窄带发射和低光致发光量子产率（PLQY）的限制。在此，Cs\n                    <贾茨：子>2</贾茨：子>\n                    氯化钠\n                    <贾茨：子>6</贾茨：子>\n                    :Ln\n                    <贾茨：sup>3+</贾茨：sup>\n                    成功合成，解决了Ln吸收率低的问题\n                    <贾茨：sup>3+</贾茨：sup>\n                    离子得到有效解决。掺入钼\n                    <贾茨：sup>4+</贾茨：sup>\n                    /银\n                    <贾茨：sup>+</贾茨：sup>\n                    离子实现了接近一致的 PLQY 并将激发光谱扩展到整个可见光范围和一小部分 NIR 区域 (250–850nm)。机理分析揭示了涉及自俘获激子和Mo中间能态的协同能量转移途径\n                    <贾茨：sup>4+</贾茨：sup>\n                    离子，增强光子吸收和 PLQY。该方法的普遍适用性已在 Bi 基和多种镧系离子（Ln：Ho、Er、Tm、Yb）中得到验证。这些优化的材料表现出卓越的宽带发射特性，适用于多场景近红外应用，包括发光二极管 (LED)、夜视、成像、防伪技术。这种共掺杂方法建立了一个通用框架，用于克服 Ln 的固有局限性\n                    <贾茨：sup>3+</贾茨：sup>\n                    -活性材料，为高效近红外光电器件提供了新的可能性。\n                  </贾茨：p>"
    },
    {
        "id": "https://doi.org/10.1364/optica.586100",
        "title": "Quantum Geometric Limit for Directional Nonreciprocity in Thin-Film Thermal Emitters",
        "link": "https://doi.org/10.1364/optica.586100",
        "published": "2026-01-26",
        "author": "Satoru Konabe",
        "summary": "Abstract not available.",
        "journal": "Optica",
        "title_cn": "薄膜热发射体中方向非互易性的量子几何极限",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.586431",
        "title": "Optical active perception with 3D Gaussian splatting enables autonomous instrument insertion in robotic vitreoretinal surgery",
        "link": "https://doi.org/10.1364/ol.586431",
        "published": "2026-01-26",
        "author": "Qi Lan, Zheng Li, Haoran Zhang, Jianlong Yang",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "借助 3D 高斯散射的光学主动感知可实现机器人玻璃体视网膜手术中的自主器械插入",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.581745",
        "title": "Low-Angular-Dependence Dynamic Structural Colors Enabled by Sb2S3 Phase Change Material",
        "link": "https://doi.org/10.1364/ol.581745",
        "published": "2026-01-26",
        "author": "Haotong Wang, Min Lu, Xiaotan Ji, Yuheng Jin, Jiawei Zhang, Xiaoruo Yan, Weifan Lai, Mingyun Li, Tianle Yang, nan chen, Yikun Bu",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "Sb2S3 相变材料实现低角度依赖性动态结构颜色",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.585539",
        "title": "Forward scanning Endoscopic optical coherence elastography for elasticity measurements of anisotropic tissues",
        "link": "https://doi.org/10.1364/ol.585539",
        "published": "2026-01-26",
        "author": "Chongyang Wang, Pengbo Wang, xiaochen meng, Jiawei Ma, Zongqing Ma, Fan Fan, Jiang Zhu",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "前向扫描内窥镜光学相干弹性成像用于各向异性组织的弹性测量",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.585897",
        "title": "Optical Spin Hall Effect of Polariton Condensation in Organic Crystal Microcavities",
        "link": "https://doi.org/10.1364/ol.585897",
        "published": "2026-01-26",
        "author": "Shida Feng, Jun Cui, Xiaohong Zhang, Bin Wang, Fulong Dong, Chunling Gu, Qing Liao, Xiao-Hui Zhao, Jia-Huan Ren",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "有机晶体微腔中极化子凝聚的光学自旋霍尔效应",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.584623",
        "title": "Highly reflective Cr/C based multilayer X-ray mirrors for the wavelength λ = 4.47 nm",
        "link": "https://doi.org/10.1364/ol.584623",
        "published": "2026-01-26",
        "author": "Roman Shaposhnikov, Vladimir Polkovnikov, Sergey Garakhin, Nikolay Chkhalo, Maria Barysheva",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "基于 Cr/C 的高反射多层 X 射线反射镜，波长 λ = 4.47 nm",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.583797",
        "title": "Efficient Generation of Quantum Light using Bound States in the Continuum in Silicon-Nanowire Slow Light Waveguides",
        "link": "https://doi.org/10.1364/ol.583797",
        "published": "2026-01-26",
        "author": "Runzhi Cao, TIANJIAO SUN, Chong Sheng, TIANYU ZHANG, JIAXUAN ZHOU, Shining Zhu, Hui Liu",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "利用硅纳米线慢光波导连续体中的束缚态有效生成量子光",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/ol.580595",
        "title": "An implementation of a nitrogen-vacancy micro-pillar array in diamond for wide-field quantum imaging application with improved sensitivity",
        "link": "https://doi.org/10.1364/ol.580595",
        "published": "2026-01-26",
        "author": "Gengyou Zhao, Kun Tang, kai yang, Bo Feng, Liangxue Gu, Shunming Zhu, Xiang Xiong, JIANDONG YE, Shulin Gu",
        "summary": "Abstract not available.",
        "journal": "Optics Letters",
        "title_cn": "金刚石中氮空位微柱阵列的实现，用于提高灵敏度的宽视场量子成像应用",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "https://doi.org/10.1364/josaa.584917",
        "title": "An inverse design method for generalized zero-étendue sources and two targets",
        "link": "https://doi.org/10.1364/josaa.584917",
        "published": "2026-01-26",
        "author": "Pieter Braam, Jan ten Thije Boonkkamp, Martijn Anthonissen, Koondanibha Mitra, Lisa Kusch, Wilbert IJzerman",
        "summary": "Abstract not available.",
        "journal": "J. Opt. Soc. Am. A",
        "title_cn": "广义零光展量源和两个目标的逆设计方法",
        "abstract_cn": "（摘要暂缺，等待官方补全）"
    },
    {
        "id": "http://arxiv.org/abs/2601.17201v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17201v1",
        "title": "Remote dispersion scan: transformer-network retrieval of ultrafast pulses after non-linear propagation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Remote dispersion scan: transformer-network retrieval of ultrafast pulses after non-linear propagation"
        },
        "updated": "2026-01-23T22:15:45Z",
        "updated_parsed": [
            2026,
            1,
            23,
            22,
            15,
            45,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17201v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17201v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate and rapid characterization of broadband electric fields is essential for all ultrafast applications and remains an active field of research. In this work, we introduce remote dispersion scan, a transformer neural network enabled dispersion scan based pulse characterization method that can characterize femtosecond laser pulses. A local scan of the non-linear spectral phase before several linear and nonlinear processes, including amplification, compression, and self phase modulation, allows for the field retrieval remotely at the interaction region. We show that the reconstruction accuracy obtained from a single measurement of the fundamental and second harmonic is comparable to that of a full two dimensional scan. We confirm the technique experimentally by compressing a 300 W, 1.3 ps, 1030 nm pulse in a hollow core fiber to 100 fs and measuring the fundamental and second harmonic spectra while scanning the second order phase in a pulse shaper before power amplification. These results establish a simple, robust, alignment insensitive live-view pulse reconstruction modality.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate and rapid characterization of broadband electric fields is essential for all ultrafast applications and remains an active field of research. In this work, we introduce remote dispersion scan, a transformer neural network enabled dispersion scan based pulse characterization method that can characterize femtosecond laser pulses. A local scan of the non-linear spectral phase before several linear and nonlinear processes, including amplification, compression, and self phase modulation, allows for the field retrieval remotely at the interaction region. We show that the reconstruction accuracy obtained from a single measurement of the fundamental and second harmonic is comparable to that of a full two dimensional scan. We confirm the technique experimentally by compressing a 300 W, 1.3 ps, 1030 nm pulse in a hollow core fiber to 100 fs and measuring the fundamental and second harmonic spectra while scanning the second order phase in a pulse shaper before power amplification. These results establish a simple, robust, alignment insensitive live-view pulse reconstruction modality."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T22:15:45Z",
        "published_parsed": [
            2026,
            1,
            23,
            22,
            15,
            45,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Kevin Watson"
            },
            {
                "name": "Yutong Geng"
            },
            {
                "name": "Tobias Saule"
            },
            {
                "name": "Thomas Weinacht"
            },
            {
                "name": "Carlos A. Trallero-Herrero"
            }
        ],
        "author_detail": {
            "name": "Carlos A. Trallero-Herrero"
        },
        "author": "Carlos A. Trallero-Herrero",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "远程色散扫描：非线性传播后超快脉冲的变压器网络检索",
        "abstract_cn": "准确、快速地表征宽带电场对于所有超快应用至关重要，并且仍然是一个活跃的研究领域。在这项工作中，我们介绍了远程色散扫描，这是一种基于变压器神经网络的色散扫描脉冲表征方法，可以表征飞秒激光脉冲。在几个线性和非线性过程（包括放大、压缩和自相位调制）之前对非线性光谱相位进行局部扫描，允许在相互作用区域进行远程场检索。我们表明，从基波和二次谐波的单次测量获得的重建精度与全二维扫描的重建精度相当。我们通过实验将空心光纤中的 300 W、1.3 ps、1030 nm 脉冲压缩至 100 fs，并在功率放大之前在脉冲整形器中扫描二阶相位时测量基波和二次谐波频谱，从而通过实验证实了该技术。这些结果建立了一种简单、稳健、对准不敏感的实时查看脉冲重建模式。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17266v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17266v1",
        "title": "Exceptional-point-like Sensing near Hermitian Critical Points",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Exceptional-point-like Sensing near Hermitian Critical Points"
        },
        "updated": "2026-01-24T02:41:56Z",
        "updated_parsed": [
            2026,
            1,
            24,
            2,
            41,
            56,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17266v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17266v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "A non-Hermitian system at an exceptional point (EP), a specific critical point (CP) associated with the parity-time symmetric phase transition, exhibits a sublinear response to perturbation and promise unprecedented sensitivity beyond the linear-response Hermitian sensors, so far operating at the diabolic points (DP). Despite great advancements, its sensitivity enhancement is fundamentally limited by the divergent Petermann factor, intrinsically rooted in the non-Hermitian eigenvector degeneracy, and practically by the system complexity. Here, we report the CP-resulting square-root response to the refractive index change and enhanced sensitivity in a simple chiral Hermitian cavity without phase transitions. Because of the inherent eigenvector orthogonality, this CP-based Hermitian sensor exhibits an EP-like response and enhanced sensitivity, breaking the Petermann-factor limit of sensitivity in non-Hermitian counterparts. This work paves the way towards exploring the Hermitian CPs for ultrasensitive sensing outperforming both the EP- and DP-based sensors.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A non-Hermitian system at an exceptional point (EP), a specific critical point (CP) associated with the parity-time symmetric phase transition, exhibits a sublinear response to perturbation and promise unprecedented sensitivity beyond the linear-response Hermitian sensors, so far operating at the diabolic points (DP). Despite great advancements, its sensitivity enhancement is fundamentally limited by the divergent Petermann factor, intrinsically rooted in the non-Hermitian eigenvector degeneracy, and practically by the system complexity. Here, we report the CP-resulting square-root response to the refractive index change and enhanced sensitivity in a simple chiral Hermitian cavity without phase transitions. Because of the inherent eigenvector orthogonality, this CP-based Hermitian sensor exhibits an EP-like response and enhanced sensitivity, breaking the Petermann-factor limit of sensitivity in non-Hermitian counterparts. This work paves the way towards exploring the Hermitian CPs for ultrasensitive sensing outperforming both the EP- and DP-based sensors."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T02:41:56Z",
        "published_parsed": [
            2026,
            1,
            24,
            2,
            41,
            56,
            5,
            24,
            0
        ],
        "arxiv_comment": "23 pages, 10 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jiang-Shan Tang"
            },
            {
                "name": "Long-Qi Xiao"
            },
            {
                "name": "Hao-Dong Wu"
            },
            {
                "name": "Yuwei Jing"
            },
            {
                "name": "Han Zhang"
            },
            {
                "name": "Ya-Ping Ruan"
            },
            {
                "name": "Wuming Liu"
            },
            {
                "name": "Yan-Qing Lu"
            },
            {
                "name": "Keyu Xia"
            }
        ],
        "author_detail": {
            "name": "Keyu Xia"
        },
        "author": "Keyu Xia",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "厄米临界点附近的异常点状传感",
        "abstract_cn": "处于特殊点 (EP) 的非厄米系统，即与奇偶时间对称相变相关的特定临界点 (CP)，表现出对扰动的次线性响应，并有望超越迄今为止在恶魔点 (DP) 运行的线性响应厄米传感器。尽管取得了巨大进步，但其灵敏度的增强从根本上受到发散彼得曼因子的限制，本质上植根于非厄米特征向量简并性，并且实际上受到系统复杂性的限制。在这里，我们报告了在没有相变的简单手性厄米特腔中，CP 产生的对折射率变化的平方根响应和增强的灵敏度。由于固有的特征向量正交性，这种基于 CP 的厄米特传感器表现出类似 EP 的响应和增强的灵敏度，突破了非厄米特传感器的彼得曼因子灵敏度限制。这项工作为探索 Hermitian CP 的超灵敏传感铺平了道路，其性能优于基于 EP 和 DP 的传感器。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17289v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17289v1",
        "title": "Enhancing Volumetric Optical Chirality through 2D-3D Structural Design Evolution",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Enhancing Volumetric Optical Chirality through 2D-3D Structural Design Evolution"
        },
        "updated": "2026-01-24T03:57:05Z",
        "updated_parsed": [
            2026,
            1,
            24,
            3,
            57,
            5,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17289v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17289v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Circular dichroism (CD) sensing plays a pivotal role in probing molecular chirality in biomedical sciences. However, engineering superchiral electromagnetic fields that can reliably amplify the faint signatures of chiral analytes remains profoundly challenging. Central to this difficulty is the need to balance two competing demands: maximizing the enhancement of chiral fields while maintaining a sufficiently large interaction volume for effective molecular interrogation. Here, we introduce a figure of merit (FOM) that captures the enhancement and spatial coverage of superchiral fields to benchmark different chiral-field configurations. We examine the effects of helix-geometry evolution on the FOM, including 2D to 3D chirality induction, winding-number escalation, helical-order enhancement, and transverse dilation. By tuning these structural degrees of freedom, the sensing volume can be enlarged without compromising the distribution and enhancement strength of fields. The optimized triple-strand helix markedly enhanced the analyte CD signal, yielding a FOM of 2.43*10^10 nm3, which surpassed prior 2D and 3D configurations by over an order of magnitude. The proposed FOM exhibits a strong linear correlation (R^2 = 0.9256) with the analyte CD signal. Our findings provide a systematic design framework for 3D chiral structures and a robust metric for assessing their chiroptical sensing performance, particularly in scenarios involving clusters of randomly oriented small molecules or a large chiral molecule.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Circular dichroism (CD) sensing plays a pivotal role in probing molecular chirality in biomedical sciences. However, engineering superchiral electromagnetic fields that can reliably amplify the faint signatures of chiral analytes remains profoundly challenging. Central to this difficulty is the need to balance two competing demands: maximizing the enhancement of chiral fields while maintaining a sufficiently large interaction volume for effective molecular interrogation. Here, we introduce a figure of merit (FOM) that captures the enhancement and spatial coverage of superchiral fields to benchmark different chiral-field configurations. We examine the effects of helix-geometry evolution on the FOM, including 2D to 3D chirality induction, winding-number escalation, helical-order enhancement, and transverse dilation. By tuning these structural degrees of freedom, the sensing volume can be enlarged without compromising the distribution and enhancement strength of fields. The optimized triple-strand helix markedly enhanced the analyte CD signal, yielding a FOM of 2.43*10^10 nm3, which surpassed prior 2D and 3D configurations by over an order of magnitude. The proposed FOM exhibits a strong linear correlation (R^2 = 0.9256) with the analyte CD signal. Our findings provide a systematic design framework for 3D chiral structures and a robust metric for assessing their chiroptical sensing performance, particularly in scenarios involving clusters of randomly oriented small molecules or a large chiral molecule."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T03:57:05Z",
        "published_parsed": [
            2026,
            1,
            24,
            3,
            57,
            5,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Chia-Te Chang"
            },
            {
                "name": "Xiaoyan Zhou"
            },
            {
                "name": "Dmitrii Gromyko"
            },
            {
                "name": "John You En Chan"
            },
            {
                "name": "Lin Wu"
            },
            {
                "name": "Chia-Ming Yang"
            },
            {
                "name": "Sejeong Kim"
            },
            {
                "name": "Hongtao Wang"
            },
            {
                "name": "Joel K. W. Yang"
            }
        ],
        "author_detail": {
            "name": "Joel K. W. Yang"
        },
        "author": "Joel K. W. Yang",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过 2D-3D 结构设计演进增强体积光学手性",
        "abstract_cn": "圆二色性 (CD) 传感在探测生物医学中的分子手性方面发挥着关键作用。然而，设计能够可靠地放大手性分析物微弱特征的超手性电磁场仍然具有很大的挑战性。这一困难的核心是需要平衡两个相互竞争的需求：最大限度地增强手性场，同时保持足够大的相互作用体积以进行有效的分子询问。在这里，我们引入了一个品质因数（FOM），它捕获超手性场的增强和空间覆盖范围，以对不同的手性场配置进行基准测试。我们研究了螺旋几何演化对 FOM 的影响，包括 2D 到 3D 手性感应、绕数升级、螺旋阶数增强和横向膨胀。通过调整这些结构自由度，可以在不影响场的分布和增强强度的情况下扩大传感体积。优化的三链螺旋显着增强了分析物 CD 信号，产生 2.43*10^10 nm3 的 FOM，超出了之前的 2D 和 3D 配置一个数量级以上。所提出的 FOM 与分析物 CD 信号表现出很强的线性相关性 (R^2 = 0.9256)。我们的研究结果为 3D 手性结构提供了系统的设计框架，并为评估其手性光学传感性能提供了可靠的指标，特别是在涉及随机取向的小分子或大手性分子簇的情况下。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17369v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17369v1",
        "title": "Classical Petermann Factor as a Measure of Quantum Squeezing in Photonic Time Crystals",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Classical Petermann Factor as a Measure of Quantum Squeezing in Photonic Time Crystals"
        },
        "updated": "2026-01-24T08:27:25Z",
        "updated_parsed": [
            2026,
            1,
            24,
            8,
            27,
            25,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17369v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17369v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Photonic time crystals realize a continuum of momentum-resolved SU(1,1) parametric amplifiers. We show that a classical quantity, the Petermann factor of the effective Floquet Bogoliubov-de Gennes (BdG) dynamical matrix, sets the scale of their quantum noise. In stable bands it fixes the Bogoliubov mixing and the vacuum quasiparticle population, while in momentum gaps it sets the photon-number prefactor and enhances the squeezing dynamics, with the Floquet growth rate setting the time scale. This converts classical measurements of mode nonorthogonality into quantitative predictions for squeezing and photon generation, and offers a compact design parameter for engineering quantum resources in two-mode BdG platforms.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Photonic time crystals realize a continuum of momentum-resolved SU(1,1) parametric amplifiers. We show that a classical quantity, the Petermann factor of the effective Floquet Bogoliubov-de Gennes (BdG) dynamical matrix, sets the scale of their quantum noise. In stable bands it fixes the Bogoliubov mixing and the vacuum quasiparticle population, while in momentum gaps it sets the photon-number prefactor and enhances the squeezing dynamics, with the Floquet growth rate setting the time scale. This converts classical measurements of mode nonorthogonality into quantitative predictions for squeezing and photon generation, and offers a compact design parameter for engineering quantum resources in two-mode BdG platforms."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T08:27:25Z",
        "published_parsed": [
            2026,
            1,
            24,
            8,
            27,
            25,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Younsung Kim"
            },
            {
                "name": "Kyungmin Lee"
            },
            {
                "name": "Changhun Oh"
            },
            {
                "name": "Young-Sik Ra"
            },
            {
                "name": "Kun Woo Kim"
            },
            {
                "name": "Bumki Min"
            }
        ],
        "author_detail": {
            "name": "Bumki Min"
        },
        "author": "Bumki Min",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "经典彼得曼因子作为光子时间晶体中量子压缩的测量",
        "abstract_cn": "光子时间晶体实现了动量分辨 SU(1,1) 参量放大器的连续体。我们证明了一个经典量，即有效 Floquet Bogoliubov-de Gennes (BdG) 动力学矩阵的彼得曼因子，决定了它们的量子噪声的大小。在稳定带中，它修复了 Bogoliubov 混合和真空准粒子群，而在动量间隙中，它设置了光子数前因子并增强了挤压动力学，并用 Floquet 增长率设置了时间尺度。这将模式非正交性的经典测量转换为压缩和光子生成的定量预测，并为双模式 BdG 平台中的工程量子资源提供紧凑的设计参数。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17385v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17385v1",
        "title": "Suspended thin-film lithium niobate modulator for broadband mid-infrared light modulation and frequency comb generation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Suspended thin-film lithium niobate modulator for broadband mid-infrared light modulation and frequency comb generation"
        },
        "updated": "2026-01-24T09:17:29Z",
        "updated_parsed": [
            2026,
            1,
            24,
            9,
            17,
            29,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17385v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17385v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The mid-infrared (MIR) spectral regime is central to applications including remote sensing, precision spectroscopy, higher harmonic generation, and free-space optical communication. However, coherent and broadband MIR modulation remains challenging owing to high optical loss, limited bandwidth, and large drive voltages in existing platforms. Here, we overcome these challenges by deploying a suspended thin-film lithium-niobate (TFLN) based electro-optic (EO) platform co-designed with high-performance traveling-wave microwave (MW) electrodes. We demonstrate a record-low Vpi,DC of 2.3 to 4.3 V over a broadband MIR bandwidth from 2.4 to 3.6 um, and a 2.7 dB EO bandwidth of 40 GHz (extracted 3 dB bandwidth of 50 GHz), yielding a figure of merit of 17.4 GHz/V, more than an order of magnitude higher than the state of the art. We demonstrate, for the first time, high-frequency Vpi,MW of 4.5 to 6.5 V in the 25 to 35 GHz range, and frequency-agile MIR EO frequency comb generation with a 10 dB optical bandwidth over 0.8 THz using a suspended phase modulator of 4 cm active modulation length. We further validate the platform in a free-space optical communication link. Our results establish a monolithic MIR photonic platform capable of powerful EO modulation and spectral synthesis, and represent a significant step toward reconfigurable MIR sensing and communication systems on chip.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The mid-infrared (MIR) spectral regime is central to applications including remote sensing, precision spectroscopy, higher harmonic generation, and free-space optical communication. However, coherent and broadband MIR modulation remains challenging owing to high optical loss, limited bandwidth, and large drive voltages in existing platforms. Here, we overcome these challenges by deploying a suspended thin-film lithium-niobate (TFLN) based electro-optic (EO) platform co-designed with high-performance traveling-wave microwave (MW) electrodes. We demonstrate a record-low Vpi,DC of 2.3 to 4.3 V over a broadband MIR bandwidth from 2.4 to 3.6 um, and a 2.7 dB EO bandwidth of 40 GHz (extracted 3 dB bandwidth of 50 GHz), yielding a figure of merit of 17.4 GHz/V, more than an order of magnitude higher than the state of the art. We demonstrate, for the first time, high-frequency Vpi,MW of 4.5 to 6.5 V in the 25 to 35 GHz range, and frequency-agile MIR EO frequency comb generation with a 10 dB optical bandwidth over 0.8 THz using a suspended phase modulator of 4 cm active modulation length. We further validate the platform in a free-space optical communication link. Our results establish a monolithic MIR photonic platform capable of powerful EO modulation and spectral synthesis, and represent a significant step toward reconfigurable MIR sensing and communication systems on chip."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T09:17:29Z",
        "published_parsed": [
            2026,
            1,
            24,
            9,
            17,
            29,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Chun-Ho Lee"
            },
            {
                "name": "Xinyi Ren"
            },
            {
                "name": "Xinzhou Su"
            },
            {
                "name": "Wonho Lee"
            },
            {
                "name": "Zile Jiang"
            },
            {
                "name": "Yue Yu"
            },
            {
                "name": "Huibin Zhou"
            },
            {
                "name": "Yue Zuo"
            },
            {
                "name": "Shaoyuan Ou"
            },
            {
                "name": "Reshma Kopparapu"
            },
            {
                "name": "Adam T. Heiniger"
            },
            {
                "name": "Moshe Tur"
            },
            {
                "name": "Alan E. Willner"
            },
            {
                "name": "Zaijun Chen"
            },
            {
                "name": "Mengjie Yu"
            }
        ],
        "author_detail": {
            "name": "Mengjie Yu"
        },
        "author": "Mengjie Yu",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于宽带中红外光调制和频率梳生成的悬浮薄膜铌酸锂调制器",
        "abstract_cn": "中红外 (MIR) 光谱范围是遥感、精密光谱、高次谐波生成和自由空间光通信等应用的核心。然而，由于现有平台中的高光损耗、有限的带宽和大的驱动电压，相干和宽带 MIR 调制仍然具有挑战性。在这里，我们通过部署与高性能行波微波（MW）电极共同设计的基于悬浮薄膜铌酸锂（TFLN）的电光（EO）平台来克服这些挑战。我们在 2.4 至 3.6 um 的宽带 MIR 带宽上展示了 2.3 至 4.3 V 的创纪录低 Vpi,DC，以及 40 GHz 的 2.7 dB EO 带宽（提取 50 GHz 的 3 dB 带宽），产生 17.4 GHz/V 的品质因数，比现有技术高出一个数量级以上。我们首次展示了在 25 至 35 GHz 范围内 4.5 至 6.5 V 的高频 Vpi,MW，以及使用 4 cm 有效调制长度的悬浮相位调制器生成频率捷变 MIR EO 频率梳，在 0.8 THz 上具有 10 dB 光带宽。我们进一步在自由空间光通信链路中验证该平台。我们的研究结果建立了一个能够进行强大的电光调制和光谱合成的单片中红外光子平台，代表着朝着片上可重构中红外传感和通信系统迈出了重要一步。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17409v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17409v1",
        "title": "Transition Metal Dichalcogenides Multijunction Solar Cells Toward the Multicolor Limit",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Transition Metal Dichalcogenides Multijunction Solar Cells Toward the Multicolor Limit"
        },
        "updated": "2026-01-24T10:54:24Z",
        "updated_parsed": [
            2026,
            1,
            24,
            10,
            54,
            24,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17409v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17409v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Transition metal dichalcogenides (TMDs) and other van der Waals semiconductors enable transfer-printed, lattice-mismatch--free stacking of many photovoltaic junctions (N), motivating a re-examination of multijunction detailed-balance limits under realistic material and optical constraints. Here we develop an unlimited-junction detailed-balance framework for split-spectrum, multi-terminal vdW multijunction solar cells and apply it to a conservative TMD bandgap window (1.0-2.1 eV). Dynamic-programming optimization shows that the accessible bandgap window imposes a large-N efficiency limit: under full concentration, unconstrained ladders approach 84.5% at N=50, whereas the TMD window plateaus near 63.4%. This plateau is set by photons outside the gaps, so radiative quality and optics dominate beyond five junctions for realistic transfer-printed stacks. We identify an experimentally achievable N=5 ladder Eg~(2.10,1.78,1.50,1.24,1.00) eV and map each rung to candidate vdW/TMD absorbers. Using reciprocity and luminescence thermodynamics, we quantify penalties from finite external radiative efficiency, two-sided emission, and luminescent coupling, and we introduce the upward-emitted luminescence power as a computable entropy-loss proxy. Incorporating excitonic absorptance and nanophotonic thickness bounds yields practical thickness and light-management targets for transfer-printed stacks. Finally, inserting an idealized nonreciprocal multijunction model into the reciprocity-optimized ladders provides conservative headroom estimates, consistent with negligible benefit for single junctions but measurable gains for multijunction TMD stacks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Transition metal dichalcogenides (TMDs) and other van der Waals semiconductors enable transfer-printed, lattice-mismatch--free stacking of many photovoltaic junctions (N), motivating a re-examination of multijunction detailed-balance limits under realistic material and optical constraints. Here we develop an unlimited-junction detailed-balance framework for split-spectrum, multi-terminal vdW multijunction solar cells and apply it to a conservative TMD bandgap window (1.0-2.1 eV). Dynamic-programming optimization shows that the accessible bandgap window imposes a large-N efficiency limit: under full concentration, unconstrained ladders approach 84.5% at N=50, whereas the TMD window plateaus near 63.4%. This plateau is set by photons outside the gaps, so radiative quality and optics dominate beyond five junctions for realistic transfer-printed stacks. We identify an experimentally achievable N=5 ladder Eg~(2.10,1.78,1.50,1.24,1.00) eV and map each rung to candidate vdW/TMD absorbers. Using reciprocity and luminescence thermodynamics, we quantify penalties from finite external radiative efficiency, two-sided emission, and luminescent coupling, and we introduce the upward-emitted luminescence power as a computable entropy-loss proxy. Incorporating excitonic absorptance and nanophotonic thickness bounds yields practical thickness and light-management targets for transfer-printed stacks. Finally, inserting an idealized nonreciprocal multijunction model into the reciprocity-optimized ladders provides conservative headroom estimates, consistent with negligible benefit for single junctions but measurable gains for multijunction TMD stacks."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T10:54:24Z",
        "published_parsed": [
            2026,
            1,
            24,
            10,
            54,
            24,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Seungwoo Lee"
            }
        ],
        "author_detail": {
            "name": "Seungwoo Lee"
        },
        "author": "Seungwoo Lee",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "过渡金属二硫化物多结太阳能电池迈向多色极限",
        "abstract_cn": "过渡金属二硫属化物 (TMD) 和其他范德华半导体能够实现许多光伏结 (N) 的转印、无晶格失配堆叠，从而激发了在实际材料和光学约束下重新检查多结详细平衡限制。在这里，我们开发了一种用于分谱、多端 vdW 多结太阳能电池的无限结详细平衡框架，并将其应用于保守的 TMD 带隙窗口（1.0-2.1 eV）。动态编程优化表明，可访问的带隙窗口施加了大 N 的效率限制：在完全浓度下，无约束梯在 N=50 时接近 84.5%，而 TMD 窗口稳定在 63.4% 附近。该平台是由间隙外部的光子设置的，因此辐射质量和光学在真实的转移印刷堆栈的五个结点之外占主导地位。我们确定了一个实验上可实现的 N=5 阶梯 Eg~(2.10,1.78,1.50,1.24,1.00) eV 并将每个梯级映射到候选 vdW/TMD 吸收体。利用互易性和发光热力学，我们量化了有限外部辐射效率、两侧发射和发光耦合的损失，并引入了向上发射的发光功率作为可计算的熵损失代理。结合激子吸收率和纳米光子厚度界限，为转移印刷叠层产生实用的厚度和光管理目标。最后，将理想化的非互易多结模型插入到互易优化梯中可以提供保守的余量估计，这与单结的可忽略不计的收益一致，但多结 TMD 堆栈的可测量收益一致。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17496v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17496v1",
        "title": "Emission of nitrogen-vacancy centers in diamond shaped by topological photonic waveguide modes",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Emission of nitrogen-vacancy centers in diamond shaped by topological photonic waveguide modes"
        },
        "updated": "2026-01-24T15:46:25Z",
        "updated_parsed": [
            2026,
            1,
            24,
            15,
            46,
            25,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17496v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17496v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1038/s41565-025-02001-3",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "As the ability to integrate single photon emitters into photonic architectures improves, so does the need to characterize and understand their interaction. Here, we use a scanning diamond nanocrystal to investigate the interplay between the emission of room-temperature nitrogen-vacancy (NV) centers and a proximal topological waveguide. In our experiments, NVs serve as local, spectrally broad light sources which we exploit to characterize the waveguide bandwidth as well as the correspondence between light injection site and directionality of wave propagation. Further, we find that near-field coupling to the waveguide influences the spectral shape and ellipticity of the NV photoluminescence, hence allowing us to reveal nanostructured light fields with a spatial resolution defined by the nanoparticle size. Our results expand on the sensing modalities afforded by color centers, and portend novel opportunities in the development of on-chip, quantum optics devices leveraging topological photonics to best manipulate and readout single-photon emitters.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "As the ability to integrate single photon emitters into photonic architectures improves, so does the need to characterize and understand their interaction. Here, we use a scanning diamond nanocrystal to investigate the interplay between the emission of room-temperature nitrogen-vacancy (NV) centers and a proximal topological waveguide. In our experiments, NVs serve as local, spectrally broad light sources which we exploit to characterize the waveguide bandwidth as well as the correspondence between light injection site and directionality of wave propagation. Further, we find that near-field coupling to the waveguide influences the spectral shape and ellipticity of the NV photoluminescence, hence allowing us to reveal nanostructured light fields with a spatial resolution defined by the nanoparticle size. Our results expand on the sensing modalities afforded by color centers, and portend novel opportunities in the development of on-chip, quantum optics devices leveraging topological photonics to best manipulate and readout single-photon emitters."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T15:46:25Z",
        "published_parsed": [
            2026,
            1,
            24,
            15,
            46,
            25,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "arxiv_journal_ref": "Nat. Nanotech. 20, 1605 (2025)",
        "authors": [
            {
                "name": "Raman Kumar"
            },
            {
                "name": "Chandan"
            },
            {
                "name": "Gabriel I. López Morales"
            },
            {
                "name": "Richard Monge"
            },
            {
                "name": "Anton Vakulenko"
            },
            {
                "name": "Svetlana Kiriushechkina"
            },
            {
                "name": "Alexander B. Khanikaev"
            },
            {
                "name": "Johannes Flick"
            },
            {
                "name": "Carlos A. Meriles"
            }
        ],
        "author_detail": {
            "name": "Carlos A. Meriles"
        },
        "author": "Carlos A. Meriles",
        "arxiv_doi": "10.1038/s41565-025-02001-3",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "拓扑光子波导模式下金刚石中氮空位中心的发射",
        "abstract_cn": "随着将单光子发射器集成到光子架构中的能力不断提高，表征和理解它们相互作用的需求也随之提高。在这里，我们使用扫描金刚石纳米晶体来研究室温氮空位（NV）中心的发射与近端拓扑波导之间的相互作用。在我们的实验中，NV 用作局部光谱宽光源，我们利用它来表征波导带宽以及光注入位置和波传播方向性之间的对应关系。此外，我们发现波导的近场耦合会影响 NV 光致发光的光谱形状和椭圆率，因此使我们能够揭示具有由纳米颗粒尺寸定义的空间分辨率的纳米结构光场。我们的研究结果扩展了色心提供的传感模式，并预示着开发片上量子光学器件的新机会，利用拓扑光子学来最好地操纵和读出单光子发射器。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17547v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17547v1",
        "title": "Plasma Decay of Nanosecond Pulsed Laser-Produced Ar and Ar-H2O Sparks at Atmospheric Pressure",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Plasma Decay of Nanosecond Pulsed Laser-Produced Ar and Ar-H2O Sparks at Atmospheric Pressure"
        },
        "updated": "2026-01-24T18:34:57Z",
        "updated_parsed": [
            2026,
            1,
            24,
            18,
            34,
            57,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17547v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17547v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Time-resolved diagnostics were applied to investigate free-electron properties in nanosecond laser-produced discharges sustained at atmospheric pressure in Ar and in Ar with 3% H2O. The discharges were generated using 23 ns, 1064 nm laser pulses. Broadband plasma imaging and laser Thomson scattering were combined with optical emission spectroscopy, with particular emphasis on Stark broadening of the Halpha and Hbeta lines. The plasma exhibited a bright emission that persisted for up to 30--40 us after breakdown, followed by a very weak glow lasting up to 19 ms. Peak electron number density of about 2 x 10^17 cm-3 and electron temperature of about 7 eV were measured. Excellent agreement between both techniques was obtained for absolute electron number densities. The inferred temporal decay of free electrons is consistent with processes dominated by ambipolar expansion and two- and three-body electron-ion recombination. These results provide benchmark data for modeling nanosecond laser discharges and demonstrate the reliability of combining Thomson scattering with Stark broadening in atmospheric laser sparks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Time-resolved diagnostics were applied to investigate free-electron properties in nanosecond laser-produced discharges sustained at atmospheric pressure in Ar and in Ar with 3% H2O. The discharges were generated using 23 ns, 1064 nm laser pulses. Broadband plasma imaging and laser Thomson scattering were combined with optical emission spectroscopy, with particular emphasis on Stark broadening of the Halpha and Hbeta lines. The plasma exhibited a bright emission that persisted for up to 30--40 us after breakdown, followed by a very weak glow lasting up to 19 ms. Peak electron number density of about 2 x 10^17 cm-3 and electron temperature of about 7 eV were measured. Excellent agreement between both techniques was obtained for absolute electron number densities. The inferred temporal decay of free electrons is consistent with processes dominated by ambipolar expansion and two- and three-body electron-ion recombination. These results provide benchmark data for modeling nanosecond laser discharges and demonstrate the reliability of combining Thomson scattering with Stark broadening in atmospheric laser sparks."
        },
        "tags": [
            {
                "term": "physics.plasm-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T18:34:57Z",
        "published_parsed": [
            2026,
            1,
            24,
            18,
            34,
            57,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.plasm-ph"
        },
        "authors": [
            {
                "name": "Ji Yung Ahn"
            },
            {
                "name": "Jianan Wang"
            },
            {
                "name": "Tasnim Akbar Faruquee"
            },
            {
                "name": "Marien Simeni Simeni"
            }
        ],
        "author_detail": {
            "name": "Marien Simeni Simeni"
        },
        "author": "Marien Simeni Simeni",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "大气压下纳秒脉冲激光产生的 Ar 和 Ar-H2O 火花的等离子体衰变",
        "abstract_cn": "应用时间分辨诊断来研究纳秒激光产生的放电中的自由电子特性，这些放电在大气压下的氩气和含 3% H2O 的氩气中持续。放电是使用 23 ns、1064 nm 激光脉冲产生的。宽带等离子体成像和激光汤姆逊散射与光学发射光谱相结合，特别强调 Halpha 和 Hbeta 谱线的 Stark 展宽。等离子体表现出明亮的发射，在击穿后持续长达 30--40 us，随后是持续长达 19 ms 的非常微弱的辉光。测得峰值电子数密度约为 2 x 10^17 cm-3，电子温度约为 7 eV。两种技术在绝对电子数密度方面获得了极好的一致性。推断的自由电子的时间衰变与双极膨胀和二体和三体电子-离子复合主导的过程一致。这些结果为纳秒激光放电建模提供了基准数据，并证明了将汤姆逊散射与大气激光火花中的斯塔克展宽相结合的可靠性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17653v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17653v1",
        "title": "Dual Flat-Bands of Bound State in the Continuum and Radiative Mode via TE-TM Coupling",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Dual Flat-Bands of Bound State in the Continuum and Radiative Mode via TE-TM Coupling"
        },
        "updated": "2026-01-25T02:15:44Z",
        "updated_parsed": [
            2026,
            1,
            25,
            2,
            15,
            44,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17653v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17653v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "A general symmetry-controlled mechanism is proposed for realizing dual flat-bands of bound state in the continuum (BIC) and its radiative counterpart in photonic crystal slabs. By breaking the vertical mirror symmetry of slab, inter-polarization coupling between TE-like and TM-like modes is activated, while intra-polarization coupling among modes within the same polarization class is simultaneously preserved. The cooperative action of these two coupling channels gives rise to the concurrent flattening of both the BIC-hosting band and the radiative band, resulting in a dual flat-band system with strongly contrasting quality (Q) factors. An effective two-step coupling model is constructed to capture the essential physics and show that the emergence of the flat bands is governed by geometric tuning rather than accidental degeneracies. The mechanism is shown to be generic with respect to polarization and material platform, enabling dual flat-band states in both low- and high-index systems, with substantially enhanced angular bandwidths in the latter. These finding establish a unified route for flat-band photonic engineering and provide a robust platform for angle-tolerant resonant photonic functionalities.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A general symmetry-controlled mechanism is proposed for realizing dual flat-bands of bound state in the continuum (BIC) and its radiative counterpart in photonic crystal slabs. By breaking the vertical mirror symmetry of slab, inter-polarization coupling between TE-like and TM-like modes is activated, while intra-polarization coupling among modes within the same polarization class is simultaneously preserved. The cooperative action of these two coupling channels gives rise to the concurrent flattening of both the BIC-hosting band and the radiative band, resulting in a dual flat-band system with strongly contrasting quality (Q) factors. An effective two-step coupling model is constructed to capture the essential physics and show that the emergence of the flat bands is governed by geometric tuning rather than accidental degeneracies. The mechanism is shown to be generic with respect to polarization and material platform, enabling dual flat-band states in both low- and high-index systems, with substantially enhanced angular bandwidths in the latter. These finding establish a unified route for flat-band photonic engineering and provide a robust platform for angle-tolerant resonant photonic functionalities."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T02:15:44Z",
        "published_parsed": [
            2026,
            1,
            25,
            2,
            15,
            44,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jiayao Liu"
            },
            {
                "name": "Zimeng Zeng"
            },
            {
                "name": "Zhuoyang Li"
            },
            {
                "name": "Zelong He"
            },
            {
                "name": "Zhaona Wang"
            }
        ],
        "author_detail": {
            "name": "Zhaona Wang"
        },
        "author": "Zhaona Wang",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过 TE-TM 耦合实现连续模式和辐射模式的束缚态双平带",
        "abstract_cn": "提出了一种通用的对称控制机制，用于实现连续体中束缚态的双平带（BIC）及其在光子晶体板中的辐射对应物。通过打破板条的垂直镜像对称性，激活类 TE 和类 TM 模式之间的偏振间耦合，同时保留同一偏振类别内模式之间的偏振内耦合。这两个耦合通道的协同作用导致 BIC 托管频带和辐射频带同时平坦化，从而形成具有强烈对比质量 (Q) 因子的双平坦带系统。构建了有效的两步耦合模型来捕获基本物理现象，并表明平带的出现是由几何调谐而不是偶然的简并决定的。该机制被证明在偏振和材料平台方面是通用的，可在低折射率和高折射率系统中实现双平带状态，并在后者中显着增强角带宽。这些发现为平带光子工程建立了统一的路线，并为容角谐振光子功能提供了强大的平台。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17694v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17694v1",
        "title": "Residual neural-field ptychography for dose-efficient electron, X-ray, and optical nanoscopy",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Residual neural-field ptychography for dose-efficient electron, X-ray, and optical nanoscopy"
        },
        "updated": "2026-01-25T04:50:13Z",
        "updated_parsed": [
            2026,
            1,
            25,
            4,
            50,
            13,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17694v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17694v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Ptychography spans from sub-angstrom to meter scales yet suffers from convergence instability and excessive data redundancy. Here we introduce self-correcting residual neural fields as a dose-efficient framework for electron, X-ray, and optical ptychography. Unlike approaches that split complex fields, our complex-valued architecture employs holomorphic phasor activation e^iωz to preserve intrinsic phase-amplitude coupling. We reformulate reconstruction as residual learning, where the network learns only corrections to physical priors rather than complete wavefields. By embedding the physical model as a differentiable layer within the network, we enable end-to-end automatic differentiation where experimental parameters are jointly corrected alongside the neural fields. We validate our scheme across conventional, near-field, coded, and Fourier ptychography and achieve record-breaking lensless resolution of 244-nm linewidth with visible light. Extending to electron wavelengths, we reveal synaptic connectivity in brain sections with superior performance over conventional approaches. Our framework provides a solution for high-throughput, dose-efficient nanoscopy across the electromagnetic spectrum.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Ptychography spans from sub-angstrom to meter scales yet suffers from convergence instability and excessive data redundancy. Here we introduce self-correcting residual neural fields as a dose-efficient framework for electron, X-ray, and optical ptychography. Unlike approaches that split complex fields, our complex-valued architecture employs holomorphic phasor activation e^iωz to preserve intrinsic phase-amplitude coupling. We reformulate reconstruction as residual learning, where the network learns only corrections to physical priors rather than complete wavefields. By embedding the physical model as a differentiable layer within the network, we enable end-to-end automatic differentiation where experimental parameters are jointly corrected alongside the neural fields. We validate our scheme across conventional, near-field, coded, and Fourier ptychography and achieve record-breaking lensless resolution of 244-nm linewidth with visible light. Extending to electron wavelengths, we reveal synaptic connectivity in brain sections with superior performance over conventional approaches. Our framework provides a solution for high-throughput, dose-efficient nanoscopy across the electromagnetic spectrum."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.comp-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T04:50:13Z",
        "published_parsed": [
            2026,
            1,
            25,
            4,
            50,
            13,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Qianhao Zhao"
            },
            {
                "name": "Zhixuan Hong"
            },
            {
                "name": "Ruihai Wang"
            },
            {
                "name": "Tianbo Wang"
            },
            {
                "name": "Lingzhi Jiang"
            },
            {
                "name": "Qiong Ma"
            },
            {
                "name": "Peng-Han Lu"
            },
            {
                "name": "Rafal E. Dunin-Borkowski"
            },
            {
                "name": "Andrew Maiden"
            },
            {
                "name": "Guoan Zheng"
            }
        ],
        "author_detail": {
            "name": "Guoan Zheng"
        },
        "author": "Guoan Zheng",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于剂量有效的电子、X 射线和光学纳米镜的残余神经场叠层描记术",
        "abstract_cn": "叠印法的范围从亚埃到米尺度，但仍面临收敛不稳定和过多数据冗余的问题。在这里，我们引入自我校正残余神经场作为电子、X 射线和光学叠层描记术的剂量有效框架。与分割复杂场的方法不同，我们的复值架构采用全纯相量激活 e^iωz 来保持内在的相位幅度耦合。我们将重建重新表述为残差学习，其中网络仅学习对物理先验的校正，而不是完整的波场。通过将物理模型作为可微分层嵌入到网络中，我们可以实现端到端的自动微分，其中实验参数与神经场一起共同校正。我们在传统、近场、编码和傅立叶叠层成像中验证了我们的方案，并在可见光下实现了破纪录的 244 nm 线宽无透镜分辨率。扩展到电子波长，我们揭示了大脑切片中的突触连接，其性能优于传统方法。我们的框架为整个电磁频谱的高通量、剂量高效的纳米显微镜提供了解决方案。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17701v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17701v1",
        "title": "Metasurface-assisted balanced-injection synchronization for turbulence-resilient long-haul chaotic free-space link",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Metasurface-assisted balanced-injection synchronization for turbulence-resilient long-haul chaotic free-space link"
        },
        "updated": "2026-01-25T05:22:20Z",
        "updated_parsed": [
            2026,
            1,
            25,
            5,
            22,
            20,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17701v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17701v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Optical chaotic synchronization between coupled nonlinear lasers underpins most chaos-based applications, including complex laser network dynamics, secure communication, key distribution, and reinforcement learning. In free-space links, however, chaotic synchronization is highly vulnerable to stochastic fluctuation induced by atmospheric turbulence, which results in temporal injection imbalances at symmetric receivers and triggers intermittent desynchronization. Here, we introduce a full Poincaré vector beam-enabled balanced-injection synchronization (BIS) mechanism, which passively mitigates coupling fluctuations and preserves injection symmetry through a complementary metasurface pair, without requiring any channel estimation or active control. Over a 3.2 km urban link under moderately strong turbulence, BIS suppresses coupling power fluctuations by a factor of 4.6 (from 0.4511 to 0.0975). It eliminates desynchronization events and increases the high-quality synchronization probability from 58.6% to 91.0%. This enables a record-high bit rate-distance product of 720 Gbps \\cdot km, reducing communication interruption probability by up to 77% compared to Gaussian beam transmission. Our innovative strategy bridges the gap between nanophotonics and engineering optics, offering a new insight into advancing next-generation LiDAR, secure communication, and integrated sensing and communication systems in turbulent environments.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Optical chaotic synchronization between coupled nonlinear lasers underpins most chaos-based applications, including complex laser network dynamics, secure communication, key distribution, and reinforcement learning. In free-space links, however, chaotic synchronization is highly vulnerable to stochastic fluctuation induced by atmospheric turbulence, which results in temporal injection imbalances at symmetric receivers and triggers intermittent desynchronization. Here, we introduce a full Poincaré vector beam-enabled balanced-injection synchronization (BIS) mechanism, which passively mitigates coupling fluctuations and preserves injection symmetry through a complementary metasurface pair, without requiring any channel estimation or active control. Over a 3.2 km urban link under moderately strong turbulence, BIS suppresses coupling power fluctuations by a factor of 4.6 (from 0.4511 to 0.0975). It eliminates desynchronization events and increases the high-quality synchronization probability from 58.6% to 91.0%. This enables a record-high bit rate-distance product of 720 Gbps \\cdot km, reducing communication interruption probability by up to 77% compared to Gaussian beam transmission. Our innovative strategy bridges the gap between nanophotonics and engineering optics, offering a new insight into advancing next-generation LiDAR, secure communication, and integrated sensing and communication systems in turbulent environments."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "nlin.CD",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T05:22:20Z",
        "published_parsed": [
            2026,
            1,
            25,
            5,
            22,
            20,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Yiqun Zhang"
            },
            {
                "name": "Mingfeng Xu"
            },
            {
                "name": "Ning Jiang"
            },
            {
                "name": "Mengjie Zhou"
            },
            {
                "name": "Yuhan Zheng"
            },
            {
                "name": "Sichao Chen"
            },
            {
                "name": "Jiazheng Ding"
            },
            {
                "name": "Shuangcheng Chen"
            },
            {
                "name": "Yong Yu"
            },
            {
                "name": "Xianglei Yan"
            },
            {
                "name": "Fei Zhang"
            },
            {
                "name": "Yinghui Guo"
            },
            {
                "name": "Mingbo Pu"
            },
            {
                "name": "Kun Qiu"
            },
            {
                "name": "Xiangang Luo"
            }
        ],
        "author_detail": {
            "name": "Xiangang Luo"
        },
        "author": "Xiangang Luo",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "超表面辅助平衡注入同步用于抗湍流长距离混沌自由空间链路",
        "abstract_cn": "耦合非线性激光器之间的光学混沌同步支撑着大多数基于混沌的应用，包括复杂的激光网络动力学、安全通信、密钥分发和强化学习。然而，在自由空间链路中，混沌同步非常容易受到大气湍流引起的随机涨落的影响，从而导致对称接收器处的时间注入不平衡并触发间歇性去同步。在这里，我们引入了一种完整的庞加莱矢量束启用的平衡注入同步（BIS）机制，该机制可以被动地减轻耦合波动并通过互补的超表面对保持注入对称性，而不需要任何通道估计或主动控制。在中强湍流下的 3.2 公里城市链路上，BIS 将耦合功率波动抑制了 4.6 倍（从 0.4511 到 0.0975）。它消除了失步事件，并将高质量同步概率从 58.6% 提高到 91.0%。这使得比特率-距离乘积达到创纪录的 720 Gbps \\cdot km，与高斯波束传输相比，通信中断概率降低了 77%。我们的创新策略弥合了纳米光子学和工程光学之间的差距，为在湍流环境中推进下一代激光雷达、安全通信以及集成传感和通信系统提供了新的见解。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17719v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17719v1",
        "title": "Topological antilaser",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Topological antilaser"
        },
        "updated": "2026-01-25T06:44:01Z",
        "updated_parsed": [
            2026,
            1,
            25,
            6,
            44,
            1,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17719v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17719v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Coherent perfect absorption (CPA)-the time-reversed operation of lasing at threshold-relies on finely tuned interference and is intrinsically fragile to disorder and structural imperfections. Whether absorption can be endowed with topological protection, by analogy to topological lasing, has remained an open question. Here, we experimentally demonstrate a topological antilaser: the time-reversed counterpart of a topological laser, in which chiral edge modes of a photonic lattice enable perfect light absorption protected by topology. Using a nonreciprocal microwave network with low intrinsic loss, we show that the topological antilaser preserves near-unity absorption under strong disorder, and, unlike conventional antilasers, remains functional for arbitrary placements of dissipation and input ports, even when the lattice is strongly perturbed. This robustness arises from the disorder-immune propagation and stable spatial profile of the topological edge modes. Our results establish topologically protected absorption as the missing counterpart of topological lasing, opening new directions for studying robust energy dissipation, wave control, and coherent-absorption-based detection technologies.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Coherent perfect absorption (CPA)-the time-reversed operation of lasing at threshold-relies on finely tuned interference and is intrinsically fragile to disorder and structural imperfections. Whether absorption can be endowed with topological protection, by analogy to topological lasing, has remained an open question. Here, we experimentally demonstrate a topological antilaser: the time-reversed counterpart of a topological laser, in which chiral edge modes of a photonic lattice enable perfect light absorption protected by topology. Using a nonreciprocal microwave network with low intrinsic loss, we show that the topological antilaser preserves near-unity absorption under strong disorder, and, unlike conventional antilasers, remains functional for arbitrary placements of dissipation and input ports, even when the lattice is strongly perturbed. This robustness arises from the disorder-immune propagation and stable spatial profile of the topological edge modes. Our results establish topologically protected absorption as the missing counterpart of topological lasing, opening new directions for studying robust energy dissipation, wave control, and coherent-absorption-based detection technologies."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mes-hall",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T06:44:01Z",
        "published_parsed": [
            2026,
            1,
            25,
            6,
            44,
            1,
            6,
            25,
            0
        ],
        "arxiv_comment": "8 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Rui-Chang Shen"
            },
            {
                "name": "Chunquan Peng"
            },
            {
                "name": "Bingbing Wang"
            },
            {
                "name": "Wentao Xie"
            },
            {
                "name": "Siyuan Zhang"
            },
            {
                "name": "Peiheng Zhou"
            },
            {
                "name": "Baile Zhang"
            },
            {
                "name": "Y. D. Chong"
            },
            {
                "name": "Haoran Xue"
            }
        ],
        "author_detail": {
            "name": "Haoran Xue"
        },
        "author": "Haoran Xue",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "拓扑抗激光",
        "abstract_cn": "相干完美吸收（CPA）——在阈值处激光发射的时间反转操作——依赖于精细调谐的干扰，并且本质上容易受到无序和结构缺陷的影响。类似于拓扑激光，吸收是否可以被赋予拓扑保护，仍然是一个悬而未决的问题。在这里，我们通过实验演示了拓扑反激光：拓扑激光的时间反转对应物，其中光子晶格的手性边缘模式能够实现受拓扑保护的完美光吸收。使用具有低固有损耗的不可逆微波网络，我们表明拓扑反激光在强无序下保持近乎一致的吸收，并且与传统反激光不同，即使在晶格受到强烈扰动时，其对于耗散和输入端口的任意放置仍然保持功能。这种鲁棒性源于拓扑边缘模式的无序免疫传播和稳定的空间轮廓。我们的研究结果确立了拓扑保护吸收作为拓扑激光的缺失对应物，为研究鲁棒能量耗散、波控制和基于相干吸收的检测技术开辟了新的方向。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17742v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17742v1",
        "title": "Partitionable Diffractive Neural Networks for Multifunctional Optical Operations",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Partitionable Diffractive Neural Networks for Multifunctional Optical Operations"
        },
        "updated": "2026-01-25T08:23:35Z",
        "updated_parsed": [
            2026,
            1,
            25,
            8,
            23,
            35,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17742v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17742v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Diffractive neural network (DNN), which can perform machine learning tasks based on the light propagation and diffraction, has recently emerged as a promising optical computing paradigm due to its high parallel processing speed and low power consumption nature. However, existing diffractive network architectures face challenges in implementing functional reconfiguration. Once a diffractive neural network is fabricated, its functionality is fixed. Deploying such systems for different tasks typically requires reconstructing the entire physical setup, which significantly compromises hardware efficiency in practical applications. In this work, we propose the multifunctional partitionable diffractive neural networks (PDNNs) that can generate networks with additional capabilities by stacking multiple sub-modules with independent functions in the horizontal direction. Each submodule functions as an independent diffractive network capable of performing specific imaging or classification tasks. When these submodules are combined, they can form a new network with additional functionalities. Moreover, assembling these submodules in different configurations enables structures with diverse functions. This powerful PDNN framework demonstrates remarkable advantages in flexibility and reconfigurability for multitask operations, opening a new pathway for realizing multifunctional and integrated optical artificial intelligence systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Diffractive neural network (DNN), which can perform machine learning tasks based on the light propagation and diffraction, has recently emerged as a promising optical computing paradigm due to its high parallel processing speed and low power consumption nature. However, existing diffractive network architectures face challenges in implementing functional reconfiguration. Once a diffractive neural network is fabricated, its functionality is fixed. Deploying such systems for different tasks typically requires reconstructing the entire physical setup, which significantly compromises hardware efficiency in practical applications. In this work, we propose the multifunctional partitionable diffractive neural networks (PDNNs) that can generate networks with additional capabilities by stacking multiple sub-modules with independent functions in the horizontal direction. Each submodule functions as an independent diffractive network capable of performing specific imaging or classification tasks. When these submodules are combined, they can form a new network with additional functionalities. Moreover, assembling these submodules in different configurations enables structures with diverse functions. This powerful PDNN framework demonstrates remarkable advantages in flexibility and reconfigurability for multitask operations, opening a new pathway for realizing multifunctional and integrated optical artificial intelligence systems."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T08:23:35Z",
        "published_parsed": [
            2026,
            1,
            25,
            8,
            23,
            35,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Yudong Tian"
            },
            {
                "name": "Haifeng Xu"
            },
            {
                "name": "Yuqing Liu"
            },
            {
                "name": "Xiangyu Zhao"
            },
            {
                "name": "Jierong Cheng"
            },
            {
                "name": "Chongzhao Wu"
            }
        ],
        "author_detail": {
            "name": "Chongzhao Wu"
        },
        "author": "Chongzhao Wu",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于多功能光学操作的可分区衍射神经网络",
        "abstract_cn": "衍射神经网络（DNN）可以执行基于光传播和衍射的机器学习任务，由于其高并行处理速度和低功耗特性，最近已成为一种有前途的光学计算范式。然而，现有的衍射网络架构在实现功能重新配置方面面临挑战。一旦衍射神经网络被制造出来，它的功能就被固定了。为不同的任务部署此类系统通常需要重建整个物理设置，这会严重影响实际应用中的硬件效率。在这项工作中，我们提出了多功能可分区衍射神经网络（PDNN），它可以通过在水平方向上堆叠多个具有独立功能的子模块来生成具有附加功能的网络。每个子模块都充当独立的衍射网络，能够执行特定的成像或分类任务。当这些子模块组合起来时，它们可以形成具有附加功能的新网络。此外，以不同的配置组装这些子模块可以实现具有不同功能的结构。这种强大的PDNN框架在多任务操作的灵活性和可重构性方面表现出显着的优势，为实现多功能和集成光学人工智能系统开辟了新途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17759v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17759v1",
        "title": "Extended Self-similarity in Multimode Optical Fiber Speckles",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Extended Self-similarity in Multimode Optical Fiber Speckles"
        },
        "updated": "2026-01-25T09:09:49Z",
        "updated_parsed": [
            2026,
            1,
            25,
            9,
            9,
            49,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17759v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17759v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Extended Self-Similarity (ESS) is a widely used tool for uncovering universal power-law scaling in systems dominated by nonlinear interactions. This work demonstrates that ESS scaling can also emerge in a system governed by purely linear physics: the propagation of coherent light in a multimode fiber. The system produces complex speckle patterns arising solely from deterministic linear mode interference. We analyze the intensity structure functions of these speckles and observe a robust extended scaling range. The measured scaling exponents align with the classical Kolmogorov scaling exponents. This finding establishes that the statistical signatures captured by ESS are not exclusive to nonlinear systems, revealing a broader applicability of this scaling framework to complex linear systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Extended Self-Similarity (ESS) is a widely used tool for uncovering universal power-law scaling in systems dominated by nonlinear interactions. This work demonstrates that ESS scaling can also emerge in a system governed by purely linear physics: the propagation of coherent light in a multimode fiber. The system produces complex speckle patterns arising solely from deterministic linear mode interference. We analyze the intensity structure functions of these speckles and observe a robust extended scaling range. The measured scaling exponents align with the classical Kolmogorov scaling exponents. This finding establishes that the statistical signatures captured by ESS are not exclusive to nonlinear systems, revealing a broader applicability of this scaling framework to complex linear systems."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "nlin.PS",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T09:09:49Z",
        "published_parsed": [
            2026,
            1,
            25,
            9,
            9,
            49,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Mengxin Wu"
            },
            {
                "name": "Ziye Chen"
            },
            {
                "name": "Guang Yang"
            },
            {
                "name": "Mingshu Zhao"
            }
        ],
        "author_detail": {
            "name": "Mingshu Zhao"
        },
        "author": "Mingshu Zhao",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "多模光纤散斑的扩展自相似性",
        "abstract_cn": "扩展自相似性 (ESS) 是一种广泛使用的工具，用于揭示以非线性相互作用为主的系统中的普遍幂律标度。这项工作表明，ESS 缩放也可以出现在由纯线性物理控制的系统中：相干光在多模光纤中的传播。该系统产生仅由确定性线性模式干涉产生的复杂散斑图案。我们分析这些散斑的强度结构函数并观察到稳健的扩展缩放范围。测量的缩放指数与经典的柯尔莫哥洛夫缩放指数一致。这一发现表明 ESS 捕获的统计特征并非非线性系统所独有，揭示了该缩放框架对复杂线性系统的更广泛适用性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17804v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17804v1",
        "title": "Bosonic Diffusive Channel: Quantum Metrology via Finite Non-Gaussian Resource",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Bosonic Diffusive Channel: Quantum Metrology via Finite Non-Gaussian Resource"
        },
        "updated": "2026-01-25T11:46:28Z",
        "updated_parsed": [
            2026,
            1,
            25,
            11,
            46,
            28,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17804v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17804v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We investigate the estimation of dephasing-induced decoherence in continuous-variable quantum systems using non-Gaussian probe states. By purifying the open system, we identify optimal probes, specifically squeezed cat and symmetric squeezed compass states, via quantum Fisher information. These results are in agreement with numerical simulation. In settings where the intra-cavity field is inaccessible and standard measurements are impractical, utilizing an ancilla approach where a qubit traverses or interacts with the cavity field, leading to measurement of the qubit, hence allowing estimation of the dephasing rate via Wigner function reconstruction or less costly marginal distribution.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We investigate the estimation of dephasing-induced decoherence in continuous-variable quantum systems using non-Gaussian probe states. By purifying the open system, we identify optimal probes, specifically squeezed cat and symmetric squeezed compass states, via quantum Fisher information. These results are in agreement with numerical simulation. In settings where the intra-cavity field is inaccessible and standard measurements are impractical, utilizing an ancilla approach where a qubit traverses or interacts with the cavity field, leading to measurement of the qubit, hence allowing estimation of the dephasing rate via Wigner function reconstruction or less costly marginal distribution."
        },
        "tags": [
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T11:46:28Z",
        "published_parsed": [
            2026,
            1,
            25,
            11,
            46,
            28,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "quant-ph"
        },
        "authors": [
            {
                "name": "Arman"
            },
            {
                "name": "Prasanta K. Panigrahi"
            }
        ],
        "author_detail": {
            "name": "Prasanta K. Panigrahi"
        },
        "author": "Prasanta K. Panigrahi",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "玻色扩散通道：通过有限非高斯资源进行量子计量",
        "abstract_cn": "我们研究了使用非高斯探测态对连续变量量子系统中相移引起的退相干的估计。通过净化开放系统，我们通过量子费希尔信息确定了最佳探针，特别是挤压猫和对称挤压罗盘态。这些结果与数值模拟一致。在腔内场不可访问且标准测量不切实际的情况下，利用量子位穿过腔场或与腔场相互作用的辅助方法，导致对量子位的测量，从而允许通过维格纳函数重建或成本较低的边际分布来估计移相率。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17816v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17816v1",
        "title": "High-Repetition-Rate Projection Multiphoton Lithography for Large-Area Sub-Micron 3D Printing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "High-Repetition-Rate Projection Multiphoton Lithography for Large-Area Sub-Micron 3D Printing"
        },
        "updated": "2026-01-25T12:45:14Z",
        "updated_parsed": [
            2026,
            1,
            25,
            12,
            45,
            14,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17816v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17816v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "High-resolution lithographic techniques are often limited by low volumetric throughput, since there is no universal and scalable manufacturing process that can produce 3D metasurfaces. In this work, we demonstrate a high-speed holographic 3D printing platform based on spatiotemporal beam shaping, exceeding the repetition rate while keeping the resolution high. The system integrates a femtosecond laser source with a spectral pulse compressor and a beam shaper to project uniform, axially confined light fields to project patterns directly on the advanced photoresists using a Digital Micromirror Device DMD. We investigate the process window for rapid polymerization, optimizing the photoinitiator choice to eliminate thermal crosstalk at high repetition rates. Using this setup, we achieve a production throughput of more than a million voxels per second with sub-micron resolution below 400 nm. The system's reliability is validated through the fabrication of large-area woodpile-like lattices and uniform micropillar arrays, establishing a workflow for scalable manufacturing of micro-optical components.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "High-resolution lithographic techniques are often limited by low volumetric throughput, since there is no universal and scalable manufacturing process that can produce 3D metasurfaces. In this work, we demonstrate a high-speed holographic 3D printing platform based on spatiotemporal beam shaping, exceeding the repetition rate while keeping the resolution high. The system integrates a femtosecond laser source with a spectral pulse compressor and a beam shaper to project uniform, axially confined light fields to project patterns directly on the advanced photoresists using a Digital Micromirror Device DMD. We investigate the process window for rapid polymerization, optimizing the photoinitiator choice to eliminate thermal crosstalk at high repetition rates. Using this setup, we achieve a production throughput of more than a million voxels per second with sub-micron resolution below 400 nm. The system's reliability is validated through the fabrication of large-area woodpile-like lattices and uniform micropillar arrays, establishing a workflow for scalable manufacturing of micro-optical components."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T12:45:14Z",
        "published_parsed": [
            2026,
            1,
            25,
            12,
            45,
            14,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Savvas Papamakarios"
            },
            {
                "name": "Maria Manousidaki"
            },
            {
                "name": "Michalis Stavrou"
            },
            {
                "name": "David Gray"
            },
            {
                "name": "Maria Farsari Institute of Electronic Structure"
            },
            {
                "name": "Laser"
            },
            {
                "name": "Foundation for Research"
            },
            {
                "name": "Technology-Hellas"
            }
        ],
        "author_detail": {
            "name": "Technology-Hellas"
        },
        "author": "Technology-Hellas",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于大面积亚微米 3D 打印的高重复率投影多光子光刻",
        "abstract_cn": "高分辨率光刻技术通常受到低体积吞吐量的限制，因为没有可以生产 3D 超表面的通用且可扩展的制造工艺。在这项工作中，我们展示了一种基于时空光束整形的高速全息3D打印平台，在保持高分辨率的同时超越了重复率。该系统将飞秒激光源与光谱脉冲压缩器和光束整形器集成在一起，可投射均匀、轴向受限的光场，从而使用数字微镜器件 DMD 将图案直接投射到先进的光刻胶上。我们研究了快速聚合的工艺窗口，优化了光引发剂的选择，以消除高重复率下的热串扰。使用此设置，我们实现了每秒超过一百万体素的生产吞吐量，且亚微米分辨率低于 400 nm。该系统的可靠性通过大面积木桩状晶格和均匀微柱阵列的制造得到验证，建立了微光学元件可扩展制造的工作流程。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17876v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17876v1",
        "title": "Coherent Amplifier-Empowered Quantum Interferometer: Preserving Sensitivity and Quantum Advantage under High Loss",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Coherent Amplifier-Empowered Quantum Interferometer: Preserving Sensitivity and Quantum Advantage under High Loss"
        },
        "updated": "2026-01-25T15:12:54Z",
        "updated_parsed": [
            2026,
            1,
            25,
            15,
            12,
            54,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17876v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17876v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Quantum interferometers offer phase measurement capabilities that surpass the standard quantum limit (SQL), with phase sensitivity and quantum enhancement factor serving as key performance metrics. However, practical implementations face severe degradation of both metrics due to unavoidable losses, representing the foremost challenge in advancing quantum interferometry toward real-world applications. To address this challenge, we propose a coherent-amplifier-empowered quantum interferometer. The coherent amplifier dramatically suppresses the decay of both sensitivity and quantum enhancement under high-loss conditions, maintaining phase sensitivity beyond the original SQL even for losses exceeding 90%. Using an injected 4.2 dB squeezed-vacuum state in experimental demonstration, our scheme reduces the quantum enhancement degradation under 90% loss from 3.7 dB in a conventional quantum interferometer (CQI) to only 1.5 dB. More importantly, the phase sensitivity degradation under the same loss is limited to 4.0 dB, markedly outperforming the 11.2 dB degradation observed in a CQI. This improvement is enabled by the coherent amplifier's phase-sensitive photon amplification and its protection of the quantum state. This breakthrough in amplifier-empowered quantum interferometry overcomes the critical barrier to practical deployment, enabling robust quantum-enhanced measurements in lossy environments.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Quantum interferometers offer phase measurement capabilities that surpass the standard quantum limit (SQL), with phase sensitivity and quantum enhancement factor serving as key performance metrics. However, practical implementations face severe degradation of both metrics due to unavoidable losses, representing the foremost challenge in advancing quantum interferometry toward real-world applications. To address this challenge, we propose a coherent-amplifier-empowered quantum interferometer. The coherent amplifier dramatically suppresses the decay of both sensitivity and quantum enhancement under high-loss conditions, maintaining phase sensitivity beyond the original SQL even for losses exceeding 90%. Using an injected 4.2 dB squeezed-vacuum state in experimental demonstration, our scheme reduces the quantum enhancement degradation under 90% loss from 3.7 dB in a conventional quantum interferometer (CQI) to only 1.5 dB. More importantly, the phase sensitivity degradation under the same loss is limited to 4.0 dB, markedly outperforming the 11.2 dB degradation observed in a CQI. This improvement is enabled by the coherent amplifier's phase-sensitive photon amplification and its protection of the quantum state. This breakthrough in amplifier-empowered quantum interferometry overcomes the critical barrier to practical deployment, enabling robust quantum-enhanced measurements in lossy environments."
        },
        "tags": [
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T15:12:54Z",
        "published_parsed": [
            2026,
            1,
            25,
            15,
            12,
            54,
            6,
            25,
            0
        ],
        "arxiv_comment": "6 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "quant-ph"
        },
        "authors": [
            {
                "name": "Jie Zhao"
            },
            {
                "name": "Zeliang Wu"
            },
            {
                "name": "Haoran Liu"
            },
            {
                "name": "Yueya Liu"
            },
            {
                "name": "Xin Chen"
            },
            {
                "name": "Xinyun Liang"
            },
            {
                "name": "Wenfeng Huang"
            },
            {
                "name": "Chun-Hua Yuan"
            },
            {
                "name": "L. Q. Chen"
            }
        ],
        "author_detail": {
            "name": "L. Q. Chen"
        },
        "author": "L. Q. Chen",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "相干放大器支持的量子干涉仪：在高损耗下保持灵敏度和量子优势",
        "abstract_cn": "量子干涉仪提供超越标准量子极限 (SQL) 的相位测量功能，其中相位灵敏度和量子增强因子作为关键性能指标。然而，由于不可避免的损失，实际实现面临着这两个指标的严重退化，这是将量子干涉测量技术推向实际应用的最大挑战。为了应对这一挑战，我们提出了一种相干放大器驱动的量子干涉仪。相干放大器在高损耗条件下极大地抑制了灵敏度和量子增强的衰减，即使在损耗超过 90% 的情况下，也能保持超出原始 SQL 的相位灵敏度。在实验演示中使用注入的 4.2 dB 压缩真空态，我们的方案将 90% 损耗下的量子增强衰减从传统量子干涉仪 (CQI) 中的 3.7 dB 降低到仅 1.5 dB。更重要的是，相同损耗下的相位灵敏度下降被限制在 4.0 dB，明显优于 CQI 中观察到的 11.2 dB 下降。这种改进是通过相干放大器的相敏光子放大及其对量子态的保护来实现的。放大器支持的量子干涉测量技术的这一突破克服了实际部署的关键障碍，能够在有损环境中实现稳健的量子增强测量。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17886v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17886v1",
        "title": "Experimental Realization of Optimized Ternary Mirror Coatings",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Experimental Realization of Optimized Ternary Mirror Coatings"
        },
        "updated": "2026-01-25T15:41:05Z",
        "updated_parsed": [
            2026,
            1,
            25,
            15,
            41,
            5,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17886v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17886v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We report on the first experimental realization of multi-material dielectric mirror coatings designed through a multi-objective optimization algorithm to simultaneously minimize thermal noise and optical losses. We validate this design strategy by fabricating and characterizing two distinct ternary systems: a SiNx -based proof-of-concept and a Ti:GeO2 -based system targeting lower optical losses. The performance of the SiN x coating shows remarkable agreement with predictions, demonstrating a noise amplitude spectral density reduction of 0.82 with respect to current reference coatings, and validating our design-to-fabrication pipeline. The Ti:GeO2 -based system achieves the crucial goal of sub-ppm absorption; its measured thermal noise, however, is higher than the theoretically predicted level of 0.71, extrapolated from single-layer material characterization. A dedicated tolerance analysis confirms that this discrepancy is not attributable to random thickness errors, emphasizing that further studies of the manufacturing process are needed to fully exploit this combination of materials. This work establishes a robust methodology for producing complex, high-performance optical coatings tailored for precision experiments.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We report on the first experimental realization of multi-material dielectric mirror coatings designed through a multi-objective optimization algorithm to simultaneously minimize thermal noise and optical losses. We validate this design strategy by fabricating and characterizing two distinct ternary systems: a SiNx -based proof-of-concept and a Ti:GeO2 -based system targeting lower optical losses. The performance of the SiN x coating shows remarkable agreement with predictions, demonstrating a noise amplitude spectral density reduction of 0.82 with respect to current reference coatings, and validating our design-to-fabrication pipeline. The Ti:GeO2 -based system achieves the crucial goal of sub-ppm absorption; its measured thermal noise, however, is higher than the theoretically predicted level of 0.71, extrapolated from single-layer material characterization. A dedicated tolerance analysis confirms that this discrepancy is not attributable to random thickness errors, emphasizing that further studies of the manufacturing process are needed to fully exploit this combination of materials. This work establishes a robust methodology for producing complex, high-performance optical coatings tailored for precision experiments."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "gr-qc",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T15:41:05Z",
        "published_parsed": [
            2026,
            1,
            25,
            15,
            41,
            5,
            6,
            25,
            0
        ],
        "arxiv_comment": "8 pages main text (3 figures, 3 tables); includes extensive Supplementary Material (26 pages, 19 figures, 1 table). Total 34 pages",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "V. Pierro"
            },
            {
                "name": "M. Granata"
            },
            {
                "name": "C. Michel"
            },
            {
                "name": "L. Pinard"
            },
            {
                "name": "B. Sassolas"
            },
            {
                "name": "D. Forest"
            },
            {
                "name": "N. Demos"
            },
            {
                "name": "S. Gras"
            },
            {
                "name": "M. Evans"
            },
            {
                "name": "I. M. Pinto"
            },
            {
                "name": "G. Avallone"
            },
            {
                "name": "V. Granata"
            }
        ],
        "author_detail": {
            "name": "V. Granata"
        },
        "author": "V. Granata",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "优化三元镜涂层的实验实现",
        "abstract_cn": "我们报告了通过多目标优化算法设计的多材料介质镜涂层的首次实验实现，以同时最大限度地减少热噪声和光学损耗。我们通过制造和表征两个不同的三元系统来验证这一设计策略：基于 SiNx 的概念验证系统和旨在降低光学损耗的基于 Ti:GeO2 的系统。 SiN x 涂层的性能与预测非常吻合，相对于当前的参考涂层，噪声幅度谱密度降低了 0.82，并验证了我们的设计到制造流程。基于 Ti:GeO2 的系统实现了亚 ppm 吸收的关键目标；然而，其测量的热噪声高于根据单层材料特性推断的理论预测水平 0.71。专门的公差分析证实，这种差异并非归因于随机厚度误差，并强调需要对制造工艺进行进一步研究，以充分利用这种材料组合。这项工作建立了一种稳健的方法来生产专为精密实验定制的复杂、高性能光学涂层。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17889v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17889v1",
        "title": "Near-Field Mechanical Fingerprints for THz Sensing of 'Hidden' Nanoparticles in Complex Media",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Near-Field Mechanical Fingerprints for THz Sensing of 'Hidden' Nanoparticles in Complex Media"
        },
        "updated": "2026-01-25T15:56:35Z",
        "updated_parsed": [
            2026,
            1,
            25,
            15,
            56,
            35,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17889v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17889v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Terahertz (THz) spectroscopy holds transformative potential for non-invasive sensing, yet characterizing individual nanoparticles in complex biological environments remains challenging due to the far-field diffraction limit. While near-field dipolar theory is well established, its application to characterizing/identifying nanoparticles immersed in complex media at THz frequencies is largely unexplored.\n  This work utilizes numerical simulations of magneto-optical (MO) heterodimers -- comprising n-doped Indium Antimonide (n-InSb) and isotropic or birefringent particles (e.g., SiO2, GaSe) -- under counter-propagating, circularly polarized THz illumination. We demonstrate that while far-field observables like absorption cross-sections are often dominated by the MO-active particle, mechanical variables-specifically induced binding forces and spin/orbital torques-exhibit superior sensitivity for detecting \"hidden\" neighboring components. Because these mechanical signatures depend directly on near-field interactions, they provide higher information density regarding interparticle coupling.\n  Key findings reveal material-specific spectral \"hotspots\" and \"zeros\" that serve as robust calibration markers even within dispersive biological surrogates. We show that the spin torque on non-MO particles is significantly modified by MO-neighbor proximity, a phenomenon controllable via static magnetic fields. Furthermore, these variables exhibit high angular sensitivity in perpendicular configurations. Our results provide a roadmap for using optomechanical signatures as high-resolution detectors for in-vivo diagnostics, signal transduction, and low-energy nanocircuit control.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Terahertz (THz) spectroscopy holds transformative potential for non-invasive sensing, yet characterizing individual nanoparticles in complex biological environments remains challenging due to the far-field diffraction limit. While near-field dipolar theory is well established, its application to characterizing/identifying nanoparticles immersed in complex media at THz frequencies is largely unexplored.\n  This work utilizes numerical simulations of magneto-optical (MO) heterodimers -- comprising n-doped Indium Antimonide (n-InSb) and isotropic or birefringent particles (e.g., SiO2, GaSe) -- under counter-propagating, circularly polarized THz illumination. We demonstrate that while far-field observables like absorption cross-sections are often dominated by the MO-active particle, mechanical variables-specifically induced binding forces and spin/orbital torques-exhibit superior sensitivity for detecting \"hidden\" neighboring components. Because these mechanical signatures depend directly on near-field interactions, they provide higher information density regarding interparticle coupling.\n  Key findings reveal material-specific spectral \"hotspots\" and \"zeros\" that serve as robust calibration markers even within dispersive biological surrogates. We show that the spin torque on non-MO particles is significantly modified by MO-neighbor proximity, a phenomenon controllable via static magnetic fields. Furthermore, these variables exhibit high angular sensitivity in perpendicular configurations. Our results provide a roadmap for using optomechanical signatures as high-resolution detectors for in-vivo diagnostics, signal transduction, and low-energy nanocircuit control."
        },
        "tags": [
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.comp-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T15:56:35Z",
        "published_parsed": [
            2026,
            1,
            25,
            15,
            56,
            35,
            6,
            25,
            0
        ],
        "arxiv_comment": "12 pages, 6 figures, research article",
        "arxiv_primary_category": {
            "term": "physics.app-ph"
        },
        "authors": [
            {
                "name": "Ricardo Martin Abraham-Ekeroth"
            },
            {
                "name": "Dani Torrent"
            }
        ],
        "author_detail": {
            "name": "Dani Torrent"
        },
        "author": "Dani Torrent",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于复杂介质中“隐藏”纳米颗粒太赫兹传感的近场机械指纹",
        "abstract_cn": "太赫兹 (THz) 光谱具有非侵入式传感的变革潜力，但由于远场衍射极限，在复杂生物环境中表征单个纳米粒子仍然具有挑战性。虽然近场偶极理论已经很成熟，但其在太赫兹频率下表征/识别浸入复杂介质中的纳米粒子的应用在很大程度上尚未被探索。\n  这项工作利用磁光 (MO) 异二聚体（包括 n 掺杂锑化铟 (n-InSb) 和各向同性或双折射粒子（例如 SiO2、GaSe））在反向传播、圆偏振太赫兹照明下的数值模拟。我们证明，虽然吸收截面等远场可观测量通常由 MO 活性粒子主导，但机械变量（特定诱导的结合力和自旋/轨道扭矩）对于检测“隐藏”的相邻组件表现出卓越的灵敏度。由于这些机械特征直接取决于近场相互作用，因此它们提供了有关颗粒间耦合的更高信息密度。\n  主要发现揭示了特定材料的光谱“热点”和“零点”，即使在分散的生物替代品中，它们也可以充当强大的校准标记。我们发现，非 MO 粒子上的自旋扭矩会因 MO 邻近粒子的接近而发生显着改变，这是一种可通过静磁场控制的现象。此外，这些变量在垂直配置中表现出高角度灵敏度。我们的结果为使用光机械特征作为高分辨率探测器进行体内诊断、信号转导和低能量纳米电路控制提供了路线图。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17908v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17908v1",
        "title": "Graphical Constructions of Wavefronts and Waist Parameters in Gaussian Beam Optics",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Graphical Constructions of Wavefronts and Waist Parameters in Gaussian Beam Optics"
        },
        "updated": "2026-01-25T17:01:29Z",
        "updated_parsed": [
            2026,
            1,
            25,
            17,
            1,
            29,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17908v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17908v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We provide several diagrams for the graphical determination of certain elements of a Gaussian beam based on prior knowledge of other elements. For example, these diagrams allow us to determine the plane of the beam waist and the Rayleigh range from knowledge of two wavefronts constituting the beam, or to determine the size of the light spot on a given wavefront. We also present a simple method for determining the waist position and the Rayleigh range of the image of a Gaussian beam formed by a lens.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We provide several diagrams for the graphical determination of certain elements of a Gaussian beam based on prior knowledge of other elements. For example, these diagrams allow us to determine the plane of the beam waist and the Rayleigh range from knowledge of two wavefronts constituting the beam, or to determine the size of the light spot on a given wavefront. We also present a simple method for determining the waist position and the Rayleigh range of the image of a Gaussian beam formed by a lens."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T17:01:29Z",
        "published_parsed": [
            2026,
            1,
            25,
            17,
            1,
            29,
            6,
            25,
            0
        ],
        "arxiv_comment": "14 pages, 10 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Pierre Pellat-Finet"
            }
        ],
        "author_detail": {
            "name": "Pierre Pellat-Finet"
        },
        "author": "Pierre Pellat-Finet",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "高斯光束光学中波前和腰参数的图形结构",
        "abstract_cn": "我们提供了几个图表，用于根据其他元素的先验知识来图形确定高斯光束的某些元素。例如，这些图使我们能够根据构成光束的两个波前的知识来确定光束腰的平面和瑞利范围，或者确定给定波前的光斑的大小。我们还提出了一种简单的方法来确定透镜形成的高斯光束图像的腰部位置和瑞利范围。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18031v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18031v1",
        "title": "Video-rate mid-infrared imaging in the molecular fingerprint region via nanosecond non-degenerate two-photon absorption",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Video-rate mid-infrared imaging in the molecular fingerprint region via nanosecond non-degenerate two-photon absorption"
        },
        "updated": "2026-01-25T22:56:00Z",
        "updated_parsed": [
            2026,
            1,
            25,
            22,
            56,
            0,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18031v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18031v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Non-degenerate two-photon absorption (NTA) offers an attractive route for wide-field mid-infrared (MIR) imaging by mapping long wavelength information into the spectral detection windows of mature near-infrared detector technologies. However, existing NTA implementations rely almost exclusively on complex, large-footprint femtosecond laser systems, severely limiting practicality and scalability. Here, we demonstrate an NTA imaging platform that replaces the ultrafast laser with a compact nanosecond mid-IR source coupled to a high-definition indium gallium arsenide (InGaAs) camera. Operating in the nanosecond regime removes stringent temporal-overlap requirements, dramatically simplifying system architecture while preserving high nonlinear sensitivity. Using this approach, we achieve chemically selective, wide-field imaging deep into the mid-IR molecular fingerprint region and demonstrate, for the first time, video-rate NTA imaging in this spectrally rich regime. By combining relaxed alignment constraints, compact excitation, and high-speed fingerprint-region imaging, this work establishes nanosecond NTA as a practical and scalable foundation for next-generation mid-IR chemical imaging.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Non-degenerate two-photon absorption (NTA) offers an attractive route for wide-field mid-infrared (MIR) imaging by mapping long wavelength information into the spectral detection windows of mature near-infrared detector technologies. However, existing NTA implementations rely almost exclusively on complex, large-footprint femtosecond laser systems, severely limiting practicality and scalability. Here, we demonstrate an NTA imaging platform that replaces the ultrafast laser with a compact nanosecond mid-IR source coupled to a high-definition indium gallium arsenide (InGaAs) camera. Operating in the nanosecond regime removes stringent temporal-overlap requirements, dramatically simplifying system architecture while preserving high nonlinear sensitivity. Using this approach, we achieve chemically selective, wide-field imaging deep into the mid-IR molecular fingerprint region and demonstrate, for the first time, video-rate NTA imaging in this spectrally rich regime. By combining relaxed alignment constraints, compact excitation, and high-speed fingerprint-region imaging, this work establishes nanosecond NTA as a practical and scalable foundation for next-generation mid-IR chemical imaging."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T22:56:00Z",
        "published_parsed": [
            2026,
            1,
            25,
            22,
            56,
            0,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Evan P. Garcia"
            },
            {
                "name": "Yryx Y. L. Palacios"
            },
            {
                "name": "Ryan Nguyen"
            },
            {
                "name": "Adam Hanninen"
            },
            {
                "name": "Aleksei I. Noskov"
            },
            {
                "name": "Eric O. Potma"
            },
            {
                "name": "Dmitry A. Fishman"
            }
        ],
        "author_detail": {
            "name": "Dmitry A. Fishman"
        },
        "author": "Dmitry A. Fishman",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过纳秒非简并双光子吸收在分子指纹区域进行视频速率中红外成像",
        "abstract_cn": "非简并双光子吸收（NTA）通过将长波长信息映射到成熟的近红外探测器技术的光谱探测窗口中，为宽场中红外（MIR）成像提供了一条有吸引力的途径。然而，现有的 NTA 实现几乎完全依赖于复杂、占用空间大的飞秒激光系统，严重限制了实用性和可扩展性。在这里，我们演示了一个 NTA 成像平台，该平台用与高清砷化铟镓 (InGaAs) 相机耦合的紧凑型纳秒中红外源取代了超快激光器。在纳秒范围内运行消除了严格的时间重叠要求，极大地简化了系统架构，同时保持了高非线性灵敏度。利用这种方法，我们实现了深入中红外分子指纹区域的化学选择性宽视场成像，并首次展示了在这个光谱丰富的区域中的视频速率 NTA 成像。通过结合宽松的对准约束、紧凑激发和高速指纹区域成像，这项工作将纳秒 NTA 确立为下一代中红外化学成像的实用且可扩展的基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18047v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18047v1",
        "title": "Laser interferometry as a robust neuromorphic platform for machine learning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Laser interferometry as a robust neuromorphic platform for machine learning"
        },
        "updated": "2026-01-26T00:26:12Z",
        "updated_parsed": [
            2026,
            1,
            26,
            0,
            26,
            12,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18047v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18047v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We present a method for implementing an optical neural network using only linear optical resources, namely field displacement and interferometry applied to coherent states of light. The nonlinearity required for learning in a neural network is realized via an encoding of the input into phase shifts allowing for far more straightforward experimental implementation compared to previous proposals for, and demonstrations of, $\\textit{in situ}$ inference. Beyond $\\textit{in situ}$ inference, the method enables $\\textit{in situ}$ training by utilizing established techniques like parameter shift rules or physical backpropagation to extract gradients directly from measurements of the linear optical circuit. We also investigate the effect of photon losses and find the model to be very resilient to these.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We present a method for implementing an optical neural network using only linear optical resources, namely field displacement and interferometry applied to coherent states of light. The nonlinearity required for learning in a neural network is realized via an encoding of the input into phase shifts allowing for far more straightforward experimental implementation compared to previous proposals for, and demonstrations of, $\\textit{in situ}$ inference. Beyond $\\textit{in situ}$ inference, the method enables $\\textit{in situ}$ training by utilizing established techniques like parameter shift rules or physical backpropagation to extract gradients directly from measurements of the linear optical circuit. We also investigate the effect of photon losses and find the model to be very resilient to these."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.ET",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T00:26:12Z",
        "published_parsed": [
            2026,
            1,
            26,
            0,
            26,
            12,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Amanuel Anteneh"
            },
            {
                "name": "Kyungeun Kim"
            },
            {
                "name": "J. M. Schwarz"
            },
            {
                "name": "Israel Klich"
            },
            {
                "name": "Olivier Pfister"
            }
        ],
        "author_detail": {
            "name": "Olivier Pfister"
        },
        "author": "Olivier Pfister",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "激光干涉测量作为机器学习的强大神经形态平台",
        "abstract_cn": "我们提出了一种仅使用线性光学资源来实现光学神经网络的方法，即应用于光相干态的场位移和干涉测量。神经网络中学习所需的非线性是通过将输入编码为相移来实现的，与之前的 $\\textit{in situ}$ 推理的提议和演示相比，允许更直接的实验实现。除了 $\\textit{in situ}$ 推理之外，该方法还可以利用参数移位规则或物理反向传播等现有技术来直接从线性光路的测量中提取梯度，从而进行 $\\textit{in situ}$ 训练。我们还研究了光子损失的影响，发现该模型对这些影响非常有弹性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18074v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18074v1",
        "title": "Physics-Integrated Inference for Signal Recovery in Non-Gaussian Regimes",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Physics-Integrated Inference for Signal Recovery in Non-Gaussian Regimes"
        },
        "updated": "2026-01-26T02:13:15Z",
        "updated_parsed": [
            2026,
            1,
            26,
            2,
            13,
            15,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18074v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18074v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "High-performance room-temperature sensing is often limited by non-stationary $1/f$ fluctuations and non-Gaussian stochasticity. In spintronic devices, thermally activated Néel switching creates heavy-tailed noise that masks weak signals, defeating linear filters optimized for Gaussian statistics. Here, we introduce a physics-integrated inference framework that decouples signal morphology from stochastic transients using a hierarchical 1D CNN-GRU topology. By learning the temporal signatures of Néel relaxation, this architecture reduces the Noise Equivalent Differential Temperature (NEDT) of spintronic Poisson bolometers by a factor of six (233.78 mK to 40.44 mK), effectively elevating room-temperature sensitivity toward cryogenic limits. We demonstrate the framework's universality across the electromagnetic and biological spectrum, achieving a 9-fold error suppression in Radar tracking, a 40\\% uncertainty reduction in LiDAR, and a 15.56 dB SNR enhancement in ECG. This hardware-inference coupling recovers deterministic signals from fluctuation-dominated regimes, enabling near-ideal detection limits in noisy edge environments.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "High-performance room-temperature sensing is often limited by non-stationary $1/f$ fluctuations and non-Gaussian stochasticity. In spintronic devices, thermally activated Néel switching creates heavy-tailed noise that masks weak signals, defeating linear filters optimized for Gaussian statistics. Here, we introduce a physics-integrated inference framework that decouples signal morphology from stochastic transients using a hierarchical 1D CNN-GRU topology. By learning the temporal signatures of Néel relaxation, this architecture reduces the Noise Equivalent Differential Temperature (NEDT) of spintronic Poisson bolometers by a factor of six (233.78 mK to 40.44 mK), effectively elevating room-temperature sensitivity toward cryogenic limits. We demonstrate the framework's universality across the electromagnetic and biological spectrum, achieving a 9-fold error suppression in Radar tracking, a 40\\% uncertainty reduction in LiDAR, and a 15.56 dB SNR enhancement in ECG. This hardware-inference coupling recovers deterministic signals from fluctuation-dominated regimes, enabling near-ideal detection limits in noisy edge environments."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T02:13:15Z",
        "published_parsed": [
            2026,
            1,
            26,
            2,
            13,
            15,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Mohamed A. Mousa"
            },
            {
                "name": "Leif Bauer"
            },
            {
                "name": "Ziyi Yang"
            },
            {
                "name": "Utkarsh Singh"
            },
            {
                "name": "Angshuman Deka"
            },
            {
                "name": "Zubin Jacob"
            }
        ],
        "author_detail": {
            "name": "Zubin Jacob"
        },
        "author": "Zubin Jacob",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "非高斯状态下信号恢复的物理综合推理",
        "abstract_cn": "高性能室温传感通常受到非平稳 $1/f$ 波动和非高斯随机性的限制。在自旋电子器件中，热激活尼尔开关会产生掩盖微弱信号的重尾噪声，从而击败针对高斯统计优化的线性滤波器。在这里，我们引入了一种物理集成推理框架，该框架使用分层 1D CNN-GRU 拓扑将信号形态与随机瞬态解耦。通过学习 Néel 弛豫的时间特征，该架构将自旋电子泊松测辐射热计的噪声等效温差 (NEDT) 降低了六倍（233.78 mK 至 40.44 mK），有效地将室温灵敏度提升至低温极限。我们证明了该框架在电磁和生物频谱上的通用性，实现了雷达跟踪中 9 倍的误差抑制、LiDAR 中 40% 的不确定性降低以及 ECG 中 15.56 dB 的 SNR 增强。这种硬件推理耦合可以从波动主导的状态中恢复确定性信号，从而在噪声边缘环境中实现接近理想的检测限制。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18084v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18084v1",
        "title": "Pulse-driven photonic transitions and nonreciprocity in space-time modulated metasurfaces",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Pulse-driven photonic transitions and nonreciprocity in space-time modulated metasurfaces"
        },
        "updated": "2026-01-26T02:44:34Z",
        "updated_parsed": [
            2026,
            1,
            26,
            2,
            44,
            34,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18084v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18084v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Time-varying photonic systems open new possibilities for controlling light, enabling photonic time crystals, time reflection and refraction, frequency conversion, synthetic gauge fields, optical nonreciprocity, among others. These effects emerge from the dynamic modulation of optical properties, which can mediate photonic transitions between eigenstates of different frequencies and/or wavevectors. To achieve such transitions, conventional approaches rely on periodic modulation schemes that demand ultrafast modulation rates and continuous energy input, posing significant practical challenges at optical frequencies. Here, we demonstrate that periodic-modulation-driven photonic transitions within the radiation continuum can be effectively mimicked using a single-period ultrafast pulse modulation, eliminating the need for sustained continuous modulation. By leveraging dispersion engineering in metasurfaces to tailor the density of states in the radiation continuum, we achieve controlled frequency transitions and theoretically demonstrate strong nonreciprocity for free-space waves as a key application. Our findings may guide future experimental research on time-varying photonics using materials such as transparent conductive oxides and semiconductors, expanding the possibilities for ultrafast and reconfigurable optical technologies. More broadly, our work may establish a practical and energy-efficient framework for dynamic photonic systems, with potential applications ranging from spatio-temporal wavefront manipulation to photonic computing and ultrafast signal processing.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Time-varying photonic systems open new possibilities for controlling light, enabling photonic time crystals, time reflection and refraction, frequency conversion, synthetic gauge fields, optical nonreciprocity, among others. These effects emerge from the dynamic modulation of optical properties, which can mediate photonic transitions between eigenstates of different frequencies and/or wavevectors. To achieve such transitions, conventional approaches rely on periodic modulation schemes that demand ultrafast modulation rates and continuous energy input, posing significant practical challenges at optical frequencies. Here, we demonstrate that periodic-modulation-driven photonic transitions within the radiation continuum can be effectively mimicked using a single-period ultrafast pulse modulation, eliminating the need for sustained continuous modulation. By leveraging dispersion engineering in metasurfaces to tailor the density of states in the radiation continuum, we achieve controlled frequency transitions and theoretically demonstrate strong nonreciprocity for free-space waves as a key application. Our findings may guide future experimental research on time-varying photonics using materials such as transparent conductive oxides and semiconductors, expanding the possibilities for ultrafast and reconfigurable optical technologies. More broadly, our work may establish a practical and energy-efficient framework for dynamic photonic systems, with potential applications ranging from spatio-temporal wavefront manipulation to photonic computing and ultrafast signal processing."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T02:44:34Z",
        "published_parsed": [
            2026,
            1,
            26,
            2,
            44,
            34,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Zeki Hayran"
            },
            {
                "name": "John B. Pendry"
            },
            {
                "name": "Prasad P. Iyer"
            },
            {
                "name": "Francesco Monticone"
            }
        ],
        "author_detail": {
            "name": "Francesco Monticone"
        },
        "author": "Francesco Monticone",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "时空调制超表面中的脉冲驱动光子跃迁和非互易性",
        "abstract_cn": "时变光子系统为控制光开辟了新的可能性，实现了光子时间晶体、时间反射和折射、频率转换、合成规范场、光学非互易性等。这些效应是由光学特性的动态调制产生的，它可以介导不同频率和/或波矢量的本征态之间的光子跃迁。为了实现这种转变，传统方法依赖于周期性调制方案，需要超快调制速率和连续能量输入，这对光频率提出了重大的实际挑战。在这里，我们证明了可以使用单周期超快脉冲调制有效地模拟辐射连续体内的周期性调制驱动的光子跃迁，从而消除了持续连续调制的需要。通过利用超表面中的色散工程来调整辐射连续体中的态密度，我们实现了受控的频率跃迁，并在理论上证明了自由空间波作为关键应用的强非互易性。我们的研究结果可能会指导未来使用透明导电氧化物和半导体等材料进行时变光子学的实验研究，从而扩大超快和可重构光学技术的可能性。更广泛地说，我们的工作可能为动态光子系统建立一个实用且节能的框架，其潜在应用范围从时空波前操纵到光子计算和超快信号处理。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18104v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18104v1",
        "title": "Broadband asymmetric transmission with tunable bilayer silicon nanoarrays: from visible to near-infrared",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Broadband asymmetric transmission with tunable bilayer silicon nanoarrays: from visible to near-infrared"
        },
        "updated": "2026-01-26T03:26:57Z",
        "updated_parsed": [
            2026,
            1,
            26,
            3,
            26,
            57,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18104v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18104v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "A kind of asymmetric transmission (AT) device based on bilayer silicon arrays (BSA) nanostructure is theoretically explored, which achieves high forward transmissivity and suppressed backward transmissivity for broadband by simply adjusting the parameters of the structure. The structure consists of two silicon cylinder arrays, one on the SiO2 substrate and the other embedded in the substrate. Particularly, three AT devices with different configurations are designed, which exhibit broadband AT with high isolation ratios in the wavelength ranges of 685-807 nm, 866-1029 nm, and 1285-1536 nm, respectively. A comprehensive analysis of the BSA structure's performance across different array periods highlights its potential for broadband optical applications, such as optical isolation and multi-channel optical sensors.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A kind of asymmetric transmission (AT) device based on bilayer silicon arrays (BSA) nanostructure is theoretically explored, which achieves high forward transmissivity and suppressed backward transmissivity for broadband by simply adjusting the parameters of the structure. The structure consists of two silicon cylinder arrays, one on the SiO2 substrate and the other embedded in the substrate. Particularly, three AT devices with different configurations are designed, which exhibit broadband AT with high isolation ratios in the wavelength ranges of 685-807 nm, 866-1029 nm, and 1285-1536 nm, respectively. A comprehensive analysis of the BSA structure's performance across different array periods highlights its potential for broadband optical applications, such as optical isolation and multi-channel optical sensors."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T03:26:57Z",
        "published_parsed": [
            2026,
            1,
            26,
            3,
            26,
            57,
            0,
            26,
            0
        ],
        "arxiv_comment": "4 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Ruihan Ma"
            },
            {
                "name": "Yuqing Cheng"
            },
            {
                "name": "Mengtao Sun"
            }
        ],
        "author_detail": {
            "name": "Mengtao Sun"
        },
        "author": "Mengtao Sun",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "可调谐双层硅纳米阵列的宽带不对称传输：从可见光到近红外",
        "abstract_cn": "从理论上探索了一种基于双层硅阵列（BSA）纳米结构的非对称传输（AT）器件，通过简单地调整结构参数即可实现宽带的高前向透射率和抑制后向透射率。该结构由两个硅圆柱阵列组成，一个位于SiO2基板上，另一个嵌入基板内。特别地，设计了三种具有不同配置的AT器件，它们分别在685-807 nm、866-1029 nm和1285-1536 nm波长范围内表现出具有高隔离比的宽带AT。对 BSA 结构在不同阵列周期内的性能的全面分析凸显了其在宽带光学应用中的潜力，例如光学隔离和多通道光学传感器。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18235v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18235v1",
        "title": "Data-Efficient Electromagnetic Surrogate Solver Through Dissipative Relaxation Transfer Learning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Data-Efficient Electromagnetic Surrogate Solver Through Dissipative Relaxation Transfer Learning"
        },
        "updated": "2026-01-26T07:41:30Z",
        "updated_parsed": [
            2026,
            1,
            26,
            7,
            41,
            30,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18235v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18235v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In neural-network surrogate solvers for electromagnetic simulations, accurately modeling resonant phenomena remains a central challenge. High-amplitude resonances generate strongly localized field patterns that appear as outlier samples, deviating significantly from the general distribution of non-resonant cases, leading to instability and degraded predictive performance. To address this, we introduce dissipative relaxation transfer learning (DIRTL), a data-efficient training framework that integrates transfer learning with loss-regularized optimization principles from high-Q photonics. DIRTL first pretrains the model on data generated with a small fictitious material loss, which broadens sharp resonant modes and suppresses extreme field amplitudes. This smoothing of the response landscape enables the model to learn global modal features more effectively. The pretrained model is subsequently fine-tuned on the target lossless dataset containing the true high-amplitude resonances, allowing stable adaptation based on the pretrained information. Applied to both the Fourier Neural Operator (FNO) and UNet architectures, DIRTL yields substantial improvements in prediction accuracy, including up to a two-fold error reduction for the FNO variant. Furthermore, DIRTL exhibits robustness across diverse training conditions and supports strong multi-task performance, underscoring the generalizability and flexibility of the pretrained core. Altogether, these results establish DIRTL as a physically grounded and architecture-agnostic curriculum for enhancing the reliability of machine-learning-based electromagnetic surrogate solvers.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In neural-network surrogate solvers for electromagnetic simulations, accurately modeling resonant phenomena remains a central challenge. High-amplitude resonances generate strongly localized field patterns that appear as outlier samples, deviating significantly from the general distribution of non-resonant cases, leading to instability and degraded predictive performance. To address this, we introduce dissipative relaxation transfer learning (DIRTL), a data-efficient training framework that integrates transfer learning with loss-regularized optimization principles from high-Q photonics. DIRTL first pretrains the model on data generated with a small fictitious material loss, which broadens sharp resonant modes and suppresses extreme field amplitudes. This smoothing of the response landscape enables the model to learn global modal features more effectively. The pretrained model is subsequently fine-tuned on the target lossless dataset containing the true high-amplitude resonances, allowing stable adaptation based on the pretrained information. Applied to both the Fourier Neural Operator (FNO) and UNet architectures, DIRTL yields substantial improvements in prediction accuracy, including up to a two-fold error reduction for the FNO variant. Furthermore, DIRTL exhibits robustness across diverse training conditions and supports strong multi-task performance, underscoring the generalizability and flexibility of the pretrained core. Altogether, these results establish DIRTL as a physically grounded and architecture-agnostic curriculum for enhancing the reliability of machine-learning-based electromagnetic surrogate solvers."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.comp-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T07:41:30Z",
        "published_parsed": [
            2026,
            1,
            26,
            7,
            41,
            30,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Sunghyun Nam"
            },
            {
                "name": "Chan Y. Park"
            },
            {
                "name": "Min Seok Jang"
            }
        ],
        "author_detail": {
            "name": "Min Seok Jang"
        },
        "author": "Min Seok Jang",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过耗散弛豫迁移学习的数据高效电磁替代求解器",
        "abstract_cn": "在用于电磁仿真的神经网络代理求解器中，准确建模谐振现象仍然是一个核心挑战。高振幅共振会产生强烈的局部场模式，这些场模式表现为异常样本，显着偏离非共振情况的一般分布，导致不稳定和预测性能下降。为了解决这个问题，我们引入了耗散松弛迁移学习（DIRTL），这是一种数据高效的训练框架，它将迁移学习与高 Q 光子学的损失正则化优化原理相结合。 DIRTL 首先根据少量虚拟材料损失生成的数据对模型进行预训练，从而拓宽尖锐的谐振模式并抑制极端场振幅。这种响应景观的平滑使模型能够更有效地学习全局模态特征。随后在包含真实高振幅共振的目标无损数据集上对预训练模型进行微调，从而实现基于预训练信息的稳定适应。 DIRTL 应用于傅里叶神经算子 (FNO) 和 UNet 架构，可显着提高预测精度，包括将 FNO 变体的误差减少多达两倍。此外，DIRTL 在不同的训练条件下表现出鲁棒性，并支持强大的多任务性能，强调了预训练核心的通用性和灵活性。总而言之，这些结果将 DIRTL 确立为物理基础且与架构无关的课程，用于增强基于机器学习的电磁代理求解器的可靠性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18246v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18246v1",
        "title": "Tensor-driven geometric phase in nonlinear AlGaAs metasurfaces",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Tensor-driven geometric phase in nonlinear AlGaAs metasurfaces"
        },
        "updated": "2026-01-26T08:07:05Z",
        "updated_parsed": [
            2026,
            1,
            26,
            8,
            7,
            5,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18246v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18246v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Dielectric metasurfaces provide a unique platform for efficient harmonic generation and optical wavefront manipulation at the nanoscale. While several approaches are available for performing wavefront shaping, the one exploiting geometric phase streamlines significantly the design and fabrication process. It has been recently shown that, in III-V semiconductor alloys, the rotation of the crystal axes affects the phase and amplitude of second-harmonic generation (SHG) induced by circularly polarized light [1]. Based on this notion, we fabricated and characterized two aluminum gallium arsenide metasurfaces displaying the versatility of the geometric phase design approach through nonlinear beam steering and structured-light generation on the harmonic field.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Dielectric metasurfaces provide a unique platform for efficient harmonic generation and optical wavefront manipulation at the nanoscale. While several approaches are available for performing wavefront shaping, the one exploiting geometric phase streamlines significantly the design and fabrication process. It has been recently shown that, in III-V semiconductor alloys, the rotation of the crystal axes affects the phase and amplitude of second-harmonic generation (SHG) induced by circularly polarized light [1]. Based on this notion, we fabricated and characterized two aluminum gallium arsenide metasurfaces displaying the versatility of the geometric phase design approach through nonlinear beam steering and structured-light generation on the harmonic field."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T08:07:05Z",
        "published_parsed": [
            2026,
            1,
            26,
            8,
            7,
            5,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Giorgio Guercio"
            },
            {
                "name": "Andrea Gerini"
            },
            {
                "name": "Kristina Frizyuk"
            },
            {
                "name": "Costantino De Angelis"
            },
            {
                "name": "Martina Morassi"
            },
            {
                "name": "Aristide Lemaître"
            },
            {
                "name": "Luca Carletti"
            },
            {
                "name": "Giuseppe Leo"
            }
        ],
        "author_detail": {
            "name": "Giuseppe Leo"
        },
        "author": "Giuseppe Leo",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "非线性 AlGaAs 超表面中张量驱动的几何相位",
        "abstract_cn": "介电超表面为纳米级的高效谐波生成和光学波前操纵提供了独特的平台。虽然有多种方法可用于执行波前整形，但利用几何相位的方法显着简化了设计和制造过程。最近的研究表明，在III-V族半导体合金中，晶轴的旋转会影响圆偏振光引起的二次谐波产生（SHG）的相位和振幅[1]。基于这个概念，我们制造并表征了两个铝砷化镓超表面，通过谐波场上的非线性光束控制和结构光生成，展示了几何相位设计方法的多功能性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18265v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18265v1",
        "title": "A Field-Weighted model for Surface Layer Characterization using Single Channel Intensity Interrogation SPR",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Field-Weighted model for Surface Layer Characterization using Single Channel Intensity Interrogation SPR"
        },
        "updated": "2026-01-26T08:39:53Z",
        "updated_parsed": [
            2026,
            1,
            26,
            8,
            39,
            53,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18265v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18265v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "To address the difficulty of characterizing the surface layer rigorously, especially the thickness and refractive index (RI) in surface plasmon resonance (SPR) technology, we propose a field-weighted analysis method. This approach enables simultaneous quantitative determination of RI for the bulk solution and the surface layer. This study utilizes the aluminum-based Kretschmann structure with the intensity interrogation technique. We construct the field-weighted model governed by the evanescent field penetration depth to decompose the SPR reflected intensity into the bulk and surface responses. Experiments are conducted using bovine serum albumin (BSA) solution to form a surface adsorbed protein layer, and different concentrations of BSA are tested. Results show that the separated surface response fits well with the Langmuir formula, representing a significant improvement over the untreated SPR signal. The bulk and surface responses are then incorporated into the field-weighted model to determine the RI values of the bulk BSA solution and the surface adsorbed BSA layer at various concentrations. The experimental results of BSA solution match the Abbe refractometer measurements with a maximum error 0.0004 in RI, while the results of the adsorbed BSA layer, both the RI and thickness, aligned well with reported parameters for a single BSA layer. This method eliminates the stage rotation in the common angular interrogation SPR technique and complicated optical design and nano-fabrication in the nano-optics sensing schemes, making it suitable for compact, low-cost SPR platforms for practical applications needing surface layer characterization.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "To address the difficulty of characterizing the surface layer rigorously, especially the thickness and refractive index (RI) in surface plasmon resonance (SPR) technology, we propose a field-weighted analysis method. This approach enables simultaneous quantitative determination of RI for the bulk solution and the surface layer. This study utilizes the aluminum-based Kretschmann structure with the intensity interrogation technique. We construct the field-weighted model governed by the evanescent field penetration depth to decompose the SPR reflected intensity into the bulk and surface responses. Experiments are conducted using bovine serum albumin (BSA) solution to form a surface adsorbed protein layer, and different concentrations of BSA are tested. Results show that the separated surface response fits well with the Langmuir formula, representing a significant improvement over the untreated SPR signal. The bulk and surface responses are then incorporated into the field-weighted model to determine the RI values of the bulk BSA solution and the surface adsorbed BSA layer at various concentrations. The experimental results of BSA solution match the Abbe refractometer measurements with a maximum error 0.0004 in RI, while the results of the adsorbed BSA layer, both the RI and thickness, aligned well with reported parameters for a single BSA layer. This method eliminates the stage rotation in the common angular interrogation SPR technique and complicated optical design and nano-fabrication in the nano-optics sensing schemes, making it suitable for compact, low-cost SPR platforms for practical applications needing surface layer characterization."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T08:39:53Z",
        "published_parsed": [
            2026,
            1,
            26,
            8,
            39,
            53,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Zhiying Chen"
            },
            {
                "name": "Zihao Luo"
            },
            {
                "name": "Changsen Sun"
            },
            {
                "name": "Dmitry Kiesewetter"
            },
            {
                "name": "Sergey Krivosheev"
            },
            {
                "name": "Sergey Magazinov"
            },
            {
                "name": "Victor Malyugin"
            },
            {
                "name": "Xue Han"
            }
        ],
        "author_detail": {
            "name": "Xue Han"
        },
        "author": "Xue Han",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "使用单通道强度询问 SPR 进行表面层表征的场加权模型",
        "abstract_cn": "为了解决严格表征表面层的困难，特别是表面等离子体共振（SPR）技术中的厚度和折射率（RI），我们提出了一种场加权分析方法。这种方法能够同时定量测定本体溶液和表面层的 RI。本研究利用铝基 Kretschmann 结构和强度询问技术。我们构建了由渐逝场穿透深度控制的场加权模型，将 SPR 反射强度分解为体响应和表面响应。实验采用牛血清白蛋白（BSA）溶液形成表面吸附蛋白层，并测试了不同浓度的BSA。结果表明，分离的表面响应与 Langmuir 公式非常吻合，比未经处理的 SPR 信号有显着改善。然后将本体和表面响应纳入场加权模型中，以确定不同浓度下本体 BSA 溶液和表面吸附的 BSA 层的 RI 值。 BSA 溶液的实验结果与阿贝折射计测量结果相匹配，折射率最大误差为 0.0004，而吸附的 BSA 层的结果（折射率和厚度）与单个 BSA 层的报告参数非常一致。该方法消除了常见角度询问SPR技术中的载物台旋转以及纳米光学传感方案中复杂的光学设计和纳米制造，使其适用于需要表面层表征的实际应用的紧凑、低成本SPR平台。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18373v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18373v1",
        "title": "Atom-light hybrid interferometer for atomic sensing with quantum memory",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Atom-light hybrid interferometer for atomic sensing with quantum memory"
        },
        "updated": "2026-01-26T11:26:46Z",
        "updated_parsed": [
            2026,
            1,
            26,
            11,
            26,
            46,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18373v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18373v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Quantum memories feature a reversible conversion of optical fields into long-lived atomic spin waves, and are therefore ideal for operating as sensitive atomic sensors. However, up to now, atom-light interferometers have lacked an efficient approach to exploit their ultimate atomic sensing performance, since an extra optical delay line is required to compensate for the memory time. Here, we report a new protocol that records the photocurrent via heterodyne mixing with a stable local oscillator. The obtained complex quadrature amplitude that carries information imprinted on its phase by an external magnetic field, is successfully recovered from the interference patterns between the light and the atomic spin wave, without the stringent requirement of having them overlap in time. Our results reveal that the sensitivity scales favorably with the lifetime of the quantum memory. Our work may have important applications in building distributed quantum networks through quantum memory-assisted atom-light interferometers.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Quantum memories feature a reversible conversion of optical fields into long-lived atomic spin waves, and are therefore ideal for operating as sensitive atomic sensors. However, up to now, atom-light interferometers have lacked an efficient approach to exploit their ultimate atomic sensing performance, since an extra optical delay line is required to compensate for the memory time. Here, we report a new protocol that records the photocurrent via heterodyne mixing with a stable local oscillator. The obtained complex quadrature amplitude that carries information imprinted on its phase by an external magnetic field, is successfully recovered from the interference patterns between the light and the atomic spin wave, without the stringent requirement of having them overlap in time. Our results reveal that the sensitivity scales favorably with the lifetime of the quantum memory. Our work may have important applications in building distributed quantum networks through quantum memory-assisted atom-light interferometers."
        },
        "tags": [
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.atom-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T11:26:46Z",
        "published_parsed": [
            2026,
            1,
            26,
            11,
            26,
            46,
            0,
            26,
            0
        ],
        "arxiv_comment": "8 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "quant-ph"
        },
        "authors": [
            {
                "name": "Xingchang Wang"
            },
            {
                "name": "Xinyun Liang"
            },
            {
                "name": "Liang Dong"
            },
            {
                "name": "Ying Zuo"
            },
            {
                "name": "Jianmin Wang"
            },
            {
                "name": "Dasen Yang"
            },
            {
                "name": "Linyu Chen"
            },
            {
                "name": "Georgios A. Siviloglou"
            },
            {
                "name": "Z. Y. Ou"
            },
            {
                "name": "J. F. Chen"
            }
        ],
        "author_detail": {
            "name": "J. F. Chen"
        },
        "author": "J. F. Chen",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于具有量子存储器的原子传感的原子光混合干涉仪",
        "abstract_cn": "量子存储器具有将光场可逆转换为长寿命原子自旋波的功能，因此非常适合用作灵敏的原子传感器。然而，到目前为止，原子光干涉仪缺乏一种有效的方法来利用其最终的原子传感性能，因为需要额外的光学延迟线来补偿存储时间。在这里，我们报告了一种新协议，通过外差与稳定的本地振荡器混合来记录光电流。所获得的复正交幅度携带通过外部磁场印在其相位上的信息，可以从光和原子自旋波之间的干涉图案中成功恢复，而无需严格要求它们在时间上重叠。我们的结果表明，灵敏度与量子存储器的寿命成正比。我们的工作可能在通过量子存储辅助原子光干涉仪构建分布式量子网络方面具有重要的应用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18459v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18459v1",
        "title": "Phase conjugated master oscillator fiber power amplifier",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Phase conjugated master oscillator fiber power amplifier"
        },
        "updated": "2026-01-26T13:07:31Z",
        "updated_parsed": [
            2026,
            1,
            26,
            13,
            7,
            31,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18459v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18459v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "High-power narrow-linewidth fiber lasers are fundamentally limited by stimulated Brillouin scattering (SBS), which constrains further power scaling while maintaining spectral linewidth. Traditional mitigation techniques, such as active phase modulation, often introduce trade-offs among complexity, cost, and spectral brightness. In this study, we propose and experimentally demonstrate a novel all-optical approach for spectral linewidth manipulation and SBS suppression using optical phase conjugation (OPC). By leveraging nonlinear spectral broadening followed by phase conjugation, this method enables sophisticated linewidth narrowing in fiber amplifier, resulting in narrow linewidth output and enhanced SBS threshold. Using a low-cost fiber oscillator as the seed source, we achieve a spectral compression ratio exceeding 3 times. This method not only eliminates the need for complex electro-optic modulation systems but also provides a pathway toward simpler, high-brightness fiber laser systems. Our findings underscore the viability of OPC as a transformative tool for nonlinearity management and power scaling in high-performance fiber laser architectures.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "High-power narrow-linewidth fiber lasers are fundamentally limited by stimulated Brillouin scattering (SBS), which constrains further power scaling while maintaining spectral linewidth. Traditional mitigation techniques, such as active phase modulation, often introduce trade-offs among complexity, cost, and spectral brightness. In this study, we propose and experimentally demonstrate a novel all-optical approach for spectral linewidth manipulation and SBS suppression using optical phase conjugation (OPC). By leveraging nonlinear spectral broadening followed by phase conjugation, this method enables sophisticated linewidth narrowing in fiber amplifier, resulting in narrow linewidth output and enhanced SBS threshold. Using a low-cost fiber oscillator as the seed source, we achieve a spectral compression ratio exceeding 3 times. This method not only eliminates the need for complex electro-optic modulation systems but also provides a pathway toward simpler, high-brightness fiber laser systems. Our findings underscore the viability of OPC as a transformative tool for nonlinearity management and power scaling in high-performance fiber laser architectures."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T13:07:31Z",
        "published_parsed": [
            2026,
            1,
            26,
            13,
            7,
            31,
            0,
            26,
            0
        ],
        "arxiv_comment": "17 pages, 6 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Tingwei Gu"
            },
            {
                "name": "Xin Zeng"
            },
            {
                "name": "Huawei Jiang"
            },
            {
                "name": "Suming Luo"
            },
            {
                "name": "Maokai Yang"
            },
            {
                "name": "Xuezong Yang"
            },
            {
                "name": "Yan Feng"
            }
        ],
        "author_detail": {
            "name": "Yan Feng"
        },
        "author": "Yan Feng",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "相位共轭主振荡器光纤功率放大器",
        "abstract_cn": "高功率窄线宽光纤激光器从根本上受到受激布里渊散射（SBS）的限制，这在保持光谱线宽的同时限制了进一步的功率缩放。传统的缓解技术，例如有源相位调制，通常会在复杂性、成本和光谱亮度之间进行权衡。在这项研究中，我们提出并通过实验证明了一种利用光学相位共轭 (OPC) 进行光谱线宽操控和 SBS 抑制的新型全光学方法。通过利用非线性光谱展宽和相位共轭，该方法可以在光纤放大器中实现复杂的线宽窄化，从而实现窄线宽输出和增强的 SBS 阈值。使用低成本光纤振荡器作为种子源，我们实现了超过 3 倍的频谱压缩比。这种方法不仅消除了对复杂电光调制系统的需求，而且还提供了一条通往更简单、高亮度光纤激光系统的途径。我们的研究结果强调了 OPC 作为高性能光纤激光器架构中非线性管理和功率缩放变革工具的可行性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18501v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18501v1",
        "title": "Gravitational wave detectors from an experimental perspective",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Gravitational wave detectors from an experimental perspective"
        },
        "updated": "2026-01-26T14:06:40Z",
        "updated_parsed": [
            2026,
            1,
            26,
            14,
            6,
            40,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18501v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18501v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This chapter introduces the fundamental principles of gravitational wave detectors in a simple and comprehensive manner. Because these instruments aim for extremely high sensitivity, it is essential to understand their various noise sources, how such noise couples to the detector output, and the strategies used to mitigate them. These noises contributions are computed in the frame of the Virgo detector and a sensitivity curve is calculated. Although a simplified layout of a gravitational wave detector is considered, it takes into account the most dominant effects and yields in a sensitivity estimate close to the what is observed in real detectors.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This chapter introduces the fundamental principles of gravitational wave detectors in a simple and comprehensive manner. Because these instruments aim for extremely high sensitivity, it is essential to understand their various noise sources, how such noise couples to the detector output, and the strategies used to mitigate them. These noises contributions are computed in the frame of the Virgo detector and a sensitivity curve is calculated. Although a simplified layout of a gravitational wave detector is considered, it takes into account the most dominant effects and yields in a sensitivity estimate close to the what is observed in real detectors."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "astro-ph.IM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "gr-qc",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T14:06:40Z",
        "published_parsed": [
            2026,
            1,
            26,
            14,
            6,
            40,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Marina Trad-Nery"
            },
            {
                "name": "Margherita Turconi"
            },
            {
                "name": "Walid Chaibi"
            }
        ],
        "author_detail": {
            "name": "Walid Chaibi"
        },
        "author": "Walid Chaibi",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "从实验角度看引力波探测器",
        "abstract_cn": "本章简单全面地介绍了引力波探测器的基本原理。由于这些仪器的目标是极高的灵敏度，因此有必要了解它们的各种噪声源、这些噪声如何耦合到探测器输出以及用于减轻噪声的策略。这些噪声贡献在 Virgo 探测器的框架中计算，并计算灵敏度曲线。尽管考虑了引力波探测器的简化布局，但它考虑了最主要的影响，并且灵敏度估计结果接近于实际探测器中观察到的结果。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18550v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18550v1",
        "title": "An exploration of lateral optical forces from a triangular periodic motif",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "An exploration of lateral optical forces from a triangular periodic motif"
        },
        "updated": "2026-01-26T14:57:20Z",
        "updated_parsed": [
            2026,
            1,
            26,
            14,
            57,
            20,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18550v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18550v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This computational study investigates lateral optical forces in asymmetric dielectric nanostructures, focusing on their connection to resonant light-matter interactions. We examine isosceles triangular motifs that exhibit two distinct types of optical force response under plane wave illumination. Through parameter-space analysis, we identify stable zones where optical forces remain consistent and switching bands where forces change abruptly as parameters are altered. The observed force spectra show characteristic asymmetric lineshapes, suggesting Fano-resonance behavior. Eigenfrequency analysis confirms these effects arise from interference between discrete eigenmodes and continuum propagation states, with the eigenmode Q-factors correlating with transition sharpness. These findings provide insights into how structural geometry influences optical forces through resonant effects, offering guidance for designing optically-driven systems where controlled optical force responses are desired.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This computational study investigates lateral optical forces in asymmetric dielectric nanostructures, focusing on their connection to resonant light-matter interactions. We examine isosceles triangular motifs that exhibit two distinct types of optical force response under plane wave illumination. Through parameter-space analysis, we identify stable zones where optical forces remain consistent and switching bands where forces change abruptly as parameters are altered. The observed force spectra show characteristic asymmetric lineshapes, suggesting Fano-resonance behavior. Eigenfrequency analysis confirms these effects arise from interference between discrete eigenmodes and continuum propagation states, with the eigenmode Q-factors correlating with transition sharpness. These findings provide insights into how structural geometry influences optical forces through resonant effects, offering guidance for designing optically-driven systems where controlled optical force responses are desired."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.comp-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T14:57:20Z",
        "published_parsed": [
            2026,
            1,
            26,
            14,
            57,
            20,
            0,
            26,
            0
        ],
        "arxiv_comment": "24 pages, 7 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Bo Gao"
            },
            {
                "name": "Henkjan Gersen"
            },
            {
                "name": "Simon Hanna"
            }
        ],
        "author_detail": {
            "name": "Simon Hanna"
        },
        "author": "Simon Hanna",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "从三角形周期图案探索横向光学力",
        "abstract_cn": "这项计算研究研究了不对称介电纳米结构中的横向光学力，重点关注它们与共振光-物质相互作用的联系。我们研究了等腰三角形图案，它们在平面波照明下表现出两种不同类型的光学力响应。通过参数空间分析，我们确定了光学力保持一致的稳定区域和随着参数改变而力突然变化的切换带。观察到的力谱显示出特征性的不对称线形，表明存在法诺共振行为。本征频率分析证实这些效应是由离散本征模和连续传播状态之间的干扰引起的，本征模 Q 因子与过渡锐度相关。这些发现提供了关于结构几何如何通过共振效应影响光学力的见解，为设计需要受控光学力响应的光学驱动系统提供了指导。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18574v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18574v1",
        "title": "Hierarchical self-organization of highly-ordered granular ensemble of optical solitons through collective motions",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Hierarchical self-organization of highly-ordered granular ensemble of optical solitons through collective motions"
        },
        "updated": "2026-01-26T15:18:15Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            18,
            15,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18574v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18574v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Self-organizations of ordered patterns in far-from-equilibrium many-body systems host fundamental importance in many disciplines. Meanwhile, complex systems often feature hierarchical structures with distinct scales for different layers, enabling high-level effective dynamics without exhaustive tracking of all possible degrees of freedoms. In this work, we report a study of the self-organization dynamics of highly-ordered soliton ensembles in a high-harmonic mode-locked fiber lasers through collective motions driven by nonlocal optomechanical interactions and local collisions, which exhibit a series of universal characteristics reminiscent of phase transitions. Moreover, the multi-soliton laser-field can be coarsely grained as a granular ensemble of limit-cycle oscillators with simple interaction rules derived from fine-scale physics. The self-organization of the multitude of solitons in the mode-locked laser cavity can then be mapped into a low-dimensional dynamic model that essentially reproduced the emergent process. Our work affords a conceptual framework for understanding the complex structure formation in nonlinear laser systems, and may help to design ultrafast lasers by exploiting universal principles of collective motions.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Self-organizations of ordered patterns in far-from-equilibrium many-body systems host fundamental importance in many disciplines. Meanwhile, complex systems often feature hierarchical structures with distinct scales for different layers, enabling high-level effective dynamics without exhaustive tracking of all possible degrees of freedoms. In this work, we report a study of the self-organization dynamics of highly-ordered soliton ensembles in a high-harmonic mode-locked fiber lasers through collective motions driven by nonlocal optomechanical interactions and local collisions, which exhibit a series of universal characteristics reminiscent of phase transitions. Moreover, the multi-soliton laser-field can be coarsely grained as a granular ensemble of limit-cycle oscillators with simple interaction rules derived from fine-scale physics. The self-organization of the multitude of solitons in the mode-locked laser cavity can then be mapped into a low-dimensional dynamic model that essentially reproduced the emergent process. Our work affords a conceptual framework for understanding the complex structure formation in nonlinear laser systems, and may help to design ultrafast lasers by exploiting universal principles of collective motions."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:18:15Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            18,
            15,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Xiaocong Wang"
            },
            {
                "name": "Benhai Wang"
            },
            {
                "name": "Haochen Lin"
            },
            {
                "name": "Wenbin He"
            },
            {
                "name": "Yu Jiang"
            },
            {
                "name": "Qi Huang"
            },
            {
                "name": "Xintong Zhang"
            },
            {
                "name": "Long Zhang"
            },
            {
                "name": "Meng Pang"
            }
        ],
        "author_detail": {
            "name": "Meng Pang"
        },
        "author": "Meng Pang",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过集体运动的高有序光孤子粒系综的层次自组织",
        "abstract_cn": "远非平衡多体系统中有序模式的自组织在许多学科中具有根本重要性。同时，复杂系统通常具有不同层具有不同尺度的分层结构，从而无需详尽跟踪所有可能的自由度即可实现高级有效动态。在这项工作中，我们报告了通过非局域光机械相互作用和局域碰撞驱动的集体运动来研究高次谐波锁模光纤激光器中高阶孤子系综的自组织动力学，其表现出一系列让人想起相变的普遍特征。此外，多孤子激光场可以被粗粒度化为具有源自精细尺度物理学的简单相互作用规则的极限环振荡器的粒状集合。然后，锁模激光腔中大量孤子的自组织可以映射到一个低维动态模型中，该模型本质上再现了涌现过程。我们的工作为理解非线性激光系统中复杂结构的形成提供了一个概念框架，并可能有助于通过利用集体运动的通用原理来设计超快激光器。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18583v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18583v1",
        "title": "Uncooled Poisson Bolometer for High-Speed Event-Based Long-wave Thermal Imaging",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Uncooled Poisson Bolometer for High-Speed Event-Based Long-wave Thermal Imaging"
        },
        "updated": "2026-01-26T15:29:12Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            29,
            12,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18583v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18583v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Event-based vision provides high-speed, energy-efficient sensing for applications such as autonomous navigation and motion tracking. However, implementing this technology in the long-wave infrared remains a significant challenge. Traditional infrared sensors are hindered by slow thermal response times or the heavy power requirements of cryogenic cooling. Here, we introduce the first event-based infrared detector operating in a Poisson-counting regime. This is realized with a spintronic Poisson bolometer capable of broadband detection from 0.8-14$μ\\text{m}$. In this regime, infrared signals are detected through statistically resolvable changes in stochastic switching events. This approach enables room-temperature operation with high timing resolution. Our device achieves a maximum event rate of 1,250 Hz, surpassing the temporal resolution of conventional uncooled microbolometers by a factor of 4. Power consumption is kept low at 0.2$μ$W per pixel. This work establishes an operating principle for infrared sensing and demonstrates a pathway toward high-speed, energy-efficient, event-driven thermal imaging.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Event-based vision provides high-speed, energy-efficient sensing for applications such as autonomous navigation and motion tracking. However, implementing this technology in the long-wave infrared remains a significant challenge. Traditional infrared sensors are hindered by slow thermal response times or the heavy power requirements of cryogenic cooling. Here, we introduce the first event-based infrared detector operating in a Poisson-counting regime. This is realized with a spintronic Poisson bolometer capable of broadband detection from 0.8-14$μ\\text{m}$. In this regime, infrared signals are detected through statistically resolvable changes in stochastic switching events. This approach enables room-temperature operation with high timing resolution. Our device achieves a maximum event rate of 1,250 Hz, surpassing the temporal resolution of conventional uncooled microbolometers by a factor of 4. Power consumption is kept low at 0.2$μ$W per pixel. This work establishes an operating principle for infrared sensing and demonstrates a pathway toward high-speed, energy-efficient, event-driven thermal imaging."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:29:12Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            29,
            12,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Mohamed A. Mousa"
            },
            {
                "name": "Leif Bauer"
            },
            {
                "name": "Utkarsh Singh"
            },
            {
                "name": "Ziyi Yang"
            },
            {
                "name": "Angshuman Deka"
            },
            {
                "name": "Zubin Jacob"
            }
        ],
        "author_detail": {
            "name": "Zubin Jacob"
        },
        "author": "Zubin Jacob",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于基于事件的高速长波热成像的非制冷泊松测辐射热计",
        "abstract_cn": "基于事件的视觉为自主导航和运动跟踪等应用提供高速、节能的传感。然而，在长波红外中实施这项技术仍然是一个重大挑战。传统的红外传感器受到缓慢的热响应时间或低温冷却的高功率需求的阻碍。在这里，我们介绍第一个在泊松计数机制下运行的基于事件的红外探测器。这是通过能够进行 0.8-14$μ\\text{m}$ 宽带检测的自旋电子泊松测辐射热计来实现的。在这种情况下，红外信号是通过随机开关事件中统计上可解析的变化来检测的。这种方法可以在室温下以高时序分辨率运行。我们的设备实现了 1,250 Hz 的最大事件速率，超过传统非制冷微测辐射热计的时间分辨率 4 倍。功耗保持在每像素 0.2$μ$W 的低水平。这项工作建立了红外传感的工作原理，并展示了通往高速、节能、事件驱动热成像的途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18609v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18609v1",
        "title": "Hybrid integrated narrow linewidth laser with external distributed optical feedback from a silicon strip waveguide",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Hybrid integrated narrow linewidth laser with external distributed optical feedback from a silicon strip waveguide"
        },
        "updated": "2026-01-26T15:48:32Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            48,
            32,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18609v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18609v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "External optical feedback via Rayleigh scattering from an integrated microresonator or an optical fiber has been demonstrated to significantly narrow the intrinsic linewidth of semiconductor lasers. Wavelength matching between the lasing cavity and the external high-Q microresonator is required to accumulate Rayleigh scattering based optical feedback. Optical fiber can provide Rayleigh scattering based optical feedback for any lasing wavelength. However, optical fibers hundreds of meters or even kilometers long are required for the accumulation of Rayleigh scattering based optical feedback, hindering the integration of narrow linewidth lasers. Here, we present an integrated scheme that collects distributed feedback signal with weak wavelength dependence by exploiting surface radiation in a silicon waveguide. The effects of waveguide width on the intensities of the surface radiation and distributed optical feedback signal are first numerically analyzed by introducing a collection coefficient. Numerical calculations show that a 1 μm-wide strip waveguide yields optimal performance for excitation and collection of distributed optical feedback, which is also experimentally verified by measuring the feedback signal with an optical frequency-domain reflectometry. Benefitting from the enhanced distributed optical feedback that is 34.72 dB higher than that in a single-mode fiber, the hybrid integrated laser demonstrates an intrinsic linewidth of 1.52 kHz, a side-mode suppression ratio (SMSR) of 74.71 dB, and a frequency noise of 24.44 Hz2/Hz. Furthermore, within a maximum allowable wavelength tuning range of 2.342 nm, the linewidth narrowing ratio depends little on the wavelength for all the waveguides with different widths.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "External optical feedback via Rayleigh scattering from an integrated microresonator or an optical fiber has been demonstrated to significantly narrow the intrinsic linewidth of semiconductor lasers. Wavelength matching between the lasing cavity and the external high-Q microresonator is required to accumulate Rayleigh scattering based optical feedback. Optical fiber can provide Rayleigh scattering based optical feedback for any lasing wavelength. However, optical fibers hundreds of meters or even kilometers long are required for the accumulation of Rayleigh scattering based optical feedback, hindering the integration of narrow linewidth lasers. Here, we present an integrated scheme that collects distributed feedback signal with weak wavelength dependence by exploiting surface radiation in a silicon waveguide. The effects of waveguide width on the intensities of the surface radiation and distributed optical feedback signal are first numerically analyzed by introducing a collection coefficient. Numerical calculations show that a 1 μm-wide strip waveguide yields optimal performance for excitation and collection of distributed optical feedback, which is also experimentally verified by measuring the feedback signal with an optical frequency-domain reflectometry. Benefitting from the enhanced distributed optical feedback that is 34.72 dB higher than that in a single-mode fiber, the hybrid integrated laser demonstrates an intrinsic linewidth of 1.52 kHz, a side-mode suppression ratio (SMSR) of 74.71 dB, and a frequency noise of 24.44 Hz2/Hz. Furthermore, within a maximum allowable wavelength tuning range of 2.342 nm, the linewidth narrowing ratio depends little on the wavelength for all the waveguides with different widths."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:48:32Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            48,
            32,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Da Wei"
            },
            {
                "name": "Leilei Shi"
            },
            {
                "name": "Yujia Li"
            },
            {
                "name": "Minzhi Xu"
            },
            {
                "name": "Chaoze Zhang"
            },
            {
                "name": "Xianming Huang"
            },
            {
                "name": "Jianxian Yu"
            },
            {
                "name": "Lei Zhai"
            },
            {
                "name": "Wenxuan Huang"
            },
            {
                "name": "Huan Tian"
            },
            {
                "name": "Tao Zhu"
            }
        ],
        "author_detail": {
            "name": "Tao Zhu"
        },
        "author": "Tao Zhu",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "混合集成窄线宽激光器，具有来自硅带波导的外部分布式光反馈",
        "abstract_cn": "已证明，通过来自集成微谐振器或光纤的瑞利散射的外部光学反馈可以显着缩小半导体激光器的固有线宽。激光腔和外部高 Q 微谐振器之间的波长匹配需要累积基于瑞利散射的光学反馈。光纤可以为任何激光波长提供基于瑞利散射的光反馈。然而，基于瑞利散射的光反馈的积累需要数百米甚至公里长的光纤，这阻碍了窄线宽激光器的集成。在这里，我们提出了一种集成方案，通过利用硅波导中的表面辐射来收集具有弱波长依赖性的分布式反馈信号。首先通过引入收集系数对波导宽度对表面辐射和分布式光反馈信号强度的影响进行数值分析。数值计算表明，1 μm 宽的条形波导在激励和收集分布式光反馈方面具有最佳性能，这也通过光频域反射计测量反馈信号进行了实验验证。得益于比单模光纤高 34.72 dB 的增强型分布式光反馈，混合集成激光器表现出 1.52 kHz 的固有线宽、74.71 dB 的边模抑制比 (SMSR) 和 24.44 Hz2/Hz 的频率噪声。此外，在2.342 nm的最大允许波长调谐范围内，对于所有不同宽度的波导，线宽窄化比对波长的依赖性很小。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18611v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18611v1",
        "title": "Broadband tunable narrow-linewidth laser based on scattering-enhanced fiber covering E-S-C-L bands",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Broadband tunable narrow-linewidth laser based on scattering-enhanced fiber covering E-S-C-L bands"
        },
        "updated": "2026-01-26T15:52:37Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            52,
            37,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18611v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18611v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This work demonstrates a broadband tunable narrow-linewidth laser based on scattering-enhanced fiber, covering the E-S-C-L wavelength bands from 1337.47 nm to 1631.39 nm, with a total tuning span of 293.92 nm. The laser employs two semiconductor optical amplifiers (SOAs) centered at 1420 nm and 1550 nm, which are connected into a single ring resonator via polarization multiplexing. Wavelength selection and tunability is realized using an ultra-broadband tunable filter based on a blazed grating. To suppress side longitude modes, an 18-meter-long femtosecond-laser-empowered random scattering fiber is utilized inside the cavity as a feedback medium, yielding an output linewidths between 1.54 kHz and 2.61 kHz. Benefited from the fast response of the galvanometer mirror and short relaxation time of SOAs, wavelength switching time is less than 1 ms under different tuning channels among the wavelength range of near 300 nm. The stable single-longitude-mode operation is maintained across the entire tuning range. The exceptionally broad tuning range and high spectral purity of the laser endow it with significant application potentials across a wide range of fields.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This work demonstrates a broadband tunable narrow-linewidth laser based on scattering-enhanced fiber, covering the E-S-C-L wavelength bands from 1337.47 nm to 1631.39 nm, with a total tuning span of 293.92 nm. The laser employs two semiconductor optical amplifiers (SOAs) centered at 1420 nm and 1550 nm, which are connected into a single ring resonator via polarization multiplexing. Wavelength selection and tunability is realized using an ultra-broadband tunable filter based on a blazed grating. To suppress side longitude modes, an 18-meter-long femtosecond-laser-empowered random scattering fiber is utilized inside the cavity as a feedback medium, yielding an output linewidths between 1.54 kHz and 2.61 kHz. Benefited from the fast response of the galvanometer mirror and short relaxation time of SOAs, wavelength switching time is less than 1 ms under different tuning channels among the wavelength range of near 300 nm. The stable single-longitude-mode operation is maintained across the entire tuning range. The exceptionally broad tuning range and high spectral purity of the laser endow it with significant application potentials across a wide range of fields."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:52:37Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            52,
            37,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Minzhi Xu"
            },
            {
                "name": "Zechun Geng"
            },
            {
                "name": "Da Wei"
            },
            {
                "name": "Yujia Li"
            },
            {
                "name": "Juntao He"
            },
            {
                "name": "Chaoze Zhang"
            },
            {
                "name": "Wei Du"
            },
            {
                "name": "Lei Gao"
            },
            {
                "name": "Leilei Shi"
            },
            {
                "name": "Ligang Huang"
            },
            {
                "name": "Jindong Wang"
            },
            {
                "name": "Tao Zhu"
            }
        ],
        "author_detail": {
            "name": "Tao Zhu"
        },
        "author": "Tao Zhu",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "基于覆盖E-S-C-L波段的散射增强光纤的宽带可调谐窄线宽激光器",
        "abstract_cn": "这项工作展示了一种基于散射增强光纤的宽带可调谐窄线宽激光器，覆盖了从1337.47 nm到1631.39 nm的E-S-C-L波长带，总调谐跨度为293.92 nm。该激光器采用两个以 1420 nm 和 1550 nm 为中心的半导体光放大器 (SOA)，通过偏振复用连接到一个单环谐振器。使用基于闪耀光栅的超宽带可调谐滤波器实现波长选择和可调谐。为了抑制侧经模，腔体内使用了 18 米长的飞秒激光赋能随机散射光纤作为反馈介质，产生 1.54 kHz 至 2.61 kHz 之间的输出线宽。得益于检流镜的快速响应和SOA的弛豫时间短，在300 nm附近的波长范围内，不同调谐通道下的波长切换时间小于1 ms。在整个调谐范围内保持稳定的单经度模式操作。激光器极其宽广的调谐范围和高光谱纯度使其在广泛的领域具有巨大的应用潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18624v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18624v1",
        "title": "Spin-redirection Berry phase with planar rays",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Spin-redirection Berry phase with planar rays"
        },
        "updated": "2026-01-26T16:01:21Z",
        "updated_parsed": [
            2026,
            1,
            26,
            16,
            1,
            21,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18624v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18624v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Geometric or Berry phases are fundamental manifestations that appear in many areas of physics. They arise from the geometry of the space describing the properties of multi-component wave fields. An important example for electromagnetic waves is the spin-redirection Berry phase associated with the evolution of the spin direction. Because this effect has traditionally been studied in isotropic media where the spin is aligned with the ray trajectory, it has become commonly assumed that this spin-redirection Berry phase requires nonplanar rays. Here we show that a spin-redirection phase can in fact arise along a planar ray if the spin evolves along the ray. We expose this effect through the singular example of a moving unmagnetized plasma, and demonstrate how this behavior can more generally arise from a finite transverse spin. In identifying this new spin-redirection mechanism our work not only provides the tools to discover additional manifestations of SOIs in nature, but also uncovers supplemental degrees of freedom to harness SOIs to control light.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Geometric or Berry phases are fundamental manifestations that appear in many areas of physics. They arise from the geometry of the space describing the properties of multi-component wave fields. An important example for electromagnetic waves is the spin-redirection Berry phase associated with the evolution of the spin direction. Because this effect has traditionally been studied in isotropic media where the spin is aligned with the ray trajectory, it has become commonly assumed that this spin-redirection Berry phase requires nonplanar rays. Here we show that a spin-redirection phase can in fact arise along a planar ray if the spin evolves along the ray. We expose this effect through the singular example of a moving unmagnetized plasma, and demonstrate how this behavior can more generally arise from a finite transverse spin. In identifying this new spin-redirection mechanism our work not only provides the tools to discover additional manifestations of SOIs in nature, but also uncovers supplemental degrees of freedom to harness SOIs to control light."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.plasm-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T16:01:21Z",
        "published_parsed": [
            2026,
            1,
            26,
            16,
            1,
            21,
            0,
            26,
            0
        ],
        "arxiv_comment": "3 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Aymeric Braud"
            },
            {
                "name": "Renaud Gueroult"
            }
        ],
        "author_detail": {
            "name": "Renaud Gueroult"
        },
        "author": "Renaud Gueroult",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "具有平面射线的自旋重定向贝里相",
        "abstract_cn": "几何相或贝里相是物理学许多领域中出现的基本表现形式。它们源自描述多分量波场特性的空间几何形状。电磁波的一个重要例子是与自旋方向演化相关的自旋重定向贝里相。由于这种效应传统上是在自旋与射线轨迹对齐的各向同性介质中研究的，因此人们普遍认为这种自旋重定向贝里相需要非平面射线。在这里，我们证明，如果自旋沿着射线演化，自旋重定向相实际上可以沿着平面射线出现。我们通过移动的未磁化等离子体的奇异例子揭示了这种效应，并演示了这种行为如何更普遍地由有限横向自旋产生。在识别这种新的自旋重定向机制时，我们的工作不仅提供了发现自然界中 SOI 的其他表现形式的工具，而且还揭示了利用 SOI 控制光的补充自由度。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18712v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18712v1",
        "title": "Giant Resonant Enhancement of Photoinduced Dynamical Cooper Pairing, far above $T_c$",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Giant Resonant Enhancement of Photoinduced Dynamical Cooper Pairing, far above $T_c$"
        },
        "updated": "2026-01-26T17:38:44Z",
        "updated_parsed": [
            2026,
            1,
            26,
            17,
            38,
            44,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18712v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18712v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Pump-probe experiments performed on $\\mathrm{K}_3\\mathrm{C}_{60}$ have unveiled both optical and transport signatures of metastable light-induced superconductivity up to room temperature, far above $T_c$. Recent experiments have uncovered that excitation in the vicinity of $50 ~\\textrm{meV}$ enables the observation of high temperature light-induced superconductivity at significantly lower fluences. Inspired by these experiments we develop a mechanism which can explain such a giant resonant enhancement of light-induced superconductivity. Within a minimal non-linear Holstein model, we show that resonantly driving optical Raman modes leads to a time-dependent electron-phonon coupling. Such a coupling then modulates the effective electron-electron attraction, with the strongest modulations occurring when the drive is resonant with the phonon frequency. These dynamical modulations of the pairing interactions lead to Floquet-BCS instabilities at temperatures far exceeding equilibrium $T_c$, as observed in experiments. We conclude by discussing the implications of our general analysis on the $\\mathrm{K}_3\\mathrm{C}_{60}$ experiments specifically and suggesting experimental signatures of our mechanism.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Pump-probe experiments performed on $\\mathrm{K}_3\\mathrm{C}_{60}$ have unveiled both optical and transport signatures of metastable light-induced superconductivity up to room temperature, far above $T_c$. Recent experiments have uncovered that excitation in the vicinity of $50 ~\\textrm{meV}$ enables the observation of high temperature light-induced superconductivity at significantly lower fluences. Inspired by these experiments we develop a mechanism which can explain such a giant resonant enhancement of light-induced superconductivity. Within a minimal non-linear Holstein model, we show that resonantly driving optical Raman modes leads to a time-dependent electron-phonon coupling. Such a coupling then modulates the effective electron-electron attraction, with the strongest modulations occurring when the drive is resonant with the phonon frequency. These dynamical modulations of the pairing interactions lead to Floquet-BCS instabilities at temperatures far exceeding equilibrium $T_c$, as observed in experiments. We conclude by discussing the implications of our general analysis on the $\\mathrm{K}_3\\mathrm{C}_{60}$ experiments specifically and suggesting experimental signatures of our mechanism."
        },
        "tags": [
            {
                "term": "cond-mat.supr-con",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.str-el",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T17:38:44Z",
        "published_parsed": [
            2026,
            1,
            26,
            17,
            38,
            44,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cond-mat.supr-con"
        },
        "authors": [
            {
                "name": "Sambuddha Chattopadhyay"
            },
            {
                "name": "Marios Michael"
            },
            {
                "name": "Andrea Cavalleri"
            },
            {
                "name": "Eugene Demler"
            }
        ],
        "author_detail": {
            "name": "Eugene Demler"
        },
        "author": "Eugene Demler",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "光致动态库珀配对的巨大共振增强，远高于 $T_c$",
        "abstract_cn": "在 $\\mathrm{K}_3\\mathrm{C}_{60}$ 上进行的泵浦探针实验揭示了亚稳态光诱导超导性的光学和传输特征，直至室温，远高于 $T_c$。最近的实验发现，在 $50 ~\\textrm{meV}$ 附近的激发能够在明显较低的注量下观察到高温光感超导性。受这些实验的启发，我们开发了一种机制，可以解释光诱导超导性的如此巨大的共振增强。在最小非线性荷斯坦模型中，我们表明共振驱动光学拉曼模式会导致时间相关的电子声子耦合。然后，这种耦合会调制有效的电子-电子吸引力，当驱动器与声子频率谐振时会发生最强的调制。正如实验中观察到的，配对相互作用的这些动态调制导致 Floquet-BCS 在远远超过平衡 $T_c$ 的温度下不稳定。最后，我们具体讨论了一般分析对 $\\mathrm{K}_3\\mathrm{C}_{60}$ 实验的影响，并提出了我们机制的实验特征。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18746v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18746v1",
        "title": "Coherent control of photon pairs via quantum interference between second- and third-order quantum nonlinear processes",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Coherent control of photon pairs via quantum interference between second- and third-order quantum nonlinear processes"
        },
        "updated": "2026-01-26T18:06:57Z",
        "updated_parsed": [
            2026,
            1,
            26,
            18,
            6,
            57,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18746v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18746v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Genuine quantum interference between independent nonlinear processes of different order provides a route to coherent control that cannot be reduced to a classical field interference. Here we present an all-optical analogue of coherent carrier injection by exploiting interference between second- and third-order quantum nonlinear processes in an integrated photonic platform. Photon pairs generated via spontaneous parametric down-conversion and spontaneous four-wave mixing coherently contribute to the same final two-photon state, resulting in a phase-dependent modulation of both the generation rate and the spectral structure of the emitted biphoton state. We illustrate the features of such interference and how it can be used to shape biphoton wavefunctions and their quantum correlations. These results identify interference between nonlinear processes of different order as a distinct form of coherent quantum control within quantum nonlinear optics.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Genuine quantum interference between independent nonlinear processes of different order provides a route to coherent control that cannot be reduced to a classical field interference. Here we present an all-optical analogue of coherent carrier injection by exploiting interference between second- and third-order quantum nonlinear processes in an integrated photonic platform. Photon pairs generated via spontaneous parametric down-conversion and spontaneous four-wave mixing coherently contribute to the same final two-photon state, resulting in a phase-dependent modulation of both the generation rate and the spectral structure of the emitted biphoton state. We illustrate the features of such interference and how it can be used to shape biphoton wavefunctions and their quantum correlations. These results identify interference between nonlinear processes of different order as a distinct form of coherent quantum control within quantum nonlinear optics."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T18:06:57Z",
        "published_parsed": [
            2026,
            1,
            26,
            18,
            6,
            57,
            0,
            26,
            0
        ],
        "arxiv_comment": "9 pages, 6 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Alessia Stefano"
            },
            {
                "name": "Samuel E. Fontaine"
            },
            {
                "name": "J. E. Sipe"
            },
            {
                "name": "Marco Liscidini"
            }
        ],
        "author_detail": {
            "name": "Marco Liscidini"
        },
        "author": "Marco Liscidini",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过二阶和三阶量子非线性过程之间的量子干涉对光子对进行相干控制",
        "abstract_cn": "不同阶的独立非线性过程之间的真正量子干涉提供了一条无法简化为经典场干涉的相干控制途径。在这里，我们通过利用集成光子平台中二阶和三阶量子非线性过程之间的干涉，提出了相干载流子注入的全光学模拟。通过自发参量下转换和自发四波混频产生的光子对相干地有助于相同的最终双光子状态，从而导致发射双光子状态的生成速率和光谱结构的相位相关调制。我们说明了这种干涉的特征以及如何使用它来塑造双光子波函数及其量子相关性。这些结果将不同阶非线性过程之间的干涉识别为量子非线性光学中相干量子控制的一种独特形式。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18797v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18797v1",
        "title": "On-chip control of the coherence matrix of four-mode partially coherent light: rank, entropy, and modal Stokes parameters",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "On-chip control of the coherence matrix of four-mode partially coherent light: rank, entropy, and modal Stokes parameters"
        },
        "updated": "2026-01-26T18:59:48Z",
        "updated_parsed": [
            2026,
            1,
            26,
            18,
            59,
            48,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18797v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18797v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Partially coherent light offers salutary capabilities in optical information processing that cannot be matched by coherent light. To date, this `coherence advantage' has been confirmed in proof-of-principle optical communications protocols using bulk optics. Taking full advantage of such opportunities necessitates processing multimode partially coherent light in integrated photonics platforms that alone provide the requisite stability for cascaded operations on a large scale. Here we demonstrate on-chip manipulation of four-mode partially coherent light described by a $4\\times4$ Hermitian coherence matrix. Starting with generic maximally incoherent light, we utilize an on-chip hexagonal mesh of Mach-Zehnder interferometers to perform all the unitary and non-unitary tasks that are critical for realizing structured coherence: controlling the coherence rank (the number of non-zero eigenvalues of the coherence matrix); tuning the field entropy; molding the structure of the coherence matrix via $4\\times4$ unitary transformations constructed out of sequences of $2\\times2$ unitaries acting on pairs of modes; and tomographic reconstruction of the coherence matrix by measuring the modal Stokes parameters associated with Kronecker Pauli matrices. These results confirm the scalability of utilizing $2\\times2$ on-chip building blocks for the synthesis and reconstruction of high-dimensional coherence matrices, and provide a decisive step towards large-scale on-chip manipulation of massively moded partially coherent light for applications in optical information processing.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Partially coherent light offers salutary capabilities in optical information processing that cannot be matched by coherent light. To date, this `coherence advantage' has been confirmed in proof-of-principle optical communications protocols using bulk optics. Taking full advantage of such opportunities necessitates processing multimode partially coherent light in integrated photonics platforms that alone provide the requisite stability for cascaded operations on a large scale. Here we demonstrate on-chip manipulation of four-mode partially coherent light described by a $4\\times4$ Hermitian coherence matrix. Starting with generic maximally incoherent light, we utilize an on-chip hexagonal mesh of Mach-Zehnder interferometers to perform all the unitary and non-unitary tasks that are critical for realizing structured coherence: controlling the coherence rank (the number of non-zero eigenvalues of the coherence matrix); tuning the field entropy; molding the structure of the coherence matrix via $4\\times4$ unitary transformations constructed out of sequences of $2\\times2$ unitaries acting on pairs of modes; and tomographic reconstruction of the coherence matrix by measuring the modal Stokes parameters associated with Kronecker Pauli matrices. These results confirm the scalability of utilizing $2\\times2$ on-chip building blocks for the synthesis and reconstruction of high-dimensional coherence matrices, and provide a decisive step towards large-scale on-chip manipulation of massively moded partially coherent light for applications in optical information processing."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T18:59:48Z",
        "published_parsed": [
            2026,
            1,
            26,
            18,
            59,
            48,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Amin Hashemi"
            },
            {
                "name": "Abbas Shiri"
            },
            {
                "name": "Bahaa E. A. Saleh"
            },
            {
                "name": "Andrea Blanco-Redondo"
            },
            {
                "name": "Ayman F. Abouraddy"
            }
        ],
        "author_detail": {
            "name": "Ayman F. Abouraddy"
        },
        "author": "Ayman F. Abouraddy",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "四模部分相干光相干矩阵的片上控制：秩、熵和模态斯托克斯参数",
        "abstract_cn": "部分相干光在光学信息处理方面提供了相干光无法比拟的有益能力。迄今为止，这种“相干性优势”已在使用体光学器件的原理验证光通信协议中得到证实。充分利用这些机会需要在集成光子学平台中处理多模部分相干光，该平台本身就可以为大规模级联操作提供必要的稳定性。在这里，我们演示了由 $4\\times4$ 埃尔米特相干矩阵描述的四模部分相干光的片上操作。从通用最大非相干光开始，我们利用马赫-曾德干涉仪的片上六边形网格来执行对于实现结构化相干性至关重要的所有酉和非酉任务：控制相干等级（相干矩阵的非零特征值的数量）；调整场熵；通过由作用于模式对的 $2\\times2$ 酉序列构造的 $4\\times4$ 酉变换来塑造相干矩阵的结构；通过测量与克罗内克泡利矩阵相关的模态斯托克斯参数来重建相干矩阵。这些结果证实了利用 2\\times2$ 片上构建模块来合成和重建高维相干矩阵的可扩展性，并为光学信息处理应用中大规模模制部分相干光的大规模片上操作迈出了决定性的一步。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17143v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17143v1",
        "title": "Fully 3D Unrolled Magnetic Resonance Fingerprinting Reconstruction via Staged Pretraining and Implicit Gridding",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fully 3D Unrolled Magnetic Resonance Fingerprinting Reconstruction via Staged Pretraining and Implicit Gridding"
        },
        "updated": "2026-01-23T19:47:47Z",
        "updated_parsed": [
            2026,
            1,
            23,
            19,
            47,
            47,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17143v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17143v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Magnetic Resonance Fingerprinting (MRF) enables fast quantitative imaging, yet reconstructing high-resolution 3D data remains computationally demanding. Non-Cartesian reconstructions require repeated non-uniform FFTs, and the commonly used Locally Low Rank (LLR) prior adds computational overhead and becomes insufficient at high accelerations. Learned 3D priors could address these limitations, but training them at scale is challenging due to memory and runtime demands. We propose SPUR-iG, a fully 3D deep unrolled subspace reconstruction framework that integrates efficient data consistency with a progressive training strategy. Data consistency leverages implicit GROG, which grids non-Cartesian data onto a Cartesian grid with an implicitly learned kernel, enabling FFT-based updates with minimal artifacts. Training proceeds in three stages: (1) pretraining a denoiser with extensive data augmentation, (2) greedy per-iteration unrolled training, and (3) final fine-tuning with gradient checkpointing. Together, these stages make large-scale 3D unrolled learning feasible within a reasonable compute budget. On a large in vivo dataset with retrospective undersampling, SPUR-iG improves subspace coefficient maps quality and quantitative accuracy at 1-mm isotropic resolution compared with LLR and a hybrid 2D/3D unrolled baseline. Whole-brain reconstructions complete in under 15-seconds, with up to $\\times$111 speedup for 2-minute acquisitions. Notably, $T_1$ maps with our method from 30-second scans achieve accuracy on par with or exceeding LLR reconstructions from 2-minute scans. Overall, the framework improves both accuracy and speed in large-scale 3D MRF reconstruction, enabling efficient and reliable accelerated quantitative imaging.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Magnetic Resonance Fingerprinting (MRF) enables fast quantitative imaging, yet reconstructing high-resolution 3D data remains computationally demanding. Non-Cartesian reconstructions require repeated non-uniform FFTs, and the commonly used Locally Low Rank (LLR) prior adds computational overhead and becomes insufficient at high accelerations. Learned 3D priors could address these limitations, but training them at scale is challenging due to memory and runtime demands. We propose SPUR-iG, a fully 3D deep unrolled subspace reconstruction framework that integrates efficient data consistency with a progressive training strategy. Data consistency leverages implicit GROG, which grids non-Cartesian data onto a Cartesian grid with an implicitly learned kernel, enabling FFT-based updates with minimal artifacts. Training proceeds in three stages: (1) pretraining a denoiser with extensive data augmentation, (2) greedy per-iteration unrolled training, and (3) final fine-tuning with gradient checkpointing. Together, these stages make large-scale 3D unrolled learning feasible within a reasonable compute budget. On a large in vivo dataset with retrospective undersampling, SPUR-iG improves subspace coefficient maps quality and quantitative accuracy at 1-mm isotropic resolution compared with LLR and a hybrid 2D/3D unrolled baseline. Whole-brain reconstructions complete in under 15-seconds, with up to $\\times$111 speedup for 2-minute acquisitions. Notably, $T_1$ maps with our method from 30-second scans achieve accuracy on par with or exceeding LLR reconstructions from 2-minute scans. Overall, the framework improves both accuracy and speed in large-scale 3D MRF reconstruction, enabling efficient and reliable accelerated quantitative imaging."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.med-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T19:47:47Z",
        "published_parsed": [
            2026,
            1,
            23,
            19,
            47,
            47,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Yonatan Urman"
            },
            {
                "name": "Mark Nishimura"
            },
            {
                "name": "Daniel Abraham"
            },
            {
                "name": "Xiaozhi Cao"
            },
            {
                "name": "Kawin Setsompop"
            }
        ],
        "author_detail": {
            "name": "Kawin Setsompop"
        },
        "author": "Kawin Setsompop",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "通过分阶段预训练和隐式网格进行完全 3D 展开的磁共振指纹重建",
        "abstract_cn": "磁共振指纹识别 (MRF) 可实现快速定量成像，但重建高分辨率 3D 数据仍然需要大量计算。非笛卡尔重建需要重复的非均匀 FFT，而常用的局部低秩 (LLR) 先验会增加计算开销，并且在高加速度下变得不足。学习的 3D 先验可以解决这些限制，但由于内存和运行时需求，大规模训练它们具有挑战性。我们提出了 SPUR-iG，一种完全 3D 深度展开子空间重建框架，它将高效的数据一致性与渐进式训练策略相结合。数据一致性利用隐式 GROG，它将非笛卡尔数据网格化到具有隐式学习内核的笛卡尔网格上，从而以最少的工件实现基于 FFT 的更新。训练分三个阶段进行：(1) 通过大量数据增强来预训练降噪器，(2) 贪婪的每次迭代展开训练，以及 (3) 使用梯度检查点进行最终微调。这些阶段共同使得大规模 3D 展开学习在合理的计算预算内变得可行。在具有回顾性欠采样的大型体内数据集上，与 LLR 和混合 2D/3D 展开基线相比，SPUR-iG 在 1 毫米各向同性分辨率下提高了子空间系数图质量和定量精度。全脑重建可在 15 秒内完成，2 分钟采集速度可提升高达 $\\times $111。值得注意的是，使用我们的方法在 30 秒扫描中进行的 $T_1$ 映射达到了与 2 分钟扫描中的 LLR 重建相当或超过的精度。总体而言，该框架提高了大规模 3D MRF 重建的准确性和速度，从而实现高效可靠的加速定量成像。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17216v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17216v1",
        "title": "Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction"
        },
        "updated": "2026-01-23T23:02:04Z",
        "updated_parsed": [
            2026,
            1,
            23,
            23,
            2,
            4,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17216v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17216v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T23:02:04Z",
        "published_parsed": [
            2026,
            1,
            23,
            23,
            2,
            4,
            4,
            23,
            0
        ],
        "arxiv_comment": "6 pages 5 figures, accepted to IEEE ICC 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Murat Arda Onsu"
            },
            {
                "name": "Poonam Lohan"
            },
            {
                "name": "Burak Kantarci"
            },
            {
                "name": "Aisha Syed"
            },
            {
                "name": "Matthew Andrews"
            },
            {
                "name": "Sean Kennedy"
            }
        ],
        "author_detail": {
            "name": "Sean Kennedy"
        },
        "author": "Sean Kennedy",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "用于协作碰撞预测的时空语义 V2X 框架",
        "abstract_cn": "智能交通系统 (ITS) 需要实时碰撞预测，以确保道路安全并降低事故严重程度。传统方法依赖于将原始视频或高维传感数据从路边单元 (RSU) 传输到车辆，这在车辆通信带宽和延迟限制下是不切实际的。在这项工作中，我们提出了一种语义 V2X 框架，其中 RSU 安装的摄像机使用视频联合嵌入预测架构 (V-JEPA) 生成未来帧的时空语义嵌入。为了评估该系统，我们构建了城市交通环境的数字孪生，能够生成包含安全和碰撞事件的不同交通场景。这些未来框架的嵌入从 V-JEPA 中提取，捕获与任务相关的交通动态，并通过 V2X 链路传输到车辆，其中轻量级注意力探测器和分类器对其进行解码以预测即将发生的碰撞。通过仅传输语义嵌入而不是原始帧，所提出的系统显着减少了通信开销，同时保持了预测准确性。实验结果表明，采用适当处理方法的框架可将碰撞预测的 F1 分数提高 10%，同时与原始视频相比，传输要求降低了四个数量级。这验证了语义 V2X 通信在 ITS 中实现协作、实时碰撞预测的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17262v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17262v1",
        "title": "Unsupervised clustering algorithm for efficient processing of 4D-STEM and 5D-STEM data",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Unsupervised clustering algorithm for efficient processing of 4D-STEM and 5D-STEM data"
        },
        "updated": "2026-01-24T02:28:40Z",
        "updated_parsed": [
            2026,
            1,
            24,
            2,
            28,
            40,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17262v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17262v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Four-dimensional scanning transmission electron microscopy (4D-STEM) enables mapping of diffraction information with nanometer-scale spatial resolution, offering detailed insight into local structure, orientation, and strain. However, as data dimensionality and sampling density increase, particularly for in situ scanning diffraction experiments (5D-STEM), robust segmentation of spatially coherent regions becomes essential for efficient and physically meaningful analysis. Here, we introduce a clustering framework that identifies crystallographically distinct domains from 4D-STEM datasets. By using local diffraction-pattern similarity as a metric, the method extracts closed contours delineating regions of coherent structural behavior. This approach produces cluster-averaged diffraction patterns that improve signal-to-noise and reduce data volume by orders of magnitude, enabling rapid and accurate orientation, phase, and strain mapping. We demonstrate the applicability of this approach to in situ liquid-cell 4D-STEM data of gold nanoparticle growth. Our method provides a scalable and generalizable route for spatially coherent segmentation, data compression, and quantitative structure-strain mapping across diverse 4D-STEM modalities. The full analysis code and example workflows are publicly available to support reproducibility and reuse.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Four-dimensional scanning transmission electron microscopy (4D-STEM) enables mapping of diffraction information with nanometer-scale spatial resolution, offering detailed insight into local structure, orientation, and strain. However, as data dimensionality and sampling density increase, particularly for in situ scanning diffraction experiments (5D-STEM), robust segmentation of spatially coherent regions becomes essential for efficient and physically meaningful analysis. Here, we introduce a clustering framework that identifies crystallographically distinct domains from 4D-STEM datasets. By using local diffraction-pattern similarity as a metric, the method extracts closed contours delineating regions of coherent structural behavior. This approach produces cluster-averaged diffraction patterns that improve signal-to-noise and reduce data volume by orders of magnitude, enabling rapid and accurate orientation, phase, and strain mapping. We demonstrate the applicability of this approach to in situ liquid-cell 4D-STEM data of gold nanoparticle growth. Our method provides a scalable and generalizable route for spatially coherent segmentation, data compression, and quantitative structure-strain mapping across diverse 4D-STEM modalities. The full analysis code and example workflows are publicly available to support reproducibility and reuse."
        },
        "tags": [
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T02:28:40Z",
        "published_parsed": [
            2026,
            1,
            24,
            2,
            28,
            40,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cond-mat.mtrl-sci"
        },
        "authors": [
            {
                "name": "Serin Lee"
            },
            {
                "name": "Stephanie M. Ribet"
            },
            {
                "name": "Arthur R. C. McCray"
            },
            {
                "name": "Andrew Barnum"
            },
            {
                "name": "Jennifer A. Dionne"
            },
            {
                "name": "Colin Ophus"
            }
        ],
        "author_detail": {
            "name": "Colin Ophus"
        },
        "author": "Colin Ophus",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "用于高效处理 4D-STEM 和 5D-STEM 数据的无监督聚类算法",
        "abstract_cn": "四维扫描透射电子显微镜 (4D-STEM) 能够以纳米级空间分辨率绘制衍射信息，提供对局部结构、方向和应变的详细了解。然而，随着数据维度和采样密度的增加，特别是对于原位扫描衍射实验 (5D-STEM)，空间相干区域的稳健分割对于高效且具有物理意义的分析变得至关重要。在这里，我们介绍了一个聚类框架，该框架可以从 4D-STEM 数据集中识别晶体学上不同的域。通过使用局部衍射图案相似性作为度量，该方法提取描绘相干结构行为区域的闭合轮廓。这种方法产生簇平均衍射图案，可提高信噪比并减少几个数量级的数据量，从而实现快速准确的方向、相位和应变映射。我们证明了这种方法对金纳米粒子生长的原位液池 4D-STEM 数据的适用性。我们的方法为跨不同 4D-STEM 模式的空间相干分割、数据压缩和定量结构应变映射提供了可扩展且可推广的途径。完整的分析代码和示例工作流程是公开的，以支持可重复性和重用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17279v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17279v1",
        "title": "SPADE: A SIMD Posit-enabled compute engine for Accelerating DNN Efficiency",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SPADE: A SIMD Posit-enabled compute engine for Accelerating DNN Efficiency"
        },
        "updated": "2026-01-24T03:38:11Z",
        "updated_parsed": [
            2026,
            1,
            24,
            3,
            38,
            11,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17279v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17279v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The growing demand for edge-AI systems requires arithmetic units that balance numerical precision, energy efficiency, and compact hardware while supporting diverse formats. Posit arithmetic offers advantages over floating- and fixed-point representations through its tapered precision, wide dynamic range, and improved numerical robustness. This work presents SPADE, a unified multi-precision SIMD Posit-based multiplyaccumulate (MAC) architecture supporting Posit (8,0), Posit (16,1), and Posit (32,2) within a single framework. Unlike prior single-precision or floating/fixed-point SIMD MACs, SPADE introduces a regime-aware, lane-fused SIMD Posit datapath that hierarchically reuses Posit-specific submodules (LOD, complementor, shifter, and multiplier) across 8/16/32-bit precisions without datapath replication. FPGA implementation on a Xilinx Virtex-7 shows 45.13% LUT and 80% slice reduction for Posit (8,0), and up to 28.44% and 17.47% improvement for Posit (16,1) and Posit (32,2) over prior work, with only 6.9% LUT and 14.9% register overhead for multi-precision support. ASIC results across TSMC nodes achieve 1.38 GHz at 6.1 mW (28 nm). Evaluation on MNIST, CIFAR-10/100, and alphabet datasets confirms competitive inference accuracy.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The growing demand for edge-AI systems requires arithmetic units that balance numerical precision, energy efficiency, and compact hardware while supporting diverse formats. Posit arithmetic offers advantages over floating- and fixed-point representations through its tapered precision, wide dynamic range, and improved numerical robustness. This work presents SPADE, a unified multi-precision SIMD Posit-based multiplyaccumulate (MAC) architecture supporting Posit (8,0), Posit (16,1), and Posit (32,2) within a single framework. Unlike prior single-precision or floating/fixed-point SIMD MACs, SPADE introduces a regime-aware, lane-fused SIMD Posit datapath that hierarchically reuses Posit-specific submodules (LOD, complementor, shifter, and multiplier) across 8/16/32-bit precisions without datapath replication. FPGA implementation on a Xilinx Virtex-7 shows 45.13% LUT and 80% slice reduction for Posit (8,0), and up to 28.44% and 17.47% improvement for Posit (16,1) and Posit (32,2) over prior work, with only 6.9% LUT and 14.9% register overhead for multi-precision support. ASIC results across TSMC nodes achieve 1.38 GHz at 6.1 mW (28 nm). Evaluation on MNIST, CIFAR-10/100, and alphabet datasets confirms competitive inference accuracy."
        },
        "tags": [
            {
                "term": "cs.AR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T03:38:11Z",
        "published_parsed": [
            2026,
            1,
            24,
            3,
            38,
            11,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.AR"
        },
        "authors": [
            {
                "name": "Sonu Kumar"
            },
            {
                "name": "Lavanya Vinnakota"
            },
            {
                "name": "Mukul Lokhande"
            },
            {
                "name": "Santosh Kumar Vishvakarma"
            },
            {
                "name": "Adam Teman"
            }
        ],
        "author_detail": {
            "name": "Adam Teman"
        },
        "author": "Adam Teman",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "SPADE：支持 SIMD Posit 的计算引擎，用于加速 DNN 效率",
        "abstract_cn": "对边缘人工智能系统日益增长的需求需要算术单元能够平衡数值精度、能源效率和紧凑的硬件，同时支持多种格式。与浮点和定点表示相比，Posit 算术通过其锥形精度、宽动态范围和改进的数值鲁棒性提供了优势。这项工作提出了 SPADE，一种统一的多精度 SIMD 基于 Posit 的乘法累加 (MAC) 架构，在单个框架内支持 Posit (8,0)、Posit (16,1) 和 Posit (32,2)。与之前的单精度或浮点/定点 SIMD MAC 不同，SPADE 引入了一种机制感知、通道融合的 SIMD Posit 数据路径，该数据路径可在 8/16/32 位精度上分层重用特定于 Posit 的子模块（LOD、补码器、移位器和乘法器），而无需数据路径复制。 Xilinx Virtex-7 上的 FPGA 实现显示，与之前的工作相比，Posit (8,0) 的 LUT 减少了 45.13%，切片减少了 80%，Posit (16,1) 和 Posit (32,2) 的切片提高了 28.44% 和 17.47%，而多精度支持的 LUT 和寄存器开销仅为 6.9% 和 14.9%。 TSMC 节点上的 ASIC 结果在 6.1 mW (28 nm) 下达到 1.38 GHz。对 MNIST、CIFAR-10/100 和字母数据集的评估证实了具有竞争力的推理准确性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17460v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17460v1",
        "title": "Entropy-Guided Agreement-Diversity: A Semi-Supervised Active Learning Framework for Fetal Head Segmentation in Ultrasound",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Entropy-Guided Agreement-Diversity: A Semi-Supervised Active Learning Framework for Fetal Head Segmentation in Ultrasound"
        },
        "updated": "2026-01-24T13:23:18Z",
        "updated_parsed": [
            2026,
            1,
            24,
            13,
            23,
            18,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17460v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17460v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Fetal ultrasound (US) data is often limited due to privacy and regulatory restrictions, posing challenges for training deep learning (DL) models. While semi-supervised learning (SSL) is commonly used for fetal US image analysis, existing SSL methods typically rely on random limited selection, which can lead to suboptimal model performance by overfitting to homogeneous labeled data. To address this, we propose a two-stage Active Learning (AL) sampler, Entropy-Guided Agreement-Diversity (EGAD), for fetal head segmentation. Our method first selects the most uncertain samples using predictive entropy, and then refines the final selection using the agreement-diversity score combining cosine similarity and mutual information. Additionally, our SSL framework employs a consistency learning strategy with feature downsampling to further enhance segmentation performance. In experiments, SSL-EGAD achieves an average Dice score of 94.57\\% and 96.32\\% on two public datasets for fetal head segmentation, using 5\\% and 10\\% labeled data for training, respectively. Our method outperforms current SSL models and showcases consistent robustness across diverse pregnancy stage data. The code is available on \\href{https://github.com/13204942/Semi-supervised-EGAD}{GitHub}.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fetal ultrasound (US) data is often limited due to privacy and regulatory restrictions, posing challenges for training deep learning (DL) models. While semi-supervised learning (SSL) is commonly used for fetal US image analysis, existing SSL methods typically rely on random limited selection, which can lead to suboptimal model performance by overfitting to homogeneous labeled data. To address this, we propose a two-stage Active Learning (AL) sampler, Entropy-Guided Agreement-Diversity (EGAD), for fetal head segmentation. Our method first selects the most uncertain samples using predictive entropy, and then refines the final selection using the agreement-diversity score combining cosine similarity and mutual information. Additionally, our SSL framework employs a consistency learning strategy with feature downsampling to further enhance segmentation performance. In experiments, SSL-EGAD achieves an average Dice score of 94.57\\% and 96.32\\% on two public datasets for fetal head segmentation, using 5\\% and 10\\% labeled data for training, respectively. Our method outperforms current SSL models and showcases consistent robustness across diverse pregnancy stage data. The code is available on \\href{https://github.com/13204942/Semi-supervised-EGAD}{GitHub}."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T13:23:18Z",
        "published_parsed": [
            2026,
            1,
            24,
            13,
            23,
            18,
            5,
            24,
            0
        ],
        "arxiv_comment": "Accepted at ISBI 2026",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Fangyijie Wang"
            },
            {
                "name": "Siteng Ma"
            },
            {
                "name": "Guénolé Silvestre"
            },
            {
                "name": "Kathleen M. Curran"
            }
        ],
        "author_detail": {
            "name": "Kathleen M. Curran"
        },
        "author": "Kathleen M. Curran",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "熵引导协议多样性：超声胎头分割的半监督主动学习框架",
        "abstract_cn": "由于隐私和监管限制，胎儿超声（US）数据通常受到限制，这给训练深度学习（DL）模型带来了挑战。虽然半监督学习 (SSL) 通常用于胎儿 US 图像分析，但现有的 SSL 方法通常依赖于随机有限选择，这可能会因过度拟合同质标记数据而导致模型性能不佳。为了解决这个问题，我们提出了一种两阶段主动学习（AL）采样器，熵引导协议多样性（EGAD），用于胎儿头部分割。我们的方法首先使用预测熵选择最不确定的样本，然后使用结合余弦相似度和互信息的一致性多样性得分来细化最终选择。此外，我们的 SSL 框架采用了具有特征下采样的一致性学习策略，以进一步提高分割性能。在实验中，SSL-EGAD 在两个用于胎儿头部分割的公共数据集上分别使用 5\\% 和 10\\% 标记数据进行训练，平均 Dice 得分为 94.57\\% 和 96.32\\%。我们的方法优于当前的 SSL 模型，并在不同妊娠阶段数据中表现出一致的稳健性。该代码可在 \\href{https://github.com/13204942/Semi-supervised-EGAD}{GitHub} 上找到。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17545v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17545v1",
        "title": "In-situ On-demand Digital Image Correlation: A New Data-rich Characterization Paradigm for Deformation and Damage Development in Solids",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In-situ On-demand Digital Image Correlation: A New Data-rich Characterization Paradigm for Deformation and Damage Development in Solids"
        },
        "updated": "2026-01-24T18:23:15Z",
        "updated_parsed": [
            2026,
            1,
            24,
            18,
            23,
            15,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17545v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17545v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Digital image correlation (DIC) has become one of the most popular methods for deformation characterization in experimental mechanics. DIC is based on optical images taken during experimentation and post-test image processing. Its advantages include the capability to capture full-field deformation in a non-contact manner, the robustness in characterizing excessive deformation induced by events such as yielding and cracking, and the versatility to integrate optical cameras with a variety of open-source and commercial codes. In this paper, we developed a new paradigm of DIC analysis by integrating camera control into the DIC process flow. The essential idea is to dynamically increase the camera imaging frame rate with excessive deformation or deformation rate, while maintaining a relatively low imaging frame rate with small and slow deformation. We refer to this new DIC paradigm as in-situ on-demand (ISOD) DIC. ISOD DIC enables real-time deformation analysis, visualization, and closed-loop camera control. ISOD DIC has captured approximately 178% more images than conventional DIC for samples undergoing crack growth due to its dynamically adjusted frame rate, with the potential to significantly enhance data richness for damage inspection without consuming excessive storage space and analysis time, thereby benefiting the characterization of intrinsic constitutive behaviors and damage mechanisms",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Digital image correlation (DIC) has become one of the most popular methods for deformation characterization in experimental mechanics. DIC is based on optical images taken during experimentation and post-test image processing. Its advantages include the capability to capture full-field deformation in a non-contact manner, the robustness in characterizing excessive deformation induced by events such as yielding and cracking, and the versatility to integrate optical cameras with a variety of open-source and commercial codes. In this paper, we developed a new paradigm of DIC analysis by integrating camera control into the DIC process flow. The essential idea is to dynamically increase the camera imaging frame rate with excessive deformation or deformation rate, while maintaining a relatively low imaging frame rate with small and slow deformation. We refer to this new DIC paradigm as in-situ on-demand (ISOD) DIC. ISOD DIC enables real-time deformation analysis, visualization, and closed-loop camera control. ISOD DIC has captured approximately 178% more images than conventional DIC for samples undergoing crack growth due to its dynamically adjusted frame rate, with the potential to significantly enhance data richness for damage inspection without consuming excessive storage space and analysis time, thereby benefiting the characterization of intrinsic constitutive behaviors and damage mechanisms"
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T18:23:15Z",
        "published_parsed": [
            2026,
            1,
            24,
            18,
            23,
            15,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Ravi Venkata Surya Sai Mogilisetti"
            },
            {
                "name": "Partha Pratim Das"
            },
            {
                "name": "Rassel Raihan"
            },
            {
                "name": "Shiyao Lin"
            }
        ],
        "author_detail": {
            "name": "Shiyao Lin"
        },
        "author": "Shiyao Lin",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "原位按需数字图像相关：一种新的数据丰富的固体变形和损伤发展表征范式",
        "abstract_cn": "数字图像相关（DIC）已成为实验力学中最流行的变形表征方法之一。 DIC 基于实验期间拍摄的光学图像和测试后图像处理。其优点包括能够以非接触方式捕获全场变形、表征由屈服和裂纹等事件引起的过度变形的鲁棒性，以及将光学相机与各种开源和商业代码集成的多功能性。在本文中，我们通过将相机控制集成到 DIC 流程中，开发了一种新的 DIC 分析范例。其本质思想是在变形过大或变形速率较大的情况下动态提高相机成像帧率，同时在变形较小且缓慢的情况下保持相对较低的成像帧率。我们将这种新的 DIC 范例称为原位按需 (ISOD) DIC。 ISOD DIC 可实现实时变形分析、可视化和闭环相机控制。由于其动态调整的帧速率，ISOD DIC 比传统 DIC 捕获的裂纹扩展样品图像多约 178%，有可能在不消耗过多存储空间和分析时间的情况下显着增强损伤检查的数据丰富度，从而有利于表征内在本构行为和损伤机制"
    },
    {
        "id": "http://arxiv.org/abs/2601.17568v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17568v1",
        "title": "Fast Multirate Encoding for 360° Video in OMAF Streaming Workflows",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fast Multirate Encoding for 360° Video in OMAF Streaming Workflows"
        },
        "updated": "2026-01-24T19:43:16Z",
        "updated_parsed": [
            2026,
            1,
            24,
            19,
            43,
            16,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17568v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17568v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1145/3789239.3793270",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Preparing high-quality 360-degree video for HTTP Adaptive Streaming requires encoding each sequence into multiple representations spanning different resolutions and quantization parameters (QPs). For ultra-high-resolution immersive content such as 8K 360-degree video, this process is computationally intensive due to the large number of representations and the high complexity of modern codecs. This paper investigates fast multirate encoding strategies that reduce encoding time by reusing encoder analysis information across QPs and resolutions. We evaluate two cross-resolution information-reuse pipelines that differ in how reference encodes propagate across resolutions: (i) a strict HD -> 4K -> 8K cascade with scaled analysis reuse, and (ii) a resolution-anchored scheme that initializes each resolution with its own highest-bitrate reference before guiding dependent encodes. In addition to evaluating these pipelines on standard equirectangular projection content, we also apply the same two pipelines to cubemap-projection (CMP) tiling, where each 360-degree frame is partitioned into independently encoded tiles. CMP introduces substantial parallelism, while still benefiting from the proposed multirate analysis-reuse strategies. Experimental results using the SJTU 8K 360-degree dataset show that hierarchical analysis reuse significantly accelerates HEVC encoding with minimal rate-distortion impact across both equirectangular and CMP-tiled content, yielding encoding-time reductions of roughly 33%-59% for ERP and about 51% on average for CMP, with Bjontegaard Delta Encoding Time (BDET) gains approaching -50% and wall-clock speedups of up to 4.2x.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Preparing high-quality 360-degree video for HTTP Adaptive Streaming requires encoding each sequence into multiple representations spanning different resolutions and quantization parameters (QPs). For ultra-high-resolution immersive content such as 8K 360-degree video, this process is computationally intensive due to the large number of representations and the high complexity of modern codecs. This paper investigates fast multirate encoding strategies that reduce encoding time by reusing encoder analysis information across QPs and resolutions. We evaluate two cross-resolution information-reuse pipelines that differ in how reference encodes propagate across resolutions: (i) a strict HD -> 4K -> 8K cascade with scaled analysis reuse, and (ii) a resolution-anchored scheme that initializes each resolution with its own highest-bitrate reference before guiding dependent encodes. In addition to evaluating these pipelines on standard equirectangular projection content, we also apply the same two pipelines to cubemap-projection (CMP) tiling, where each 360-degree frame is partitioned into independently encoded tiles. CMP introduces substantial parallelism, while still benefiting from the proposed multirate analysis-reuse strategies. Experimental results using the SJTU 8K 360-degree dataset show that hierarchical analysis reuse significantly accelerates HEVC encoding with minimal rate-distortion impact across both equirectangular and CMP-tiled content, yielding encoding-time reductions of roughly 33%-59% for ERP and about 51% on average for CMP, with Bjontegaard Delta Encoding Time (BDET) gains approaching -50% and wall-clock speedups of up to 4.2x."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.MM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T19:43:16Z",
        "published_parsed": [
            2026,
            1,
            24,
            19,
            43,
            16,
            5,
            24,
            0
        ],
        "arxiv_comment": "Mile High Video (MHV), 2026",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Amritha Premkumar"
            },
            {
                "name": "Christian Herglotz"
            }
        ],
        "author_detail": {
            "name": "Christian Herglotz"
        },
        "author": "Christian Herglotz",
        "arxiv_doi": "10.1145/3789239.3793270",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "OMAF 流媒体工作流程中 360° 视频的快速多速率编码",
        "abstract_cn": "为 HTTP 自适应流媒体准备高质量 360 度视频需要将每个序列编码为跨越不同分辨率和量化参数 (QP) 的多种表示形式。对于 8K 360 度视频等超高分辨率沉浸式内容，由于现代编解码器的大量表示和高复杂性，该过程需要大量计算。本文研究了快速多速率编码策略，通过跨 QP 和分辨率重用编码器分析信息来减少编码时间。我们评估了两个跨分辨率信息重用管道，它们在参考编码如何跨分辨率传播方面有所不同：（i）严格的 HD -> 4K -> 8K 级联，具有缩放分析重用，以及（ii）分辨率锚定方案，在引导相关编码之前用其自己的最高比特率参考初始化每个分辨率。除了在标准等距柱状投影内容上评估这些管道之外，我们还将相同的两个管道应用于立方体贴图投影 (CMP) 平铺，其中每个 360 度帧被划分为独立编码的平铺。 CMP 引入了大量的并行性，同时仍然受益于所提出的多速率分析重用策略。使用上海交通大学 8K 360 度数据集的实验结果表明，分层分析重用可显着加速 HEVC 编码，同时对等距矩形和 CMP 平铺内容的速率失真影响最小，使 ERP 的编码时间减少约 33%-59%，CMP 平均减少约 51%，Bjontegaard Delta 编码时间 (BDET) 增益接近 -50%，挂钟加速高达4.2 倍。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17586v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17586v1",
        "title": "Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization"
        },
        "updated": "2026-01-24T20:53:02Z",
        "updated_parsed": [
            2026,
            1,
            24,
            20,
            53,
            2,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17586v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17586v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit ."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T20:53:02Z",
        "published_parsed": [
            2026,
            1,
            24,
            20,
            53,
            2,
            5,
            24,
            0
        ],
        "arxiv_comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2026)",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sebastian Doerrich"
            },
            {
                "name": "Francesco Di Salvo"
            },
            {
                "name": "Jonas Alle"
            },
            {
                "name": "Christian Ledig"
            }
        ],
        "author_detail": {
            "name": "Christian Ledig"
        },
        "author": "Christian Ledig",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "风格化 ViT：用于域泛化的解剖保留实例样式传输",
        "abstract_cn": "由于数据异质性和稀缺性，医学图像分析中的深度学习模型通常难以跨领域和人口群体的通用性。传统的增强提高了鲁棒性，但在重大领域转移的情况下会失败。风格增强方面的最新进展通过改变图像风格来增强领域泛化，但在风格多样性或通过在生成的图像中引入伪影方面存在不足。为了解决这些限制，我们提出了 Stylizing ViT，这是一种新颖的 Vision Transformer 编码器，它利用权重共享的注意力块来进行自注意力和交叉注意力。这种设计允许相同的注意力块通过自注意力保持解剖学的一致性，同时通过交叉注意力执行风格转移。我们通过将其用于组织病理学和皮肤病学背景下三个不同图像分类任务的数据增强来评估我们的方法在领域泛化方面的有效性。结果表明，与现有技术相比，鲁棒性有所提高（准确度高达 +13%），同时生成没有伪影的感知上令人信服的图像。此外，我们还表明，风格化 ViT 在训练之外也很有效，当用于测试时间增强时，推理期间的性能提高了 17%。源代码可在 https://github.com/sdoerrich97/stylizing-vit 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17611v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17611v1",
        "title": "ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video"
        },
        "updated": "2026-01-24T22:26:39Z",
        "updated_parsed": [
            2026,
            1,
            24,
            22,
            26,
            39,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17611v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17611v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula."
        },
        "tags": [
            {
                "term": "eess.AS",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.SD",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T22:26:39Z",
        "published_parsed": [
            2026,
            1,
            24,
            22,
            26,
            39,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.AS"
        },
        "authors": [
            {
                "name": "Davide Berghi"
            },
            {
                "name": "Philip J. B. Jackson"
            }
        ],
        "author_detail": {
            "name": "Philip J. B. Jackson"
        },
        "author": "Philip J. B. Jackson",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "ToS：专家团队集成框架，用于立体声事件定位和检测以及视频中的距离估计",
        "abstract_cn": "视频中的声音事件定位和距离估计检测 (3D SELD) 涉及识别每个时间帧的活动声音事件，同时估计其空间坐标。这种多模态任务需要跨语义、空间和时间维度的联合推理，这是单一模型通常难以有效解决的挑战。为了解决这个问题，我们引入了专家团队（ToS）集成框架，它集成了三个互补的子网络：时空语言模型、时空模型和时间语言模型。每个子网络专门研究一对独特的维度，为最终预测提供独特的见解，类似于具有不同专业知识的协作团队。 ToS 已针对 DCASE2025 Task 3 Stereo SELD 开发集上最先进的 3D SELD 视听模型进行了基准测试，在关键指标上始终优于现有方法。未来的工作将通过加强专家的适当任务、培训和预培训课程来扩展这一概念验证。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17752v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17752v1",
        "title": "A Capsule-Sized Multi-Wavelength Wireless Optical System for Edge-AI-Based Classification of Gastrointestinal Bleeding Flow Rate",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Capsule-Sized Multi-Wavelength Wireless Optical System for Edge-AI-Based Classification of Gastrointestinal Bleeding Flow Rate"
        },
        "updated": "2026-01-25T08:50:24Z",
        "updated_parsed": [
            2026,
            1,
            25,
            8,
            50,
            24,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17752v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17752v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Post-endoscopic gastrointestinal (GI) rebleeding frequently occurs within the first 72 hours after therapeutic hemostasis and remains a major cause of early morbidity and mortality. Existing non-invasive monitoring approaches primarily provide binary blood detection and lack quantitative assessment of bleeding severity or flow dynamic, limiting their ability to support timely clinical decision-making during this high-risk period. In this work, we developed a capsule-sized, multi-wavelength optical sensing wireless platform for order-of-magnitude-level classification of GI bleeding flow rate, leveraging transmission spectroscopy and low-power edge artificial intelligence. The system performs time-resolved, multi-spectral measurements and employs a lightweight two-dimensional convolutional neural network for on-device flow-rate classification, with physics-based validation confirming consistency with wavelength-dependent hemoglobin absorption behavior. In controlled in vitro experiments under simulated gastric conditions, the proposed approach achieved an overall classification accuracy of 98.75% across multiple bleeding flow-rate levels while robustly distinguishing diverse non-blood gastrointestinal interference. By performing embedded inference directly on the capsule electronics, the system reduced overall energy consumption by approximately 88% compared with continuous wireless transmission of raw data, making prolonged, battery-powered operation feasible. Extending capsule-based diagnostics beyond binary blood detection toward continuous, site-specific assessment of bleeding severity, this platform has the potential to support earlier identification of clinically significant rebleeding and inform timely re-intervention during post-endoscopic surveillance.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Post-endoscopic gastrointestinal (GI) rebleeding frequently occurs within the first 72 hours after therapeutic hemostasis and remains a major cause of early morbidity and mortality. Existing non-invasive monitoring approaches primarily provide binary blood detection and lack quantitative assessment of bleeding severity or flow dynamic, limiting their ability to support timely clinical decision-making during this high-risk period. In this work, we developed a capsule-sized, multi-wavelength optical sensing wireless platform for order-of-magnitude-level classification of GI bleeding flow rate, leveraging transmission spectroscopy and low-power edge artificial intelligence. The system performs time-resolved, multi-spectral measurements and employs a lightweight two-dimensional convolutional neural network for on-device flow-rate classification, with physics-based validation confirming consistency with wavelength-dependent hemoglobin absorption behavior. In controlled in vitro experiments under simulated gastric conditions, the proposed approach achieved an overall classification accuracy of 98.75% across multiple bleeding flow-rate levels while robustly distinguishing diverse non-blood gastrointestinal interference. By performing embedded inference directly on the capsule electronics, the system reduced overall energy consumption by approximately 88% compared with continuous wireless transmission of raw data, making prolonged, battery-powered operation feasible. Extending capsule-based diagnostics beyond binary blood detection toward continuous, site-specific assessment of bleeding severity, this platform has the potential to support earlier identification of clinically significant rebleeding and inform timely re-intervention during post-endoscopic surveillance."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.SY",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T08:50:24Z",
        "published_parsed": [
            2026,
            1,
            25,
            8,
            50,
            24,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Yunhao Bian"
            },
            {
                "name": "Dawei Wang"
            },
            {
                "name": "Mingyang Shen"
            },
            {
                "name": "Xinze Li"
            },
            {
                "name": "Jiayi Shi"
            },
            {
                "name": "Ziyao Zhou"
            },
            {
                "name": "Tiancheng Cao"
            },
            {
                "name": "Hen-Wei Huang"
            }
        ],
        "author_detail": {
            "name": "Hen-Wei Huang"
        },
        "author": "Hen-Wei Huang",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "胶囊大小的多波长无线光学系统，用于基于边缘人工智能的胃肠道出血流量分类",
        "abstract_cn": "内镜下胃肠道 (GI) 再出血经常发生在治疗性止血后的前 72 小时内，并且仍然是早期发病和死亡的主要原因。现有的非侵入性监测方法主要提供二元血液检测，缺乏对出血严重程度或血流动态的定量评估，限制了它们在这一高风险时期支持及时临床决策的能力。在这项工作中，我们开发了一种胶囊大小的多波长光学传感无线平台，利用透射光谱和低功耗边缘人工智能，对胃肠道出血流量进行数量级分类。该系统执行时间分辨、多光谱测量，并采用轻量级二维卷积神经网络进行设备上流量分类，并通过基于物理的验证确认与波长相关的血红蛋白吸收行为的一致性。在模拟胃条件下的受控体外实验中，所提出的方法在多个出血流量水平上实现了 98.75% 的总体分类准确度，同时有力地区分了各种非血液胃肠道干扰。通过直接在胶囊电子设备上执行嵌入式推理，与连续无线传输原始数据相比，该系统将总体能耗降低了约 88%，从而使长时间的电池供电操作成为可能。该平台将基于胶囊的诊断扩展到二元血液检测之外，扩展到对出血严重程度进行连续、特定部位的评估，有可能支持早期识别临床上显着的再出血，并在内镜监测后及时进行重新干预。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18034v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18034v1",
        "title": "Dominant Sets Based Band Selection in Hyperspectral Imagery",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Dominant Sets Based Band Selection in Hyperspectral Imagery"
        },
        "updated": "2026-01-25T23:09:49Z",
        "updated_parsed": [
            2026,
            1,
            25,
            23,
            9,
            49,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18034v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18034v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Hyperspectral imagery is composed of huge amount of data which creates significant transmission latencies for communication systems. It is vital to decrease the huge data size before transmitting the Hyperspectral imagery. Besides, large data size leads to processing problems, especially in practical applications. Moreover, due to the lack of sufficient training samples, Hughes phenomena occur with huge amount of data. Feature selection can be used in order to get rid of huge data problems. In this paper, a band selection framework is introduced to reduce the data size and to find out the most proper spectral bands for a specific application. The method is based on finding \"dominant sets\" in hyperspectral data, so that spectral bands are clustered. From each cluster, the band that reflects the cluster behavior the most is selected to form the most valuable band set in the spectra for a specific application. The proposed feature selection method has low computational complexity since it performs on a small size of data when realizing the feature selection. The aim of the study is to find out a general framework that can define required bands for classification without requiring to perform on the whole data set. Results on Pavia and Salinas datasets show that the proposed framework performs better than the state-of-the-art feature selection methods in terms of classification accuracy.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Hyperspectral imagery is composed of huge amount of data which creates significant transmission latencies for communication systems. It is vital to decrease the huge data size before transmitting the Hyperspectral imagery. Besides, large data size leads to processing problems, especially in practical applications. Moreover, due to the lack of sufficient training samples, Hughes phenomena occur with huge amount of data. Feature selection can be used in order to get rid of huge data problems. In this paper, a band selection framework is introduced to reduce the data size and to find out the most proper spectral bands for a specific application. The method is based on finding \"dominant sets\" in hyperspectral data, so that spectral bands are clustered. From each cluster, the band that reflects the cluster behavior the most is selected to form the most valuable band set in the spectra for a specific application. The proposed feature selection method has low computational complexity since it performs on a small size of data when realizing the feature selection. The aim of the study is to find out a general framework that can define required bands for classification without requiring to perform on the whole data set. Results on Pavia and Salinas datasets show that the proposed framework performs better than the state-of-the-art feature selection methods in terms of classification accuracy."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T23:09:49Z",
        "published_parsed": [
            2026,
            1,
            25,
            23,
            9,
            49,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Onur Haliloğlu"
            },
            {
                "name": "Ufuk Sakarya"
            },
            {
                "name": "B. Uğur Töreyin"
            },
            {
                "name": "Orhan Gazi"
            }
        ],
        "author_detail": {
            "name": "Orhan Gazi"
        },
        "author": "Orhan Gazi",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "高光谱图像中基于主集的波段选择",
        "abstract_cn": "高光谱图像由大量数据组成，这给通信系统带来了显着的传输延迟。在传输高光谱图像之前减少庞大的数据量至关重要。此外，大数据量会导致处理问题，尤其是在实际应用中。而且，由于缺乏足够的训练样本，数据量巨大时会出现休斯现象。可以使用特征选择来解决海量数据问题。在本文中，引入了波段选择框架，以减少数据大小并找出适合特定应用的最合适的光谱波段。该方法基于在高光谱数据中寻找“主导集”，从而对光谱带进行聚类。从每个簇中，选择最能反映簇行为的波段，以形成光谱中针对特定应用最有价值的波段集。所提出的特征选择方法具有较低的计算复杂度，因为它在实现特征选择时在小规模的数据上执行。该研究的目的是找到一个通用框架，可以定义分类所需的频带，而无需在整个数据集上执行。 Pavia 和 Salinas 数据集的结果表明，所提出的框架在分类精度方面优于最先进的特征选择方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18670v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18670v1",
        "title": "COMETS: Coordinated Multi-Destination Video Transmission with In-Network Rate Adaptation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "COMETS: Coordinated Multi-Destination Video Transmission with In-Network Rate Adaptation"
        },
        "updated": "2026-01-26T16:47:45Z",
        "updated_parsed": [
            2026,
            1,
            26,
            16,
            47,
            45,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18670v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18670v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Large-scale video streaming events attract millions of simultaneous viewers, stressing existing delivery infrastructures. Client-driven adaptation reacts slowly to shared congestion, while server-based coordination introduces scalability bottlenecks and single points of failure. We present COMETS, a coordinated multi-destination video transmission framework that leverages information-centric networking principles such as request aggregation and in-network state awareness to enable scalable, fair, and adaptive rate control. COMETS introduces a novel range-interest protocol and distributed in-network decision process that aligns video quality across receiver groups while minimizing redundant transmissions. To achieve this, we develop a lightweight distributed optimization framework that guides per-hop quality adaptation without centralized control. Extensive emulation shows that COMETS consistently improves bandwidth utilization, fairness, and user-perceived quality of experience over DASH, MoQ, and ICN baselines, particularly under high concurrency. The results highlight COMETS as a practical, deployable approach for next-generation scalable video delivery.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Large-scale video streaming events attract millions of simultaneous viewers, stressing existing delivery infrastructures. Client-driven adaptation reacts slowly to shared congestion, while server-based coordination introduces scalability bottlenecks and single points of failure. We present COMETS, a coordinated multi-destination video transmission framework that leverages information-centric networking principles such as request aggregation and in-network state awareness to enable scalable, fair, and adaptive rate control. COMETS introduces a novel range-interest protocol and distributed in-network decision process that aligns video quality across receiver groups while minimizing redundant transmissions. To achieve this, we develop a lightweight distributed optimization framework that guides per-hop quality adaptation without centralized control. Extensive emulation shows that COMETS consistently improves bandwidth utilization, fairness, and user-perceived quality of experience over DASH, MoQ, and ICN baselines, particularly under high concurrency. The results highlight COMETS as a practical, deployable approach for next-generation scalable video delivery."
        },
        "tags": [
            {
                "term": "cs.NI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.MM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T16:47:45Z",
        "published_parsed": [
            2026,
            1,
            26,
            16,
            47,
            45,
            0,
            26,
            0
        ],
        "arxiv_comment": "Accepted to appear in IEEE Transactions on Multimedia (2026)",
        "arxiv_primary_category": {
            "term": "cs.NI"
        },
        "arxiv_journal_ref": "IEEE Transactions on Multimedia, 2026",
        "authors": [
            {
                "name": "Yulong Zhang"
            },
            {
                "name": "Ying Cui"
            },
            {
                "name": "Zili Meng"
            },
            {
                "name": "Abhishek Kumar"
            },
            {
                "name": "Dirk Kutscher"
            }
        ],
        "author_detail": {
            "name": "Dirk Kutscher"
        },
        "author": "Dirk Kutscher",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "COMETS：具有网内速率自适应的协调多目的地视频传输",
        "abstract_cn": "大型视频流活动吸引了数百万同时观看者，给现有的传输基础设施带来了压力。客户端驱动的适应对共享拥塞反应缓慢，而基于服务器的协调则引入了可扩展性瓶颈和单点故障。我们提出了 COMETS，这是一种协调的多目的地视频传输框架，它利用以信息为中心的网络原则（例如请求聚合和网络内状态感知）来实现可扩展、公平和自适应的速率控制。 COMETS 引入了一种新颖的范围兴趣协议和分布式网络内决策过程，可以跨接收器组调整视频质量，同时最大限度地减少冗余传输。为了实现这一目标，我们开发了一个轻量级分布式优化框架，可以指导每跳质量适应而无需集中控制。广泛的仿真表明，COMETS 在 DASH、MoQ 和 ICN 基线上持续提高了带宽利用率、公平性和用户感知的体验质量，特别是在高并发情况下。结果凸显了 COMETS 是一种实用、可部署的下一代可扩展视频传输方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18782v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18782v1",
        "title": "Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods"
        },
        "updated": "2026-01-26T18:49:16Z",
        "updated_parsed": [
            2026,
            1,
            26,
            18,
            49,
            16,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18782v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18782v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We study the quantization of real-valued bandlimited signals on graphs, focusing on low-bit representations. We propose iterative noise-shaping algorithms for quantization, including sampling approaches with and without vertex replacement. The methods leverage the spectral properties of the graph Laplacian and exploit graph incoherence to achieve high-fidelity approximations. Theoretical guarantees are provided for the random sampling method, and extensive numerical experiments on synthetic and real-world graphs illustrate the efficiency and robustness of the proposed schemes.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We study the quantization of real-valued bandlimited signals on graphs, focusing on low-bit representations. We propose iterative noise-shaping algorithms for quantization, including sampling approaches with and without vertex replacement. The methods leverage the spectral properties of the graph Laplacian and exploit graph incoherence to achieve high-fidelity approximations. Theoretical guarantees are provided for the random sampling method, and extensive numerical experiments on synthetic and real-world graphs illustrate the efficiency and robustness of the proposed schemes."
        },
        "tags": [
            {
                "term": "eess.SP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "math.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "math.NA",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "math.OC",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T18:49:16Z",
        "published_parsed": [
            2026,
            1,
            26,
            18,
            49,
            16,
            0,
            26,
            0
        ],
        "arxiv_comment": "17 pages, 5 figures",
        "arxiv_primary_category": {
            "term": "eess.SP"
        },
        "authors": [
            {
                "name": "Felix Krahmer"
            },
            {
                "name": "He Lyu"
            },
            {
                "name": "Rayan Saab"
            },
            {
                "name": "Jinna Qian"
            },
            {
                "name": "Anna Veselovska"
            },
            {
                "name": "Rongrong Wang"
            }
        ],
        "author_detail": {
            "name": "Rongrong Wang"
        },
        "author": "Rongrong Wang",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "通过迭代方法对带限图信号进行低位量化",
        "abstract_cn": "我们研究图上实值带限信号的量化，重点关注低位表示。我们提出了用于量化的迭代噪声整形算法，包括带和不带顶点替换的采样方法。该方法利用图拉普拉斯算子的谱特性并利用图不相干性来实现高保真度近似。为随机采样方法提供了理论保证，并且对合成图和真实世界图的广泛数值实验说明了所提出方案的效率和鲁棒性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17124v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17124v1",
        "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code"
        },
        "updated": "2026-01-23T19:00:35Z",
        "updated_parsed": [
            2026,
            1,
            23,
            19,
            0,
            35,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17124v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17124v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T19:00:35Z",
        "published_parsed": [
            2026,
            1,
            23,
            19,
            0,
            35,
            4,
            23,
            0
        ],
        "arxiv_comment": "Technical Report",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Bin Lin"
            },
            {
                "name": "Zongjian Li"
            },
            {
                "name": "Yuwei Niu"
            },
            {
                "name": "Kaixiong Gong"
            },
            {
                "name": "Yunyang Ge"
            },
            {
                "name": "Yunlong Lin"
            },
            {
                "name": "Mingzhe Zheng"
            },
            {
                "name": "JianWei Zhang"
            },
            {
                "name": "Miles Yang"
            },
            {
                "name": "Zhao Zhong"
            },
            {
                "name": "Liefeng Bo"
            },
            {
                "name": "Li Yuan"
            }
        ],
        "author_detail": {
            "name": "Li Yuan"
        },
        "author": "Li Yuan",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "iFSQ：用 1 行代码改进图像生成的 FSQ",
        "abstract_cn": "图像生成领域目前分为在离散标记上运行的自回归（AR）模型和利用连续潜伏的扩散模型。这种分歧源于 VQ-VAE 和 VAE 之间的区别，阻碍了统一建模和公平基准测试。有限标量量化 (FSQ) 提供了一个理论桥梁，但普通 FSQ 存在一个严重缺陷：其等间隔量化可能导致激活崩溃。这种不匹配迫使重建保真度和信息效率之间进行权衡。在这项工作中，我们通过简单地用分布匹配映射替换原始 FSQ 中的激活函数来解决这个困境，以强制执行统一的先验。这种简单的策略称为 iFSQ，仅需要一行代码，但在数学上保证了最佳的 bin 利用率和重建精度。利用 iFSQ 作为受控基准，我们发现了两个关键见解：(1) 离散表示和连续表示之间的最佳平衡在于每个维度大约 4 位。 （2）在相同的重建约束下，AR模型表现出快速的初始收敛，而扩散模型实现了优越的性能上限，这表明严格的顺序排序可能会限制生成质量的上限。最后，我们通过将表示对齐 (REPA) 应用于 AR 模型来扩展我们的分析，产生 LlamaGen-REPA。代码可见https://github.com/Tencent-Hunyuan/iFSQ"
    },
    {
        "id": "http://arxiv.org/abs/2601.17151v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17151v1",
        "title": "Scaling medical imaging report generation with multimodal reinforcement learning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Scaling medical imaging report generation with multimodal reinforcement learning"
        },
        "updated": "2026-01-23T20:14:21Z",
        "updated_parsed": [
            2026,
            1,
            23,
            20,
            14,
            21,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17151v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17151v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T20:14:21Z",
        "published_parsed": [
            2026,
            1,
            23,
            20,
            14,
            21,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Qianchu Liu"
            },
            {
                "name": "Sheng Zhang"
            },
            {
                "name": "Guanghui Qin"
            },
            {
                "name": "Yu Gu"
            },
            {
                "name": "Ying Jin"
            },
            {
                "name": "Sam Preston"
            },
            {
                "name": "Yanbo Xu"
            },
            {
                "name": "Sid Kiblawi"
            },
            {
                "name": "Wen-wai Yim"
            },
            {
                "name": "Tim Ossowski"
            },
            {
                "name": "Tristan Naumann"
            },
            {
                "name": "Mu Wei"
            },
            {
                "name": "Hoifung Poon"
            }
        ],
        "author_detail": {
            "name": "Hoifung Poon"
        },
        "author": "Hoifung Poon",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过多模态强化学习扩展医学成像报告的生成",
        "abstract_cn": "前沿模型在自然语言文本的理解和推理方面表现出了卓越的能力，但它们在多模态理解和推理方面仍然表现出重大的能力差距，特别是在生物医学等高价值垂直领域。医学成像报告生成就是一个突出的例子。有监督的微调可以显着提高性能，但它们很容易过度拟合表面的样板模式。在本文中，我们介绍通用报告生成（UniRG）作为医学成像报告生成的通用框架。通过利用强化学习作为统一机制来直接优化为最终应用设计的评估指标，UniRG 可以显着改进监督微调，并在不同机构和临床实践中实现持久的泛化。我们使用公开的胸部 X 射线 (CXR) 数据对 UniRG-CXR 进行了训练，并通过严格的评估场景对 CXR 报告生成进行了全面评估。在权威的 ReXrank 基准上，UniRG-CXR 设定了新的整体 SOTA，大幅优于先前的技术水平。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17165v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17165v1",
        "title": "Benchmarking Deep Learning-Based Reconstruction Methods for Photoacoustic Computed Tomography with Clinically Relevant Synthetic Datasets",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Benchmarking Deep Learning-Based Reconstruction Methods for Photoacoustic Computed Tomography with Clinically Relevant Synthetic Datasets"
        },
        "updated": "2026-01-23T20:52:35Z",
        "updated_parsed": [
            2026,
            1,
            23,
            20,
            52,
            35,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17165v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17165v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning (DL)-based image reconstruction methods for photoacoustic computed tomography (PACT) have developed rapidly in recent years. However, most existing methods have not employed standardized datasets, and their evaluations rely on traditional image quality (IQ) metrics that may lack clinical relevance. The absence of a standardized framework for clinically meaningful IQ assessment hinders fair comparison and raises concerns about the reproducibility and reliability of reported advancements in PACT. A benchmarking framework is proposed that provides open-source, anatomically plausible synthetic datasets and evaluation strategies for DL-based acoustic inversion methods in PACT. The datasets each include over 11,000 two-dimensional (2D) stochastic breast objects with clinically relevant lesions and paired measurements at varying modeling complexity. The evaluation strategies incorporate both traditional and task-based IQ measures to assess fidelity and clinical utility. A preliminary benchmarking study is conducted to demonstrate the framework's utility by comparing DL-based and physics-based reconstruction methods. The benchmarking study demonstrated that the proposed framework enabled comprehensive, quantitative comparisons of reconstruction performance and revealed important limitations in certain DL-based methods. Although they performed well according to traditional IQ measures, they often failed to accurately recover lesions. This highlights the inadequacy of traditional metrics and motivates the need for task-based assessments. The proposed benchmarking framework enables systematic comparisons of DL-based acoustic inversion methods for 2D PACT. By integrating clinically relevant synthetic datasets with rigorous evaluation protocols, it enables reproducible, objective assessments and facilitates method development and system optimization in PACT.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning (DL)-based image reconstruction methods for photoacoustic computed tomography (PACT) have developed rapidly in recent years. However, most existing methods have not employed standardized datasets, and their evaluations rely on traditional image quality (IQ) metrics that may lack clinical relevance. The absence of a standardized framework for clinically meaningful IQ assessment hinders fair comparison and raises concerns about the reproducibility and reliability of reported advancements in PACT. A benchmarking framework is proposed that provides open-source, anatomically plausible synthetic datasets and evaluation strategies for DL-based acoustic inversion methods in PACT. The datasets each include over 11,000 two-dimensional (2D) stochastic breast objects with clinically relevant lesions and paired measurements at varying modeling complexity. The evaluation strategies incorporate both traditional and task-based IQ measures to assess fidelity and clinical utility. A preliminary benchmarking study is conducted to demonstrate the framework's utility by comparing DL-based and physics-based reconstruction methods. The benchmarking study demonstrated that the proposed framework enabled comprehensive, quantitative comparisons of reconstruction performance and revealed important limitations in certain DL-based methods. Although they performed well according to traditional IQ measures, they often failed to accurately recover lesions. This highlights the inadequacy of traditional metrics and motivates the need for task-based assessments. The proposed benchmarking framework enables systematic comparisons of DL-based acoustic inversion methods for 2D PACT. By integrating clinically relevant synthetic datasets with rigorous evaluation protocols, it enables reproducible, objective assessments and facilitates method development and system optimization in PACT."
        },
        "tags": [
            {
                "term": "physics.med-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T20:52:35Z",
        "published_parsed": [
            2026,
            1,
            23,
            20,
            52,
            35,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.med-ph"
        },
        "authors": [
            {
                "name": "Panpan Chen"
            },
            {
                "name": "Seonyeong Park"
            },
            {
                "name": "Gangwon Jeong"
            },
            {
                "name": "Refik Mert Cam"
            },
            {
                "name": "Umberto Villa"
            },
            {
                "name": "Mark A. Anastasio"
            }
        ],
        "author_detail": {
            "name": "Mark A. Anastasio"
        },
        "author": "Mark A. Anastasio",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用临床相关合成数据集对基于深度学习的光声计算机断层扫描重建方法进行基准测试",
        "abstract_cn": "基于深度学习（DL）的光声计算机断层扫描（PACT）图像重建方法近年来发展迅速。然而，大多数现有方法没有采用标准化数据集，并且它们的评估依赖于可能缺乏临床相关性的传统图像质量（IQ）指标。缺乏具有临床意义的 IQ 评估的标准化框架阻碍了公平比较，并引发了人们对 PACT 报告进展的可重复性和可靠性的担忧。提出了一个基准测试框架，为 PACT 中基于深度学习的声学反演方法提供开源的、解剖学上合理的合成数据集和评估策略。每个数据集均包含超过 11,000 个二维 (2D) 随机乳房对象，这些对象具有临床相关病变以及不同建模复杂性的配对测量结果。评估策略结合了传统的和基于任务的 IQ 测量来评估保真度和临床实用性。我们进行了初步基准测试研究，通过比较基于深度学习和基于物理的重建方法来证明该框架的实用性。基准研究表明，所提出的框架能够对重建性能进行全面、定量的比较，并揭示了某些基于深度学习的方法的重要局限性。尽管根据传统的智商测量，他们表现良好，但他们常常无法准确地恢复病变。这凸显了传统指标的不足，并激发了基于任务的评估的需求。所提出的基准测试框架可以对基于深度学习的 2D PACT 声学反演方法进行系统比较。通过将临床相关的合成数据集与严格的评估协议相结合，它可以实现可重复、客观的评估，并促进 PACT 中的方法开发和系统优化。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17211v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17211v1",
        "title": "Structural Complexity of Brain MRI reveals age-associated patterns",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Structural Complexity of Brain MRI reveals age-associated patterns"
        },
        "updated": "2026-01-23T22:43:38Z",
        "updated_parsed": [
            2026,
            1,
            23,
            22,
            43,
            38,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17211v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17211v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T22:43:38Z",
        "published_parsed": [
            2026,
            1,
            23,
            22,
            43,
            38,
            4,
            23,
            0
        ],
        "arxiv_comment": "accepted by icassp2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Anzhe Cheng"
            },
            {
                "name": "Italo Ivo Lima Dias Pinto"
            },
            {
                "name": "Paul Bogdan"
            }
        ],
        "author_detail": {
            "name": "Paul Bogdan"
        },
        "author": "Paul Bogdan",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "大脑 MRI 的结构复杂性揭示了与年龄相关的模式",
        "abstract_cn": "我们将结构复杂性分析应用于三维信号，重点是脑磁共振成像（MRI）。该框架通过在逐渐增大的空间尺度上对信号进行粗粒度化并量化连续分辨率之间丢失的信息来捕获体数据的多尺度组织。虽然传统的基于块的方法由于采样有限而在粗分辨率下可能变得不稳定，但我们引入了滑动窗口粗粒度方案，该方案可以在大范围内提供更平滑的估计并提高鲁棒性。使用这种改进的方法，我们分析了成年中后期的大型结构 MRI 数据集，发现结构复杂性随着年龄的增长而系统性降低，在较粗的尺度上出现最强的影响。这些发现强调了结构复杂性作为一种可靠的信号处理工具，用于 3D 成像数据的多尺度分析，同时也证明了其在通过大脑 MRI 预测生物年龄方面的实用性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17228v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17228v1",
        "title": "Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification"
        },
        "updated": "2026-01-23T23:33:48Z",
        "updated_parsed": [
            2026,
            1,
            23,
            23,
            33,
            48,
            4,
            23,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17228v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17228v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "q-bio.QM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-23T23:33:48Z",
        "published_parsed": [
            2026,
            1,
            23,
            23,
            33,
            48,
            4,
            23,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Tengyue Zhang"
            },
            {
                "name": "Ruiwen Ding"
            },
            {
                "name": "Luoting Zhuang"
            },
            {
                "name": "Yuxiao Wu"
            },
            {
                "name": "Erika F. Rodriguez"
            },
            {
                "name": "William Hsu"
            }
        ],
        "author_detail": {
            "name": "William Hsu"
        },
        "author": "William Hsu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于病理图像分类的具有潜在扩散的半监督域适应",
        "abstract_cn": "由于领域转移，计算病理学中的深度学习模型通常无法跨群体和机构推广。现有方法要么无法利用目标域中的未标记数据，要么依赖图像到图像的转换，这可能会扭曲组织结构并损害模型的准确性。在这项工作中，我们提出了一种半监督域适应（SSDA）框架，该框架利用对来自源域和目标域的未标记数据进行训练的潜在扩散模型来生成形态保留和目标感知的合成图像。通过根据基础模型特征、队列身份和组织制备方法调节扩散模型，我们保留源域中的组织结构，同时引入目标域外观特征。目标感知合成图像与来自源队列的真实标记图像相结合，随后用于训练下游分类器，然后在目标队列上进行测试。所提出的 SSDA 框架的有效性在肺腺癌预测任务中得到了证明。所提出的增强在目标队列的保留测试集上产生了显着更好的性能，而不会降低源队列的性能。该方法将目标队列保留测试集的加权 F1 分数从 0.611 提高到 0.706，将宏观 F1 分数从 0.641 提高到 0.716。我们的结果表明，基于目标感知扩散的合成数据增强为改善计算病理学领域的泛化提供了一种有前途且有效的方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17254v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17254v1",
        "title": "Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization"
        },
        "updated": "2026-01-24T01:41:45Z",
        "updated_parsed": [
            2026,
            1,
            24,
            1,
            41,
            45,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17254v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17254v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T01:41:45Z",
        "published_parsed": [
            2026,
            1,
            24,
            1,
            41,
            45,
            5,
            24,
            0
        ],
        "arxiv_comment": "8 pages, 5 figures, 2 tables",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Takato Yasuno"
            }
        ],
        "author_detail": {
            "name": "Takato Yasuno"
        },
        "author": "Takato Yasuno",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "多级桥梁检测系统：将基础模型与位置匿名化相结合",
        "abstract_cn": "在日本，每五年强制通过目视检查进行民用基础设施状况监测。现场捕获的损坏图像经常包含混凝土裂缝和钢筋暴露，通常还伴有显示区域信息的施工标志。为了在不引起公众焦虑的情况下实现基础设施的安全使用，必须保护区域信息，同时准确提取损坏特征并可视化修复决策的关键指标。本文提出了一种具有区域隐私保护能力的开源桥梁损伤检测系统。我们采用 Segment Anything Model (SAM) 3 进行钢筋腐蚀检测，并利用 DBSCAN 自动完成遗漏区域。通过高斯模糊检测和保护施工标志区域。四种预处理方法提高了 OCR 准确性，GPU 优化使每张图像的处理时间为 1.7 秒。技术栈包括SAM3、PyTorch、OpenCV、pytesseract和scikit-learn，实现高效桥梁检测和区域信息保护。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17259v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17259v1",
        "title": "Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling"
        },
        "updated": "2026-01-24T02:18:25Z",
        "updated_parsed": [
            2026,
            1,
            24,
            2,
            18,
            25,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17259v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17259v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T02:18:25Z",
        "published_parsed": [
            2026,
            1,
            24,
            2,
            18,
            25,
            5,
            24,
            0
        ],
        "arxiv_comment": "25 Pages, 12 Figures, 3 Tables, 5 Appendices, 8 Algorithms",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Angad Singh Ahuja"
            },
            {
                "name": "Aarush Ram Anandh"
            }
        ],
        "author_detail": {
            "name": "Aarush Ram Anandh"
        },
        "author": "Aarush Ram Anandh",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "扩散采样中推理时间损失引导的颜色保留",
        "abstract_cn": "精确的颜色控制仍然是文本到图像扩散系统中持续存在的故障模式，特别是在以设计为导向的工作流程中，输出必须满足明确的、用户指定的颜色目标。我们提出了一种推理时间、区域约束的颜色保留方法，该方法无需任何额外的训练即可引导预训练的扩散模型。我们的方法结合了（i）基于 ROI 的空间选择性修复，（ii）背景潜在重新拼版以防止颜色漂移到 ROI 之外，以及（iii）使用 CIE Lab 和线性 RGB 中定义的复合损失通过梯度引导进行潜在微移。构建损失不仅可以控制平均 ROI 颜色，还可以通过 CVaR 式和软最大惩罚来控制像素误差分布的尾部，并使用晚启动门和时间相关的时间表来稳定整个去噪步骤的指导。我们证明，仅均值基线可以满足平均颜色约束，同时产生感知上显着的局部失败，从而激发我们的分布感知目标。由此产生的方法提供了一种实用的、免培训的机制，用于目标颜色粘附，可以集成到标准稳定扩散修复管道中。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17271v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17271v1",
        "title": "Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales"
        },
        "updated": "2026-01-24T03:00:45Z",
        "updated_parsed": [
            2026,
            1,
            24,
            3,
            0,
            45,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17271v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17271v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T03:00:45Z",
        "published_parsed": [
            2026,
            1,
            24,
            3,
            0,
            45,
            5,
            24,
            0
        ],
        "arxiv_comment": "TIP, 12 pages",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Kun Huang"
            },
            {
                "name": "Fang-Lue Zhang"
            },
            {
                "name": "Neil Dodgson"
            }
        ],
        "author_detail": {
            "name": "Neil Dodgson"
        },
        "author": "Neil Dodgson",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Cross360：通过跨尺度的交叉投影进行 360° 单目深度估计",
        "abstract_cn": "360° 深度估计是一个具有挑战性的研究问题，因为很难找到既能保持全局连续性又能避免球面图像失真的表示方法。现有方法试图利用多个预测的补充信息，但难以平衡全局和局部一致性。它们的局部补丁特征具有有限的全局感知，并且组合的全局表示不能解决补丁之间边界处的特征提取的差异。为了解决这些问题，我们提出了 Cross360，这是一种基于交叉注意力的新颖架构，使用失真较小的切线补丁和等距柱状特征来集成局部和全局信息。我们的交叉投影特征对齐模块采用交叉注意力将局部切线投影特征与等距柱状投影的 360° 视野对齐，确保每个切线投影块都了解全局上下文。此外，我们的渐进式特征聚合与注意模块逐步细化多尺度特征，提高深度估计的准确性。 Cross360 在大多数基准数据集上显着优于现有方法，尤其是那些提供整个 360° 图像的数据集，证明了其在准确且全局一致的深度估计方面的有效性。代码和模型可以在https://github.com/huangkun101230/Cross360获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17315v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17315v1",
        "title": "ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading"
        },
        "updated": "2026-01-24T05:49:41Z",
        "updated_parsed": [
            2026,
            1,
            24,
            5,
            49,
            41,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17315v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17315v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T05:49:41Z",
        "published_parsed": [
            2026,
            1,
            24,
            5,
            49,
            41,
            5,
            24,
            0
        ],
        "arxiv_comment": "12 pages",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xiaoyang Li"
            },
            {
                "name": "Runni Zhou"
            }
        ],
        "author_detail": {
            "name": "Runni Zhou"
        },
        "author": "Runni Zhou",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ClinNet：膝骨关节炎分级的双侧不对称性和原型记忆的证据序数回归",
        "abstract_cn": "由于级别间细微差异、注释不确定性以及疾病进展的固有顺序性质，基于放射线图像的膝骨关节炎 (KOA) 分级是一项关键但具有挑战性的任务。传统的深度学习方法通​​常将这个问题表述为确定性的多类分类，忽略退化的持续进展和专家注释的不确定性。在这项工作中，我们提出了 ClinNet，这是一种新颖的值得信赖的框架，它将 KOA 分级作为证据序数回归问题来解决。该方法集成了三个关键组件：（1）双边不对称编码器（BAE），明确模拟内侧-外侧结构差异； (2) 诊断内存库，维护分类原型以稳定特征表示； (3) 基于正态逆伽玛 (NIG) 分布的证据序数头，用于联合估计连续 KL 等级和认知不确定性。大量实验表明，ClinNet 的二次加权 Kappa 为 0.892，准确度为 0.768，在统计上优于最先进的基线 (p < 0.001)。至关重要的是，我们证明该模型的不确定性估计成功标记了分布外样本和潜在的误诊，为安全临床部署铺平了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17323v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17323v1",
        "title": "SkyReels-V3 Technique Report",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SkyReels-V3 Technique Report"
        },
        "updated": "2026-01-24T06:08:12Z",
        "updated_parsed": [
            2026,
            1,
            24,
            6,
            8,
            12,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17323v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17323v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T06:08:12Z",
        "published_parsed": [
            2026,
            1,
            24,
            6,
            8,
            12,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Debang Li"
            },
            {
                "name": "Zhengcong Fei"
            },
            {
                "name": "Tuanhui Li"
            },
            {
                "name": "Yikun Dou"
            },
            {
                "name": "Zheng Chen"
            },
            {
                "name": "Jiangping Yang"
            },
            {
                "name": "Mingyuan Fan"
            },
            {
                "name": "Jingtao Xu"
            },
            {
                "name": "Jiahua Wang"
            },
            {
                "name": "Baoxuan Gu"
            },
            {
                "name": "Mingshan Chang"
            },
            {
                "name": "Yuqiang Xie"
            },
            {
                "name": "Binjie Mao"
            },
            {
                "name": "Youqiang Zhang"
            },
            {
                "name": "Nuo Pang"
            },
            {
                "name": "Hao Zhang"
            },
            {
                "name": "Yuzhe Jin"
            },
            {
                "name": "Zhiheng Xu"
            },
            {
                "name": "Dixuan Lin"
            },
            {
                "name": "Guibin Chen"
            },
            {
                "name": "Yahui Zhou"
            }
        ],
        "author_detail": {
            "name": "Yahui Zhou"
        },
        "author": "Yahui Zhou",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SkyReels-V3 技术报告",
        "abstract_cn": "视频生成是构建世界模型的基石，其中多模态上下文推理是能力的定义测试。最后，我们提出了 SkyReels-V3，这是一种条件视频生成模型，它建立在带有扩散 Transformer 的统一多模态上下文学习框架之上。 SkyReels-V3 模型在单一架构中支持三种核心生成范例：参考图像到视频合成、视频到视频扩展和音频引导视频生成。 (i) 参考图像到视频模型旨在生成具有强大的主体身份保留、时间连贯性和叙事一致性的高保真视频。为了增强参考依从性和构图稳定性，我们设计了一个全面的数据处理管道，利用跨帧配对、图像编辑和语义重写，有效减少复制粘贴伪影。在训练过程中，采用图像视频混合策略与多分辨率联合优化相结合，以提高跨不同场景的泛化性和鲁棒性。 (ii) 视频扩展模型将时空一致性建模与大规模视频理解相结合，实现无缝单镜头连续和具有专业电影模式的智能多镜头切换。 （iii）会说话的化身模型通过训练首尾帧插入模式和重建关键帧推理范例来支持分钟级音频调节视频生成。在保证视觉质量的基础上，优化了音视频的同步。\n  广泛的评估表明，SkyReels-V3 在视觉质量、指令遵循和特定方面指标等关键指标上实现了最先进或接近最先进的性能，接近领先的闭源系统。 Github：https://github.com/SkyworkAI/SkyReels-V3。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17331v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17331v1",
        "title": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation"
        },
        "updated": "2026-01-24T06:27:25Z",
        "updated_parsed": [
            2026,
            1,
            24,
            6,
            27,
            25,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17331v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17331v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T06:27:25Z",
        "published_parsed": [
            2026,
            1,
            24,
            6,
            27,
            25,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Fabian Vazquez"
            },
            {
                "name": "Jose A. Nuñez"
            },
            {
                "name": "Diego Adame"
            },
            {
                "name": "Alissen Moreno"
            },
            {
                "name": "Augustin Zhan"
            },
            {
                "name": "Huimin Li"
            },
            {
                "name": "Jinghao Yang"
            },
            {
                "name": "Haoteng Tang"
            },
            {
                "name": "Bin Fu"
            },
            {
                "name": "Pengfei Gu"
            }
        ],
        "author_detail": {
            "name": "Pengfei Gu"
        },
        "author": "Pengfei Gu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用 U-Net 变体中的几何先验进行息肉分割学习",
        "abstract_cn": "准确而稳健的息肉分割对于早期结直肠癌检测和计算机辅助诊断至关重要。虽然基于卷积神经网络、Transformer 和 Mamba 的 U-Net 变体已经取得了强大的性能，但它们仍然难以捕捉几何和结构线索，特别是在低对比度或杂乱的结肠镜检查场景中。为了应对这一挑战，我们提出了一种新颖的几何先验引导模块（GPM），它将显式几何先验注入到基于 U-Net 的架构中以进行息肉分割。具体来说，我们在模拟的 ColonDepth 数据集上微调 Visual Geometry Grounded Transformer (VGGT)，以估计适合内窥镜领域的息肉图像的深度图。然后，这些深度图由 GPM 处理，将几何先验编码到编码器的特征图中，并使用强调局部空间和全局通道信息的空间和通道注意机制进一步细化它们。 GPM 是即插即用的，可以无缝集成到各种 U-Net 变体中。对五个公共息肉分割数据集的广泛实验表明，与三个强基线相比，获得了一致的增益。代码和生成的深度图可在以下位置获取：https://github.com/fvazqu/GPM-PolypSeg"
    },
    {
        "id": "http://arxiv.org/abs/2601.17340v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17340v1",
        "title": "TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution"
        },
        "updated": "2026-01-24T07:03:41Z",
        "updated_parsed": [
            2026,
            1,
            24,
            7,
            3,
            41,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17340v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17340v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T07:03:41Z",
        "published_parsed": [
            2026,
            1,
            24,
            7,
            3,
            41,
            5,
            24,
            0
        ],
        "arxiv_comment": "Accepted by ICASSP 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Haodong He"
            },
            {
                "name": "Xin Zhan"
            },
            {
                "name": "Yancheng Bai"
            },
            {
                "name": "Rui Lan"
            },
            {
                "name": "Lei Sun"
            },
            {
                "name": "Xiangxiang Chu"
            }
        ],
        "author_detail": {
            "name": "Xiangxiang Chu"
        },
        "author": "Xiangxiang Chu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "TEXTS-Diff：用于现实世界文本图像超分辨率的文本感知扩散模型",
        "abstract_cn": "现实世界的文本图像超分辨率旨在恢复遭受各种退化和文本扭曲的图像的整体视觉质量和文本易读性。然而，现有数据集中文本图像数据的稀缺导致文本区域的性能较差。此外，由孤立的文本样本组成的数据集限制了背景重建的质量。为了解决这些限制，我们构建了真实文本，这是一个从真实世界图像收集的大规模、高质量的数据集，它涵盖了不同的场景，并包含中文和英文的自然文本实例。此外，我们提出了文本感知扩散模型（TEXTS-Diff）来实现背景和文本区域的高质量生成。这种方法利用抽象概念来提高对视觉场景中文本元素的理解，并利用具体文本区域来增强文本细节。它减轻了文本区域中常见的扭曲和幻觉伪影，同时保持高质量的视觉场景保真度。大量的实验表明，我们的方法在多个评估指标上实现了最先进的性能，在复杂场景中表现出卓越的泛化能力和文本恢复准确性。所有代码、模型和数据集都将被发布。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17342v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17342v1",
        "title": "STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation"
        },
        "updated": "2026-01-24T07:07:16Z",
        "updated_parsed": [
            2026,
            1,
            24,
            7,
            7,
            16,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17342v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17342v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \\textbf{STARS} (\\textbf{S}hared-specific \\textbf{T}ranslation and \\textbf{A}lignment for missing-modality \\textbf{R}emote \\textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \\textbf{STARS} (\\textbf{S}hared-specific \\textbf{T}ranslation and \\textbf{A}lignment for missing-modality \\textbf{R}emote \\textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T07:07:16Z",
        "published_parsed": [
            2026,
            1,
            24,
            7,
            7,
            16,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Tong Wang"
            },
            {
                "name": "Xiaodong Zhang"
            },
            {
                "name": "Guanzhou Chen"
            },
            {
                "name": "Jiaqi Wang"
            },
            {
                "name": "Chenxi Liu"
            },
            {
                "name": "Xiaoliang Tan"
            },
            {
                "name": "Wenchao Guo"
            },
            {
                "name": "Xuyang Li"
            },
            {
                "name": "Xuanrui Wang"
            },
            {
                "name": "Zifan Wang"
            }
        ],
        "author_detail": {
            "name": "Zifan Wang"
        },
        "author": "Zifan Wang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "STARS：缺失模态遥感语义分割的共享特定翻译和对齐",
        "abstract_cn": "多模态遥感技术通过集成光学图像、合成孔径雷达（SAR）和数字表面模型（DSM）等异构数据，显着增强对表面语义的理解。然而，在实际应用中，模态数据（例如光学或DSM）的丢失是一个常见且严峻的挑战，这导致传统多模态融合模型的性能下降。解决缺失模态的现有方法仍然面临局限性，包括特征崩溃和过度概括的恢复特征。为了解决这些问题，我们提出了 \\textbf{STARS} （\\textbf{S}共享特定的 \\textbf{T}translation 和 \\textbf{A}lignment for Missing-modality \\textbf{R}emote \\textbf{S}ensing），这是一个针对不完整多模态输入的强大语义分割框架。 STARS 基于两个关键设计。首先，我们引入了一种具有双向平移和停止梯度的非对称对齐机制，可以有效防止特征崩溃并降低对超参数的敏感性。其次，我们提出了一种像素级语义采样对齐（PSA）策略，该策略将类平衡像素采样与跨模态语义对齐损失相结合，以减轻严重类不平衡导致的对齐失败并提高少数类识别。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17349v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17349v1",
        "title": "Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective"
        },
        "updated": "2026-01-24T07:27:54Z",
        "updated_parsed": [
            2026,
            1,
            24,
            7,
            27,
            54,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17349v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17349v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T07:27:54Z",
        "published_parsed": [
            2026,
            1,
            24,
            7,
            27,
            54,
            5,
            24,
            0
        ],
        "arxiv_comment": "Tech report",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Hailong Yan"
            },
            {
                "name": "Shice Liu"
            },
            {
                "name": "Xiangtao Zhang"
            },
            {
                "name": "Lujian Yao"
            },
            {
                "name": "Fengxiang Yang"
            },
            {
                "name": "Jinwei Chen"
            },
            {
                "name": "Bo Li"
            }
        ],
        "author_detail": {
            "name": "Bo Li"
        },
        "author": "Bo Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "重新审视轻量级低光图像增强：从 YUV 色彩空间角度",
        "abstract_cn": "在当前的移动互联网时代，轻量级低光图像增强（L3IE）对于移动设备至关重要，移动设备面临着视觉质量和模型紧凑性之间的持续权衡。虽然最近的方法采用解缠结策略来简化轻量级架构设计，例如 Retinex 理论和 YUV 色彩空间转换，但它们的性能从根本上受到忽略通道特定退化模式和跨通道交互的限制。为了解决这一差距，我们进行了频域分析，证实了 L3IE 的 YUV 颜色空间的优越性。我们发现了一个关键的见解：Y 通道主要丢失低频内容，而 UV 通道则被高频噪声破坏。利用这一发现，我们提出了一种基于 YUV 的新颖范例，该范例使用用于 Y 通道的双流全局局部注意模块、用于 UV 通道的 Y 引导局部感知频率注意模块以及用于最终特征融合的引导交互模块来战略性地恢复通道。大量的实验验证了我们的模型在多个基准上建立了新的最先进技术，以显着较低的参数数量提供卓越的视觉质量。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17350v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17350v1",
        "title": "NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields"
        },
        "updated": "2026-01-24T07:32:06Z",
        "updated_parsed": [
            2026,
            1,
            24,
            7,
            32,
            6,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17350v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17350v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \\textbf{P}atch-based \\textbf{E}ntropy for \\textbf{R}ay \\textbf{E}mitting (\\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \\textbf{P}rogressively \\textbf{I}terative \\textbf{RE}storation (\\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \\textbf{P}atch-based \\textbf{E}ntropy for \\textbf{R}ay \\textbf{E}mitting (\\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \\textbf{P}rogressively \\textbf{I}terative \\textbf{RE}storation (\\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T07:32:06Z",
        "published_parsed": [
            2026,
            1,
            24,
            7,
            32,
            6,
            5,
            24,
            0
        ],
        "arxiv_comment": "14 pages, 15 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xianliang Huang"
            },
            {
                "name": "Zhizhou Zhong"
            },
            {
                "name": "Shuhang Chen"
            },
            {
                "name": "Yi Xu"
            },
            {
                "name": "Juhong Guan"
            },
            {
                "name": "Shuigeng Zhou"
            }
        ],
        "author_detail": {
            "name": "Shuigeng Zhou"
        },
        "author": "Shuigeng Zhou",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "NeRF-MIR：利用神经辐射场实现蒙版图像的高质量恢复",
        "abstract_cn": "神经辐射场（NeRF）在新颖的视图合成中表现出了卓越的性能。然而，基于 NeRF 从损坏的图像中恢复 3D 场景还有很大的改进空间，这在自然场景捕获中很常见，并且会显着影响 NeRF 的有效性。本文介绍了 NeRF-MIR，这是一种专门为修复蒙版图像而提出的新型神经渲染方法，展示了 NeRF 在该领域的潜力。认识到在 NeRF 中向像素随机发射射线可能无法有效地学习复杂的图像纹理，我们提出了一种基于 \\textbf{P}atch 的 \\textbf{E}ntropy，用于 \\textbf{R}ay \\textbf{E}mitting (\\textbf{PERE}) 策略来正确分配发射的射线。这使得 NeRF-MIR 能够融合不同视图图像的综合信息。此外，我们引入了一种 \\textbf{P}progressively \\textbf{I}terative \\textbf{RE}storation (\\textbf{PIRE}) 机制来在自训练过程中恢复屏蔽区域。此外，我们设计了一个动态加权损失函数，可以自动重新校准屏蔽区域的损失权重。由于现有数据集不支持基于 NeRF 的蒙版图像恢复，我们构建了三个蒙版数据集来模拟损坏的场景。对真实数据和构建数据集的大量实验证明了 NeRF-MIR 在蒙版图像恢复方面优于同类产品。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17352v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17352v1",
        "title": "HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data"
        },
        "updated": "2026-01-24T07:57:01Z",
        "updated_parsed": [
            2026,
            1,
            24,
            7,
            57,
            1,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17352v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17352v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T07:57:01Z",
        "published_parsed": [
            2026,
            1,
            24,
            7,
            57,
            1,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "M. L. Mamud"
            },
            {
                "name": "Piyoosh Jaysaval"
            },
            {
                "name": "Frederick D Day-Lewis"
            },
            {
                "name": "M. K. Mudunuru"
            }
        ],
        "author_detail": {
            "name": "M. K. Mudunuru"
        },
        "author": "M. K. Mudunuru",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "HyDeMiC：使用高光谱数据的基于深度学习的矿物分类器",
        "abstract_cn": "高光谱成像 (HSI) 已成为一种强大的矿物勘探遥感工具，利用了矿物独特的光谱特征。然而，判别分析、逻辑回归和支持向量机等传统分类方法常常面临数据环境噪声、传感器限制以及分析高维 HSI 数据的计算复杂性等问题。本研究提出了 HyDeMiC（基于高光谱深度学习的矿物分类器），这是一种卷积神经网络 (CNN) 模型，专为在噪声数据下进行稳健的矿物分类而设计。为了训练 HyDeMiC，使用了来自美国地质调查局 (USGS) 图书馆的 115 种矿物的实验室测量高光谱数据，这些矿物涵盖不同的矿物组。训练数据集是通过将参考矿物光谱与 HSI 传感器响应函数进行卷积而生成的。这些数据集包含三种含铜矿物：赤铜矿、孔雀石和黄铜矿，用作性能演示的案例研究。经过训练的 CNN 模型在噪声水平为 1%、2%、5% 和 10% 的多个合成 2D 高光谱数据集上进行了评估。我们的噪声数据分析旨在复制真实的现场条件。 HyDeMiC 的性能使用马修斯相关系数 (MCC) 进行评估，提供跨不同噪声状况的综合测量。结果表明，HyDeMiC 在干净和低噪声的数据集上实现了近乎完美的分类精度 (MCC = 1.00)，并在中等噪声条件下保持了强大的性能。这些发现强调了 HyDeMiC 在存在中等噪声的情况下的鲁棒性，突出了其在高光谱成像中实际应用的潜力，在高光谱成像中，噪声往往是一个重大挑战。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17366v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17366v1",
        "title": "UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation"
        },
        "updated": "2026-01-24T08:21:14Z",
        "updated_parsed": [
            2026,
            1,
            24,
            8,
            21,
            14,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17366v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17366v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T08:21:14Z",
        "published_parsed": [
            2026,
            1,
            24,
            8,
            21,
            14,
            5,
            24,
            0
        ],
        "arxiv_comment": "Accepted by ISBI 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chengbo Ding"
            },
            {
                "name": "Fenghe Tang"
            },
            {
                "name": "Shaohua Kevin Zhou"
            }
        ],
        "author_detail": {
            "name": "Shaohua Kevin Zhou"
        },
        "author": "Shaohua Kevin Zhou",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "UCAD：用于半监督医学图像分割的不确定性引导轮廓感知位移",
        "abstract_cn": "半监督分割中现有的位移策略仅适用于矩形区域，忽略了解剖结构并导致边界扭曲和语义不一致。为了解决这些问题，我们提出了 UCAD，一种用于半监督医学图像分割的不确定性引导轮廓感知位移框架，它保留轮廓感知语义，同时增强一致性学习。我们的 UCAD 利用超像素生成与解剖学边界对齐的解剖学连贯区域，并利用不确定性引导选择机制选择性地取代具有挑战性的区域，以实现更好的一致性学习。我们进一步提出了一种动态不确定性加权一致性损失，它自适应地稳定训练并有效地规范未标记区域的模型。大量实验表明，UCAD 始终优于最先进的半监督分割方法，在有限的注释下实现了卓越的分割精度。代码位于：https://github.com/dcb937/UCAD。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17388v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17388v1",
        "title": "ONRW: Optimizing inversion noise for high-quality and robust watermark",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ONRW: Optimizing inversion noise for high-quality and robust watermark"
        },
        "updated": "2026-01-24T09:22:29Z",
        "updated_parsed": [
            2026,
            1,
            24,
            9,
            22,
            29,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17388v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17388v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T09:22:29Z",
        "published_parsed": [
            2026,
            1,
            24,
            9,
            22,
            29,
            5,
            24,
            0
        ],
        "arxiv_comment": "Preprint. Under review",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xuan Ding"
            },
            {
                "name": "Xiu Yan"
            },
            {
                "name": "Chuanlong Xie"
            },
            {
                "name": "Yao Zhu"
            }
        ],
        "author_detail": {
            "name": "Yao Zhu"
        },
        "author": "Yao Zhu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ONRW：优化反相噪声以获得高质量和鲁棒的水印",
        "abstract_cn": "水印方法一直是保护知识产权的有效手段，但也面临着重大挑战。尽管现有的基于深度学习的水印系统可以在对图像质量影响最小的情况下隐藏图像中的水印，但在传输过程中遇到图像损坏时往往缺乏鲁棒性，这损害了其实际应用价值。为此，我们提出了一种基于扩散模型的高质量且鲁棒的水印框架。我们的方法首先通过空文本优化过程将干净图像转换为反转噪声，在优化潜在空间中的反转噪声后，通过扩散模型的迭代去噪过程产生高质量的水印图像。迭代去噪过程作为强大的净化机制，既保证了水印图像的视觉质量，又增强了水印对各种损坏的鲁棒性。为了防止反演噪声的优化扭曲图像的原始语义，我们专门引入了自注意力约束和伪掩模策略。大量的实验结果证明了我们的方法针对各种图像损坏的卓越性能。特别是，我们的方法在 COCO 数据集上的 12 种不同图像转换中平均优于稳定签名方法 10%。我们的代码可在 https://github.com/920927/ONRW 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17391v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17391v1",
        "title": "SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition"
        },
        "updated": "2026-01-24T09:24:42Z",
        "updated_parsed": [
            2026,
            1,
            24,
            9,
            24,
            42,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17391v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17391v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T09:24:42Z",
        "published_parsed": [
            2026,
            1,
            24,
            9,
            24,
            42,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Rui Fan"
            },
            {
                "name": "Weidong Hao"
            }
        ],
        "author_detail": {
            "name": "Weidong Hao"
        },
        "author": "Weidong Hao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SMV-EAR：将时空多视图表示学习引入高效的基于事件的动作识别",
        "abstract_cn": "事件摄像机动作识别 (EAR) 提供了引人注目的隐私保护和效率优势，其中时间运动动态非常重要。现有的基于事件的对象识别 (EOR) 的时空多视图表示学习 (SMVRL) 方法通过沿空间轴 H 和 W 投影 H-W-T 事件提供了有前途的解决方案，但受到其平移变量空间分箱表示和朴素的早期串联融合架构的限制。本文重新审视了 EAR 的关键 SMVRL 设计阶段，并提出：（i）通过稀疏事件的平移不变密集转换来实现有原则的时空多视图表示，（ii）双分支动态融合架构，对不同视图的运动特征之间的样本互补性进行建模，以及（iii）模拟现实世界人类动作速度变化的生物启发时间扭曲增强。在 HARDVS、DailyDVS-200 和 THU-EACT-50-CHL 三个具有挑战性的 EAR 数据集上，我们显示出比现有 SVRL EOR 方法 +7.0%、+10.7% 和 +10.2% 的 Top-1 精度增益，参数减少了 30.1%，计算量减少了 35.7%，使我们的框架成为一种新颖而强大的 EAR 范式。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17420v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17420v1",
        "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction"
        },
        "updated": "2026-01-24T11:41:54Z",
        "updated_parsed": [
            2026,
            1,
            24,
            11,
            41,
            54,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17420v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17420v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T11:41:54Z",
        "published_parsed": [
            2026,
            1,
            24,
            11,
            41,
            54,
            5,
            24,
            0
        ],
        "arxiv_comment": "Project page: https://danielshkao.github.io/cot-seg.html",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Shiu-hong Kao"
            },
            {
                "name": "Chak Ho Huang"
            },
            {
                "name": "Huaiqian Liu"
            },
            {
                "name": "Yu-Wing Tai"
            },
            {
                "name": "Chi-Keung Tang"
            }
        ],
        "author_detail": {
            "name": "Chi-Keung Tang"
        },
        "author": "Chi-Keung Tang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "CoT-Seg：通过思想链推理和自我修正重新思考细分",
        "abstract_cn": "现有的推理分割工作在复杂的情况下通常存在不足，特别是在处理复杂的查询和域外图像时。受思想链推理的启发，更困难的问题需要更长的思考步骤/时间，本文旨在探索一种能够逐步思考、根据需要查找信息、生成结果、自我评估自己的结果并完善结果的系统，就像人类处理更困难的问题一样。我们引入了 CoT-Seg，这是一个免训练框架，通过将思想链推理与自我纠正相结合来重新思考推理分割。 CoT-Seg 不是进行微调，而是利用预训练的 MLLM (GPT-4o) 固有的推理能力将查询分解为元指令，从图像中提取细粒度语义，甚至在隐式或复杂的提示下识别目标对象。此外，CoT-Seg 还包含一个自我校正阶段：模型根据原始查询和推理轨迹评估其自身的分割，识别不匹配，并迭代地细化掩码。这种推理和纠正的紧密结合显着提高了可靠性和鲁棒性，特别是在模棱两可或容易出错的情况下。此外，我们的 CoT-Seg 框架可以轻松合并检索增强推理，使系统能够在输入缺乏足够信息时访问外部知识。为了展示 CoT-Seg 处理非常具有挑战性的情况的能力，我们引入了一个新的数据集 ReasonSeg-Hard。我们的结果强调，将思想链推理和自我纠正相结合，为视觉语言集成驱动的分割提供了强大的范例。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17429v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17429v1",
        "title": "Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography"
        },
        "updated": "2026-01-24T11:56:49Z",
        "updated_parsed": [
            2026,
            1,
            24,
            11,
            56,
            49,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17429v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17429v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T11:56:49Z",
        "published_parsed": [
            2026,
            1,
            24,
            11,
            56,
            49,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Mehdi Yousefzadeh"
            },
            {
                "name": "Siavash Shirzadeh Barough"
            },
            {
                "name": "Ashkan Fakharifar"
            },
            {
                "name": "Yashar Tayyarazad"
            },
            {
                "name": "Narges Eghbali"
            },
            {
                "name": "Mohaddeseh Mozaffari"
            },
            {
                "name": "Hoda Taeb"
            },
            {
                "name": "Negar Sadat Rafiee Tabatabaee"
            },
            {
                "name": "Parsa Esfahanian"
            },
            {
                "name": "Ghazaleh Sadeghi Gohar"
            },
            {
                "name": "Amineh Safavirad"
            },
            {
                "name": "Saeideh Mazloomzadeh"
            },
            {
                "name": "Ehsan khalilipur"
            },
            {
                "name": "Armin Elahifar"
            },
            {
                "name": "Majid Maleki"
            }
        ],
        "author_detail": {
            "name": "Majid Maleki"
        },
        "author": "Majid Maleki",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "X 射线血管造影中的冠状动脉分割和血管类型分类",
        "abstract_cn": "X射线冠状动脉造影（XCA）是评估冠状动脉疾病的临床参考标准，但定量分析由于常规数据中稳健的血管分割困难而受到限制。低对比度、运动、透视缩短、重叠和导管混杂会降低分割效果，并导致跨中心的域转移。可靠的分割与血管类型标记相结合，可以实现血管特定的冠状动脉分析和依赖于解剖定位的下游测量。从 670 个电影序列（407 个受试者）中，我们使用低强度直方图标准选择接近峰值不透明的最佳帧，并应用联合超分辨率和增强。我们在每图像预言调整、单一全局均值设置以及通过支持向量回归 (SVR) 的每图像参数预测下对经典 Meijering、Frangi 和 Sato 血管过滤器进行基准测试。神经基线包括 U-Net、FPN 和 Swin Transformer，仅使用冠状动脉监督和合并冠状动脉+导管监督进行训练。第二阶段分配船舶标识（LAD、LCX、RCA）。外部评估使用公共 DCA1 队列。 SVR 每图像调整提高了 Dice 的所有经典滤波器的全局平均值（例如，Frangi：0.759 与 0.741）。在深度模型中，FPN 达到 0.914+/-0.007 Dice（仅冠状动脉），合并冠状动脉+导管标签进一步提高至 0.931+/-0.006。在 DCA1 作为严格的外部测试时，Dice 下降至 0.798（仅冠状动脉）和 0.814（合并），而轻度域内微调则恢复至 0.881+/-0.014 和 0.882+/-0.015。容器类型标记的 RCA 准确率为 98.5%（Dice 0.844），LAD 为 95.4%（0.786），LCX 为 96.2%（0.794）。学习的每图像调整增强了经典管道，而高分辨率 FPN 模型和合并标签监督通过适度的适应提高了稳定性和外部传输。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17468v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17468v1",
        "title": "ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation"
        },
        "updated": "2026-01-24T13:52:21Z",
        "updated_parsed": [
            2026,
            1,
            24,
            13,
            52,
            21,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17468v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17468v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T13:52:21Z",
        "published_parsed": [
            2026,
            1,
            24,
            13,
            52,
            21,
            5,
            24,
            0
        ],
        "arxiv_comment": "Project page: https://wuw2135.github.io/ReflexSplit-ProjectPage/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chia-Ming Lee"
            },
            {
                "name": "Yu-Fan Lin"
            },
            {
                "name": "Jing-Hui Jung"
            },
            {
                "name": "Yu-Jou Hsiao"
            },
            {
                "name": "Chih-Chung Hsu"
            },
            {
                "name": "Yu-Lun Liu"
            }
        ],
        "author_detail": {
            "name": "Yu-Lun Liu"
        },
        "author": "Yu-Lun Liu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ReflexSplit：通过层融合分离的单图像反射分离",
        "abstract_cn": "单图像反射分离 (SIRS) 将混合图像分解为透射层和反射层。由于隐式融合机制和不充分的多尺度协调，现有方法在非线性混合下遭受传输反射混淆，特别是在深层解码器层中。我们提出了 ReflexSplit，这是一个具有三项关键创新的双流框架。 (1) 跨尺度门控融合 (CrGF) 自适应地聚合跨层次深度的语义先验、纹理细节和解码器上下文，稳定梯度流并保持特征一致性。 (2)层融合-分离块(LFSB)在用于共享结构提取的融合和用于特定层解开的差分分离之间交替。受差分变压器的启发，我们通过跨流减法将注意力消除扩展到双流分离。 （3）课程训练通过深度相关的初始化和epoch-wise的预热逐步加强差异分离。对合成和现实世界基准的广泛实验证明了最先进的性能、卓越的感知质量和强大的泛化能力。我们的代码可在 https://github.com/wuw2135/ReflexSplit 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17504v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17504v1",
        "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation"
        },
        "updated": "2026-01-24T16:06:43Z",
        "updated_parsed": [
            2026,
            1,
            24,
            16,
            6,
            43,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17504v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17504v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "q-bio.QM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T16:06:43Z",
        "published_parsed": [
            2026,
            1,
            24,
            16,
            6,
            43,
            5,
            24,
            0
        ],
        "arxiv_comment": "16 pages, 5 figures. Manuscript prepared for submission to ACM TOMM",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yan Zhou"
            },
            {
                "name": "Zhen Huang"
            },
            {
                "name": "Yingqiu Li"
            },
            {
                "name": "Yue Ouyang"
            },
            {
                "name": "Suncheng Xiang"
            },
            {
                "name": "Zehua Wang"
            }
        ],
        "author_detail": {
            "name": "Zehua Wang"
        },
        "author": "Zehua Wang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "BMDS-Net：用于鲁棒脑肿瘤分割的贝叶斯多模态深度监督网络",
        "abstract_cn": "多模态磁共振成像 (MRI) 的准确脑肿瘤分割是精确放疗计划和手术导航的先决条件。虽然最近基于 Transformer 的模型（例如 Swin UNETR）已经取得了令人印象深刻的基准性能，但它们的临床实用性常常受到两个关键问题的影响：对缺失模式的敏感性（在临床实践中常见）和缺乏置信度校准。仅仅追求理想化数据的更高 Dice 分数无法满足现实医疗部署的安全要求。在这项工作中，我们提出了 BMDS-Net，这是一个统一的框架，优先考虑临床稳健性和可信度，而不是简单的指标最大化。我们的贡献是三方面的。首先，我们通过集成零初始多模态上下文融合（MMCF）模块和残差门控深度解码器监督（DDS）机制构建了一个强大的确定性主干，即使在模态损坏的情况下，也能实现稳定的特征学习和精确的边界描绘，并显着降低豪斯多夫距离。其次，也是最重要的，我们引入了一种内存高效的贝叶斯微调策略，该策略将网络转变为概率预测器，提供体素方面的不确定性图来为临床医生突出潜在的错误。第三，对 BraTS 2021 数据集的综合实验表明，BMDS-Net 不仅保持了有竞争力的准确性，更重要的是，在基线模型失败的缺失模态场景中表现出了卓越的稳定性。源代码可在 https://github.com/RyanZhou168/BMDS-Net 上公开获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17529v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17529v1",
        "title": "FMIR, a foundation model-based Image Registration Framework for Robust Image Registration",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "FMIR, a foundation model-based Image Registration Framework for Robust Image Registration"
        },
        "updated": "2026-01-24T17:05:18Z",
        "updated_parsed": [
            2026,
            1,
            24,
            17,
            5,
            18,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17529v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17529v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T17:05:18Z",
        "published_parsed": [
            2026,
            1,
            24,
            17,
            5,
            18,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Fengting Zhang"
            },
            {
                "name": "Yue He"
            },
            {
                "name": "Qinghao Liu"
            },
            {
                "name": "Yaonan Wang"
            },
            {
                "name": "Xiang Chen"
            },
            {
                "name": "Hang Zhang"
            }
        ],
        "author_detail": {
            "name": "Hang Zhang"
        },
        "author": "Hang Zhang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "FMIR，一种基于模型的稳健图像配准图像配准框架",
        "abstract_cn": "深度学习通过实现前所未有的速度彻底改变了医学图像配准，但其临床应用受到训练领域之外的泛化能力有限的阻碍，考虑到医疗数据集的规模通常较小，这是一个关键弱点。在本文中，我们介绍了 FMIR，一种基于基础模型的配准框架，克服了这一限制。将用于提取解剖结构的基于基础模型的特征编码器与通用配准头相结合，并仅在单个数据集上使用通道正则化策略进行训练，FMIR 实现了最先进的（SOTA）域内性能，同时保持对域外图像的鲁棒配准。我们的方法展示了利用有限资源构建通用医学成像基础模型的可行路径。该代码可在 https://github.com/Monday0328/FMIR.git 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17535v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17535v1",
        "title": "Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries"
        },
        "updated": "2026-01-24T17:30:23Z",
        "updated_parsed": [
            2026,
            1,
            24,
            17,
            30,
            23,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17535v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17535v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T17:30:23Z",
        "published_parsed": [
            2026,
            1,
            24,
            17,
            30,
            23,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Kevin Robbins"
            },
            {
                "name": "Xiaotong Liu"
            },
            {
                "name": "Yu Wu"
            },
            {
                "name": "Le Sun"
            },
            {
                "name": "Grady McPeak"
            },
            {
                "name": "Abby Stylianou"
            },
            {
                "name": "Robert Pless"
            }
        ],
        "author_detail": {
            "name": "Robert Pless"
        },
        "author": "Robert Pless",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "它会零样本吗？：它会零样本吗？：预测任意查询的零样本分类性能",
        "abstract_cn": "像 CLIP 这样的视觉语言模型为文本和图像创建对齐的嵌入空间，使任何人都可以通过简单地命名他们想要区分的类来构建视觉分类器。然而，在一个领域运行良好的模型可能在另一个领域失败，并且非专家用户没有直接的方法来评估他们选择的 VLM 是否能解决他们的问题。我们在之前的工作基础上，使用纯文本比较来评估模型对于给定自然语言任务的效果，并探索还生成与该任务相关的合成图像的方法，以评估和完善零样本准确性的预测。我们表明，根据基线纯文本分数生成的图像大大提高了这些预测的质量。此外，它还为用户提供有关用于进行评估的图像类型的反馈。标准 CLIP 基准数据集上的实验表明，基于图像的方法可以帮助用户在没有任何标记示例的情况下预测 VLM 是否对其应用有效。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17536v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17536v1",
        "title": "OTI: A Model-free and Visually Interpretable Measure of Image Attackability",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "OTI: A Model-free and Visually Interpretable Measure of Image Attackability"
        },
        "updated": "2026-01-24T17:32:04Z",
        "updated_parsed": [
            2026,
            1,
            24,
            17,
            32,
            4,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17536v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17536v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T17:32:04Z",
        "published_parsed": [
            2026,
            1,
            24,
            17,
            32,
            4,
            5,
            24,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jiaming Liang"
            },
            {
                "name": "Haowei Liu"
            },
            {
                "name": "Chi-Man Pun"
            }
        ],
        "author_detail": {
            "name": "Chi-Man Pun"
        },
        "author": "Chi-Man Pun",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "OTI：一种无模型且可视觉解释的图像可攻击性测量方法",
        "abstract_cn": "尽管神经网络取得了巨大成功，但良性图像可能会被对抗性扰动破坏，从而欺骗这些模型。有趣的是，图像的可攻击性有所不同。具体来说，在给定攻击配置的情况下，某些图像很容易被损坏，而另一些图像则更具抵抗力。评估图像的可攻击性在主动学习、对抗训练和攻击增强方面具有重要的应用。这促使人们对开发可攻击性措施越来越感兴趣。然而，现有的方法很稀缺，并且存在两个主要限制：（1）它们依赖模型代理来提供先验知识（例如梯度或最小扰动）来提取与模型相关的图像特征。不幸的是，在实践中，许多特定于任务的模型并不容易访问。 （2）提取的表征图像可攻击性的特征缺乏视觉可解释性，模糊了它们与图像的直接关系。为了解决这些问题，我们提出了一种新颖的对象纹理强度（OTI），这是一种无模型且视觉上可解释的图像攻击性度量，它将图像攻击性衡量为图像语义对象的纹理强度。理论上，我们从决策边界以及对抗性扰动的中高频特征的角度描述了OTI的原理。综合实验表明 OTI 是有效的并且计算效率高。此外，我们的 OTI 为对抗性机器学习社区提供了对可攻击性的直观理解。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17555v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17555v1",
        "title": "Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper"
        },
        "updated": "2026-01-24T18:52:19Z",
        "updated_parsed": [
            2026,
            1,
            24,
            18,
            52,
            19,
            5,
            24,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17555v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17555v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1145/3589132.3625638",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-24T18:52:19Z",
        "published_parsed": [
            2026,
            1,
            24,
            18,
            52,
            19,
            5,
            24,
            0
        ],
        "arxiv_comment": "Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems (2023)",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Justin Downes"
            },
            {
                "name": "Sam Saltwick"
            },
            {
                "name": "Anthony Chen"
            }
        ],
        "author_detail": {
            "name": "Anthony Chen"
        },
        "author": "Anthony Chen",
        "arxiv_doi": "10.1145/3589132.3625638",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "显着性驱动的图像预处理以实现高效压缩——工业用纸",
        "abstract_cn": "卫星图像的压缩仍然是一个重要的研究领域，因为每天都会收集数百 TB 的图像，这会增加存储和带宽成本。尽管在提高这些卫星图像的分辨率方面已经取得了进展，但许多下游任务只对任何给定图像的小区域感兴趣。这些感兴趣的领域因任务而异，但一旦已知，就可以用于优化图像中信息的编码方式。尽管标准图像编码方法（即使是针对遥感优化的方法）对整个图像同样有效，但有一些新兴方法可以在显着图的指导下重点关注重要区域。在这项工作中，我们展示了如何将显着图驱动的图像预处理技术与传统的有损压缩编码标准结合使用，以在单个大型卫星图像中创建可变速率图像压缩。具体来说，我们使用映射到不同量化显着性级别的可变大小的平滑内核来处理图像像素，以优化下游压缩和编码方案。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17657v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17657v1",
        "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation"
        },
        "updated": "2026-01-25T02:32:01Z",
        "updated_parsed": [
            2026,
            1,
            25,
            2,
            32,
            1,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17657v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17657v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T02:32:01Z",
        "published_parsed": [
            2026,
            1,
            25,
            2,
            32,
            1,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Taewan Cho"
            },
            {
                "name": "Taeryang Kim"
            },
            {
                "name": "Andrew Jaeyong Choi"
            }
        ],
        "author_detail": {
            "name": "Andrew Jaeyong Choi"
        },
        "author": "Andrew Jaeyong Choi",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SPACE-CLIP：通过自适应 CLIP 嵌入实现单目深度估计的空间感知",
        "abstract_cn": "对比语言-图像预训练（CLIP）在语义理解方面取得了非凡的成功，但本质上难以感知几何结构。现有方法试图通过使用文本提示查询 CLIP 来弥补这一差距，但这一过程通常是间接且低效的。本文介绍了一种使用双通道解码器的根本不同的方法。我们提出了 SPACE-CLIP，一种直接从冻结的 CLIP 视觉编码器解锁和解释潜在几何知识的架构，完全绕过文本编码器及其相关的文本提示。语义路径解释高级特征，并使用特征线性调制 (FiLM) 动态调节全局上下文。此外，结构路径从早期层中提取细粒度的空间细节。这些互补的流是分层融合的，能够实现语义上下文和精确几何的稳健综合。 KITTI 基准测试的大量实验表明，SPACE-CLIP 的性能显着优于以前基于 CLIP 的方法。我们的消融研究证实，我们的双重途径的协同融合对于这一成功至关重要。 SPACE-CLIP 为重新利用大型视觉模型提供了一个新的、高效的、结构优雅的蓝图。所提出的方法不仅仅是一个独立的深度估计器，而且是一个易于集成的空间感知模块，适用于下一代具体人工智能系统，例如视觉-语言-动作（VLA）模型。我们的模型可在 https://github.com/taewan2002/space-clip 获取"
    },
    {
        "id": "http://arxiv.org/abs/2601.17666v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17666v1",
        "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting"
        },
        "updated": "2026-01-25T03:07:17Z",
        "updated_parsed": [
            2026,
            1,
            25,
            3,
            7,
            17,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17666v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17666v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T03:07:17Z",
        "published_parsed": [
            2026,
            1,
            25,
            3,
            7,
            17,
            6,
            25,
            0
        ],
        "arxiv_comment": "Accepted by CAI2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xinyue Pan"
            },
            {
                "name": "Yuhao Chen"
            },
            {
                "name": "Fengqing Zhu"
            }
        ],
        "author_detail": {
            "name": "Fengqing Zhu"
        },
        "author": "Fengqing Zhu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过快速嫁接生成免训练文本到图像组合食物",
        "abstract_cn": "现实世界的膳食图像通常包含多种食物，因此可靠的组合食物图像生成对于基于图像的饮食评估（需要多种食物数据增强）和食谱可视化等应用非常重要。然而，由于对象纠缠，现代文本到图像扩散模型很难生成准确的多食物图像，其中相邻食物（例如米饭和汤）融合在一起，因为许多食物没有明确的边界。为了应对这一挑战，我们引入了 Prompt Grafting (PG)，这是一种无需训练的框架，它将文本中的显式空间线索与采样过程中隐式的布局指导相结合。 PG 运行一个两阶段过程，其中布局提示首先建立不同的区域，一旦布局形成稳定，目标提示就会被嫁接。该框架支持食物纠缠控制：用户可以通过编辑布局的排列来指定哪些食物应保持分离或有意混合。在两个食物数据集中，我们的方法显着改善了目标对象的存在，并提供了可控分离的定性证据。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17673v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17673v1",
        "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing"
        },
        "updated": "2026-01-25T03:22:26Z",
        "updated_parsed": [
            2026,
            1,
            25,
            3,
            22,
            26,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17673v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17673v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T03:22:26Z",
        "published_parsed": [
            2026,
            1,
            25,
            3,
            22,
            26,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Weiyu Zhang"
            },
            {
                "name": "Yuan Hu"
            },
            {
                "name": "Yong Li"
            },
            {
                "name": "Yu Liu"
            }
        ],
        "author_detail": {
            "name": "Yu Liu"
        },
        "author": "Yu Liu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Uni-RS：遥感的空间忠实统一理解和生成模型",
        "abstract_cn": "统一的遥感多模态模型表现出明显的空间反转诅咒：虽然它们可以准确地识别和描述图像中的对象位置，但它们在文本到图像的生成过程中往往无法忠实地执行相同的空间关系，而这种关系构成了遥感的核心语义信息。受这一观察的启发，我们提出了 Uni-RS，这是第一个专为遥感定制的统一多模态模型，以明确解决理解和生成之间的空间不对称问题。具体来说，我们首先引入显式空间布局规划，将文本指令转换为空间布局规划，将几何规划与视觉合成解耦。然后，我们施加空间感知查询监督，将可学习查询偏向于指令中明确指定的空间关系。最后，我们开发了图像标题空间布局变化，使模型能够进行系统的几何一致的空间变换。跨多个基准的大量实验表明，我们的方法大大提高了文本到图像生成的空间忠实度，同时在图像字幕、视觉基础和 VQA 任务等多模态理解任务上保持强大的性能。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17703v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17703v1",
        "title": "An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays"
        },
        "updated": "2026-01-25T05:32:53Z",
        "updated_parsed": [
            2026,
            1,
            25,
            5,
            32,
            53,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17703v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17703v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T05:32:53Z",
        "published_parsed": [
            2026,
            1,
            25,
            5,
            32,
            53,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Nikhil Kadivar"
            },
            {
                "name": "Guansheng Li"
            },
            {
                "name": "Jianlu Zheng"
            },
            {
                "name": "John M. Higgins"
            },
            {
                "name": "Ming Dao"
            },
            {
                "name": "George Em Karniadakis"
            },
            {
                "name": "Mengjia Xu"
            }
        ],
        "author_detail": {
            "name": "Mengjia Xu"
        },
        "author": "Mengjia Xu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "一种支持人工智能的工具，用于量化微流体分析中重叠的红细胞镰状动力学",
        "abstract_cn": "了解镰状细胞动力学需要准确识别不同生物物理条件下的形态转变，特别是在密集和重叠的细胞群中。在这里，我们提出了一个自动化深度学习框架，该框架集成了人工智能辅助注释、分割、分类和实例计数，以量化延时显微镜数据中不同密度范围的红细胞（RBC）群体。使用 Roboflow 平台对实验图像进行注释，生成用于训练 nnU-Net 分割模型的标记数据集。经过训练的网络能够预测镰状细胞部分的时间演变，而分水岭算法可以解决重叠细胞以提高定量准确性。尽管只需要有限数量的标记数据进行训练，但该框架实现了较高的分割性能，有效解决了与稀缺的手动注释和单元重叠相关的挑战。通过定量跟踪红细胞形态的动态变化，这种方法可以通过密集的细胞悬浮液使实验通量增加一倍以上，捕获药物依赖性镰状行为，并揭示细胞形态进化的独特机械生物学特征。总体而言，这种人工智能驱动的框架建立了一个可扩展且可重复的计算平台，用于研究细胞生物力学和评估微生理系统的治疗效果。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17706v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17706v1",
        "title": "A Computational Approach to Visual Metonymy",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Computational Approach to Visual Metonymy"
        },
        "updated": "2026-01-25T05:36:03Z",
        "updated_parsed": [
            2026,
            1,
            25,
            5,
            36,
            3,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17706v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17706v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET."
        },
        "tags": [
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T05:36:03Z",
        "published_parsed": [
            2026,
            1,
            25,
            5,
            36,
            3,
            6,
            25,
            0
        ],
        "arxiv_comment": "EACL 2026",
        "arxiv_primary_category": {
            "term": "cs.CL"
        },
        "authors": [
            {
                "name": "Saptarshi Ghosh"
            },
            {
                "name": "Linfeng Liu"
            },
            {
                "name": "Tianyu Jiang"
            }
        ],
        "author_detail": {
            "name": "Tianyu Jiang"
        },
        "author": "Tianyu Jiang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "视觉转喻的计算方法",
        "abstract_cn": "图像传达的信息往往比字面上描述的要多：一套工具可以暗示一种职业，一件文化制品可以暗示一种传统。这种间接的视觉参考，称为视觉转喻，邀请观众通过相关线索而不是明确的描述来恢复目标概念。在这项工作中，我们首次提出了视觉转喻的计算研究。我们引入了一种基于符号学理论的新颖管道，它利用大型语言模型和文本到图像模型来生成转喻视觉表示。使用这个框架，我们构建了 ViMET，这是第一个视觉转喻数据集，包含 2000 个多项选择题，用于评估多模态语言模型中的认知推理能力。我们数据集的实验结果揭示了人类表现 (86.9%) 和最先进的视觉语言模型 (65.9%) 之间的显着差距，凸显了机器解释间接视觉参考的能力的局限性。我们的数据集可公开获取：https://github.com/cincynlp/ViMET。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17723v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17723v1",
        "title": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study"
        },
        "updated": "2026-01-25T07:09:20Z",
        "updated_parsed": [
            2026,
            1,
            25,
            7,
            9,
            20,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17723v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17723v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T07:09:20Z",
        "published_parsed": [
            2026,
            1,
            25,
            7,
            9,
            20,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Tayyab Nasir"
            },
            {
                "name": "Daochang Liu"
            },
            {
                "name": "Ajmal Mian"
            }
        ],
        "author_detail": {
            "name": "Ajmal Mian"
        },
        "author": "Ajmal Mian",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "基于隐式神经表示的连续单图像超分辨率：实证研究",
        "abstract_cn": "隐式神经表示（INR）已成为任意尺度图像超分辨率（ASSR）的标准方法。迄今为止，还没有实证研究系统地检验现有方法的有效性，也没有调查不同训练方案（例如缩放法则、目标设计和优化策略）的效果。严格的实证分析不仅对于基准性能和揭示真实收益至关重要，而且对于确定 ASSR 的当前状态、确定饱和限制和突出有前途的方向也至关重要。我们通过比较不同设置下的现有技术并呈现多个图像质量指标的汇总性能结果来填补这一空白。我们提供统一的框架和代码存储库，以促进可重复的比较。此外，我们研究了仔细控制的训练配置对感知图像质量的影响，并检查了一种新的损失函数，该函数在训练过程中保留边缘、纹理和更精细的细节的同时惩罚强度变化。我们总结了以下以前被忽视的关键见解：（1）最近更复杂的 INR 方法仅比早期方法提供了微小的改进。 (2) 模型性能与训练配置密切相关，这是先前工作中忽视的一个因素。 (3) 所提出的损失增强了跨架构的纹理保真度，强调了客观设计对于目标感知增益的作用。 (4) 缩放法则适用于基于 INR 的 ASSR，通过增加模型复杂性和数据多样性来确认可预测的增益。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17740v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17740v1",
        "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields"
        },
        "updated": "2026-01-25T08:18:39Z",
        "updated_parsed": [
            2026,
            1,
            25,
            8,
            18,
            39,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17740v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17740v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T08:18:39Z",
        "published_parsed": [
            2026,
            1,
            25,
            8,
            18,
            39,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Cong Cao"
            },
            {
                "name": "Ren Li"
            },
            {
                "name": "Corentin Dumery"
            },
            {
                "name": "Hao Li"
            }
        ],
        "author_detail": {
            "name": "Hao Li"
        },
        "author": "Hao Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过隐式字段的潜在流匹配来学习缝纫图案",
        "abstract_cn": "缝纫图案定义了服装的结构基础，对于时装设计、制造和物理模拟等应用至关重要。尽管在自动图案生成方面取得了进展，但由于面板几何形状和接缝排列的广泛变化，准确地建模缝纫图案仍然很困难。在这项工作中，我们介绍了一种基于隐式表示的缝纫图案建模方法。我们使用定义其边界的有符号距离场和识别接缝端点的无符号距离场来表示每个面板，并将这些场编码到连续的潜在空间中，以实现可微分的网格划分。潜在流匹配模型学习该表示中面板组合的分布，并且拼接预测模块从提取的边缘片段恢复接缝关系。该公式允许精确建模和生成具有复杂结构的缝纫图案。我们进一步表明，它可用于从图像中估计缝纫图案，相对于现有方法，其准确性更高，并支持图案完成和改装等应用，为数字时装设计提供实用工具。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17756v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17756v1",
        "title": "MV-S2V: Multi-View Subject-Consistent Video Generation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "MV-S2V: Multi-View Subject-Consistent Video Generation"
        },
        "updated": "2026-01-25T09:02:33Z",
        "updated_parsed": [
            2026,
            1,
            25,
            9,
            2,
            33,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17756v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17756v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href=\"https://szy-young.github.io/mv-s2v\">this URL</a>",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href=\"https://szy-young.github.io/mv-s2v\">this URL</a>"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T09:02:33Z",
        "published_parsed": [
            2026,
            1,
            25,
            9,
            2,
            33,
            6,
            25,
            0
        ],
        "arxiv_comment": "13 pages, 9 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ziyang Song"
            },
            {
                "name": "Xinyu Gong"
            },
            {
                "name": "Bangya Liu"
            },
            {
                "name": "Zelin Zhao"
            }
        ],
        "author_detail": {
            "name": "Zelin Zhao"
        },
        "author": "Zelin Zhao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "MV-S2V：多视图主题一致的视频生成",
        "abstract_cn": "现有的主题到视频生成（S2V）方法已经实现了高保真度和主题一致的视频生成，但仍然受限于单视图主题参考。此限制使得 S2V 任务可简化为 S2I + I2V 管道，无法充分发挥视频主题控制的潜力。在这项工作中，我们提出并解决了具有挑战性的多视图 S2V (MV-S2V) 任务，该任务从多个参考视图合成视频以强制 3D 级别的主题一致性。关于训练数据的稀缺性，我们首先开发一个合成数据管理管道来生成高度定制的合成数据，并辅以小规模的真实世界捕获的数据集来促进 MV-S2V 的训练。另一个关键问题在于条件生成中跨主题和跨视图引用之间的潜在混淆。为了克服这个问题，我们进一步引入了时间平移 RoPE (TS-RoPE) 来区分参考调节中的不同主体和同一主体的不同视图。我们的框架实现了卓越的 3D 主题一致性。多视图参考图像和高质量的视觉输出，为主题驱动的视频生成建立了一个新的有意义的方向。我们的项目页面位于<a href=\"https://szy-young.github.io/mv-s2v\">此 URL</a>"
    },
    {
        "id": "http://arxiv.org/abs/2601.17791v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17791v1",
        "title": "Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation"
        },
        "updated": "2026-01-25T11:12:33Z",
        "updated_parsed": [
            2026,
            1,
            25,
            11,
            12,
            33,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17791v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17791v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\\pm$ 0.10, MAPE = 2.22 $\\pm$ 0.56 \\%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\\pm$ 0.10, MAPE = 2.22 $\\pm$ 0.56 \\%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T11:12:33Z",
        "published_parsed": [
            2026,
            1,
            25,
            11,
            12,
            33,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Rabin Dulal"
            },
            {
                "name": "Wenfeng Jia"
            },
            {
                "name": "Lihong Zheng"
            },
            {
                "name": "Jane Quinn"
            }
        ],
        "author_detail": {
            "name": "Jane Quinn"
        },
        "author": "Jane Quinn",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "协议驱动的多视图 3D 重建用于活牛体重估算",
        "abstract_cn": "准确的牛活重估算对于牲畜管理、福利和生产力至关重要。传统方法，例如使用步入式称重系统的手动称重或使用身体状况评分的近似测量，涉及库存的手动处理，并且从库存和经济角度都会影响生产率。为了解决这些问题，本研究研究了一种使用 3D 重建计算牛活重的经济有效的非接触方法。所提出的流程利用多视图 RGB 图像和基于 SAM 3D 的协议引导融合，然后进行集成回归。我们的方法为每只动物生成单个 3D 点云，并将经典集成模型与低数据条件下的深度学习模型进行比较。结果表明，具有多视图协议融合的 SAM 3D 优于其他 3D 生成方法，而经典集成模型为实际农场场景提供了最一致的性能（R$^2$ = 0.69 $\\pm$ 0.10，MAPE = 2.22 $\\pm$ 0.56 \\%），这使得这对于农场实施来说非常实用。这些发现表明，在农场中进行可扩展部署时，提高重建质量比增加模型复杂性更为重要，因为在农场中生成大量 3D 数据具有挑战性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17818v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17818v1",
        "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning"
        },
        "updated": "2026-01-25T12:47:30Z",
        "updated_parsed": [
            2026,
            1,
            25,
            12,
            47,
            30,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17818v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17818v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T12:47:30Z",
        "published_parsed": [
            2026,
            1,
            25,
            12,
            47,
            30,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Wen Luo"
            },
            {
                "name": "Peng Chen"
            },
            {
                "name": "Xiaotao Huang"
            },
            {
                "name": "LiQun Huang"
            }
        ],
        "author_detail": {
            "name": "LiQun Huang"
        },
        "author": "LiQun Huang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ViTCoP：通过视觉和文本语义协作修剪加速大型视觉语言模型",
        "abstract_cn": "大型视觉语言模型 (LVLM) 由于其视觉标记存在大量冗余，因此会产生很高的计算成本。为了有效降低这种成本，研究人员提出了各种视觉标记修剪方法。然而，现有的方法通常是有限的，要么由于视觉编码器中的修剪而过早丢失关键视觉信息，要么由于大型语言模型（LLM）中的修剪而导致所选标记之间的信息冗余。为了应对这些挑战，我们提出了一种视觉和文本语义协作修剪框架（ViTCoP），该框架将视觉编码器中的冗余过滤与LLM中基于其层次特征的逐步共同修剪相结合，以有效地保留关键和信息多样化的视觉标记。同时，为了确保与FlashAttention等加速技术的兼容性，我们引入了K向量的L2范数作为LLM中的标记显着性度量。对各种大型视觉语言模型的大量实验表明，ViTCoP 不仅在图像和视频理解任务上实现了超越现有方法的最先进性能，而且还显着降低了模型推理延迟和 GPU 内存消耗。值得注意的是，在极端剪枝率下，其相对于其他方法的性能优势变得更加明显。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17857v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17857v1",
        "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction"
        },
        "updated": "2026-01-25T14:31:23Z",
        "updated_parsed": [
            2026,
            1,
            25,
            14,
            31,
            23,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17857v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17857v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T14:31:23Z",
        "published_parsed": [
            2026,
            1,
            25,
            14,
            31,
            23,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Lan Yang"
            },
            {
                "name": "Minghan Yang"
            },
            {
                "name": "Ke Li"
            },
            {
                "name": "Honggang Zhang"
            },
            {
                "name": "Kaiyue Pang"
            },
            {
                "name": "Yi-Zhe Song"
            }
        ],
        "author_detail": {
            "name": "Yi-Zhe Song"
        },
        "author": "Yi-Zhe Song",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SynMind：减少基于功能磁共振成像的图像重建中的语义幻觉",
        "abstract_cn": "基于功能磁共振成像的图像重建的最新进展已经实现了卓越的照片级逼真保真度。然而，一个持续存在的局限性仍然存在：虽然重建图像通常看起来自然且整体上与目标刺激相似，但它们经常遭受严重的语义错位——尽管视觉质量很高，但显着的物体经常被替换或产生幻觉。在这项工作中，我们通过重新思考显式语义解释在功能磁共振成像解码中的作用来解决这一限制。我们认为，现有的方法过于依赖纠缠的视觉嵌入，这些视觉嵌入优先考虑低级外观线索（例如纹理和全局要点），而不是显式语义标识。为了克服这个问题，我们将功能磁共振成像信号解析为丰富的句子级语义描述，反映了人类视觉理解的层次和组成性质。我们通过利用扎根的 VLM 生成合成的、类人的、多粒度的文本表示来实现这一目标，这些文本表示可以捕获对象身份和空间组织。在此基础上，我们提出了 SynMind，一个将这些显式语义编码与视觉先验集成起来的框架，以调节预训练的扩散模型。大量实验表明，SynMind 在大多数定量指标上都优于最先进的方法。值得注意的是，通过将语义推理卸载到我们的文本对齐模块，SynMind 超越了基于 SDXL 的竞争方法，同时使用更小的 Stable Diffusion 1.4 和单个消费级 GPU。大规模人类评估进一步证实 SynMind 生成的重建结果与人类视觉感知更加一致。神经可视化分析表明，SynMind 涉及更广泛、语义更相关的大脑区域，减轻了对高级视觉区域的过度依赖。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17862v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17862v1",
        "title": "Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment"
        },
        "updated": "2026-01-25T14:43:33Z",
        "updated_parsed": [
            2026,
            1,
            25,
            14,
            43,
            33,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17862v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17862v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T14:43:33Z",
        "published_parsed": [
            2026,
            1,
            25,
            14,
            43,
            33,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jingsong Xia"
            },
            {
                "name": "Siqi Wang"
            }
        ],
        "author_detail": {
            "name": "Siqi Wang"
        },
        "author": "Siqi Wang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于医学图像分类的量子增强领域泛化：跨中心部署的轻量级方法",
        "abstract_cn": "医学图像人工智能模型通常在单中心或单设备设置中取得强大的性能，但由于领域转移，其有效性在现实世界的跨中心部署中经常恶化，限制了临床普遍性。为了应对这一挑战，我们提出了一种具有量子增强协作学习的轻量级域泛化框架，能够在不依赖真实多中心标记数据的情况下对未见过的目标域进行鲁棒泛化。具体来说，通过三个关键组件构建和优化了基于MobileNetV2的域不变编码器：（1）使用亮度、对比度、锐化和噪声扰动来模拟异构采集条件的多域成像偏移模拟； （2）通过梯度反转进行领域对抗训练，以抑制领域判别特征； (3)轻量级量子特征增强层，应用参数化量子电路进行非线性特征映射和纠缠建模。此外，在推理过程中采用了测试时间适应策略，以进一步减轻分布变化。在模拟多中心医学成像数据集上的实验表明，所提出的方法显着优于未见域的域泛化或量子增强的基线模型，实现了域特定性能方差的减少和 AUC 和灵敏度的提高。这些结果凸显了计算资源受限下量子增强域泛化的临床潜力，并为混合量子-经典医学成像系统提供了可行的范例。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17866v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17866v1",
        "title": "MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance"
        },
        "updated": "2026-01-25T15:00:37Z",
        "updated_parsed": [
            2026,
            1,
            25,
            15,
            0,
            37,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17866v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17866v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T15:00:37Z",
        "published_parsed": [
            2026,
            1,
            25,
            15,
            0,
            37,
            6,
            25,
            0
        ],
        "arxiv_comment": "Project page, https://jaesung-choe.github.io/mv_sam/index.html",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yoonwoo Jeong"
            },
            {
                "name": "Cheng Sun"
            },
            {
                "name": "Yu-Chiang Frank Wang"
            },
            {
                "name": "Minsu Cho"
            },
            {
                "name": "Jaesung Choe"
            }
        ],
        "author_detail": {
            "name": "Jaesung Choe"
        },
        "author": "Jaesung Choe",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "MV-SAM：使用点图引导的多视图即时分割",
        "abstract_cn": "即时分割已成为计算机视觉中的强大范例，使用户能够通过点击、框或文本提示等提示来指导模型解析复杂的场景。最近的进展，以分段任意模型 (SAM) 为代表，已将这种范式扩展到视频和多视图图像。然而，缺乏 3D 意识通常会导致结果不一致，因此需要进行成本高昂的每个场景优化来强制实现 3D 一致性。在这项工作中，我们介绍了 MV-SAM，这是一种多视图分割框架，它使用点图（通过最新的视觉几何模型从未摆出的图像重建的 3D 点）实现 3D 一致性。利用点图的像素点一一对应，MV-SAM 将图像和提示提升到 3D 空间，从而无需显式 3D 网络或带注释的 3D 数据。具体来说，MV-SAM 通过将图像嵌入从其预训练编码器提升为 3D 点嵌入来扩展 SAM，这些点嵌入由变压器使用交叉注意力与 3D 提示嵌入进行解码。此设计将 2D 交互与 3D 几何结构保持一致，使模型能够通过 3D 位置嵌入隐式学习跨视图的一致掩模。在 SA-1B 数据集上进行训练，我们的方法可以很好地跨领域推广，优于 SAM2-Video，并在 NVOS、SPIn-NeRF、ScanNet++、uCo3D 和 DL3DV 基准上实现与每个场景优化基线相当的性能。代码将被发布。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17895v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17895v1",
        "title": "Masked Depth Modeling for Spatial Perception",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Masked Depth Modeling for Spatial Perception"
        },
        "updated": "2026-01-25T16:13:49Z",
        "updated_parsed": [
            2026,
            1,
            25,
            16,
            13,
            49,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17895v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17895v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.RO",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T16:13:49Z",
        "published_parsed": [
            2026,
            1,
            25,
            16,
            13,
            49,
            6,
            25,
            0
        ],
        "arxiv_comment": "Tech report, 19 pages, 15 figures and 4 tables",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Bin Tan"
            },
            {
                "name": "Changjiang Sun"
            },
            {
                "name": "Xiage Qin"
            },
            {
                "name": "Hanat Adai"
            },
            {
                "name": "Zelin Fu"
            },
            {
                "name": "Tianxiang Zhou"
            },
            {
                "name": "Han Zhang"
            },
            {
                "name": "Yinghao Xu"
            },
            {
                "name": "Xing Zhu"
            },
            {
                "name": "Yujun Shen"
            },
            {
                "name": "Nan Xue"
            }
        ],
        "author_detail": {
            "name": "Nan Xue"
        },
        "author": "Nan Xue",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "空间感知的掩模深度建模",
        "abstract_cn": "空间视觉感知是自动驾驶和机器人操作等物理世界应用的基本要求，这是由与 3D 环境交互的需求驱动的。使用 RGB-D 相机捕获像素对齐的度量深度将是最可行的方法，但它通常面临硬件限制和具有挑战性的成像条件带来的障碍，特别是在存在镜面或无纹理表面的情况下。在这项工作中，我们认为深度传感器的不准确性可以被视为“掩盖”信号，其本质上反映了潜在的几何模糊性。基于这一动机，我们提出了 LingBot-Depth，这是一种深度补全模型，它利用视觉上下文通过屏蔽深度建模来细化深度图，并结合用于可扩展训练的自动数据管理管道。令人鼓舞的是，我们的模型在深度精度和像素覆盖范围方面均优于顶级 RGB-D 相机。一系列下游任务的实验结果进一步表明，LingBot-Depth 提供了跨 RGB 和深度模式的对齐潜在表示。我们向空间感知社区发布代码、检查点和 3M RGB 深度对（包括 2M 真实数据和 1M 模拟数据）。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17927v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17927v1",
        "title": "RemEdit: Efficient Diffusion Editing with Riemannian Geometry",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "RemEdit: Efficient Diffusion Editing with Riemannian Geometry"
        },
        "updated": "2026-01-25T17:58:57Z",
        "updated_parsed": [
            2026,
            1,
            25,
            17,
            58,
            57,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17927v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17927v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.MM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T17:58:57Z",
        "published_parsed": [
            2026,
            1,
            25,
            17,
            58,
            57,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "arxiv_journal_ref": "IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2026",
        "authors": [
            {
                "name": "Eashan Adhikarla"
            },
            {
                "name": "Brian D. Davison"
            }
        ],
        "author_detail": {
            "name": "Brian D. Davison"
        },
        "author": "Brian D. Davison",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "RemEdit：利用黎曼几何进行高效扩散编辑",
        "abstract_cn": "可控图像生成是现代生成人工智能成功的基础，但它面临着语义保真度和推理速度之间的关键权衡。基于 RemEdit 扩散的框架通过两项协同创新解决了这种权衡问题。首先，为了编辑保真度，我们将潜在空间作为黎曼流形进行导航。基于曼巴的模块可以有效地学习流形的结构，从而实现直接、准确的测地路径计算，以实现平滑的语义编辑。通过双 SLERP 混合技术和来自视觉语言模型的目标感知提示丰富传递进一步完善了这种控制。其次，为了进一步加速，我们引入了一种新颖的特定于任务的注意力修剪机制。轻量级修剪头学习保留编辑所必需的标记，从而实现有效的优化，而不会出现与内容无关的方法中常见的语义退化。 RemEdit 超越了之前最先进的编辑框架，同时在 50% 修剪的情况下保持实时性能。因此，RemEdit 为实用且强大的图像编辑树立了新的基准。源代码：https://www.github.com/eashanadhikarla/RemEdit。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17934v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17934v1",
        "title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images"
        },
        "updated": "2026-01-25T18:13:48Z",
        "updated_parsed": [
            2026,
            1,
            25,
            18,
            13,
            48,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17934v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17934v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T18:13:48Z",
        "published_parsed": [
            2026,
            1,
            25,
            18,
            13,
            48,
            6,
            25,
            0
        ],
        "arxiv_comment": "Accepted to ISBI 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Vi Vu"
            },
            {
                "name": "Thanh-Huy Nguyen"
            },
            {
                "name": "Tien-Thinh Nguyen"
            },
            {
                "name": "Ba-Thinh Lam"
            },
            {
                "name": "Hoang-Thien Nguyen"
            },
            {
                "name": "Tianyang Wang"
            },
            {
                "name": "Xingjian Li"
            },
            {
                "name": "Min Xu"
            }
        ],
        "author_detail": {
            "name": "Min Xu"
        },
        "author": "Min Xu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "从专家到通才：释放 SAM 在未标记医学图像上的学习潜力",
        "abstract_cn": "分段任意模型 (SAM) 等基础模型显示出很强的泛化性，但由于域转移、标签稀缺以及参数高效微调 (PEFT) 无法利用未标记数据，将它们适应医学图像仍然很困难。虽然像 U-Net 这样的传统模型在半监督医学学习方面表现出色，但它们协助 PEFT SAM 的潜力却在很大程度上被忽视了。我们引入 SC-SAM，这是一个专家通才框架，其中 U-Net 提供基于点的提示和伪标签来指导 SAM 的适应，而 SAM 则充当强大的通才监督者来规范 U-Net。这种相互指导形成了一个双向协同训练循环，允许两个模型有效地利用未标记的数据。在前列腺 MRI 和息肉分割基准中，我们的方法取得了最先进的结果，优于其他现有的半监督 SAM 变体，甚至优于 MedSAM 等医学基础模型，凸显了专家与通才合作对于标签高效的医学图像分割的价值。我们的代码可在 https://github.com/vnlvi2k3/SC-SAM 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17939v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17939v1",
        "title": "DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation"
        },
        "updated": "2026-01-25T18:33:51Z",
        "updated_parsed": [
            2026,
            1,
            25,
            18,
            33,
            51,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17939v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17939v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T18:33:51Z",
        "published_parsed": [
            2026,
            1,
            25,
            18,
            33,
            51,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chengkun Sun"
            },
            {
                "name": "Jinqian Pan"
            },
            {
                "name": "Renjie Liang"
            },
            {
                "name": "Zhengkang Fan"
            },
            {
                "name": "Xin Miao"
            },
            {
                "name": "Jiang Bian"
            },
            {
                "name": "Jie Xu"
            }
        ],
        "author_detail": {
            "name": "Jie Xu"
        },
        "author": "Jie Xu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DTC：用于医学图像分割的可变形转置卷积模块",
        "abstract_cn": "在医学图像分割中，特别是在类似 UNet 的架构中，上采样主要用于将较小的特征图转换为较大的特征图，从而实现编码器和解码器特征之间的特征融合并支持多尺度预测。传统的上采样方法，例如转置卷积和线性插值，在固定位置上操作：转置卷积将内核元素应用于预定的像素或体素位置，而线性插值根据原始特征图中的固定坐标分配值。这些固定位置方法可能无法捕获超出预定义采样位置的结构信息，并可能导致伪影或细节丢失。受可变形卷积的启发，我们提出了一种新颖的上采样方法——可变形转置卷积（DTC），它学习动态坐标（即采样位置）来为 2D 和 3D 医学图像分割任务生成高分辨率特征图。在3D（例如BTCV15）和2D数据集（例如ISIC18、BUSI）上的实验表明，DTC可以有效地集成到现有的医学图像分割模型中，持续提高解码器的特征重建和细节恢复能力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17947v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17947v1",
        "title": "FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos"
        },
        "updated": "2026-01-25T18:57:36Z",
        "updated_parsed": [
            2026,
            1,
            25,
            18,
            57,
            36,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17947v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17947v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.\n  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\\sim 1.5\\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.\n  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\\sim 1.5\\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T18:57:36Z",
        "published_parsed": [
            2026,
            1,
            25,
            18,
            57,
            36,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Bora Yimenicioglu"
            },
            {
                "name": "Vishal Manikanden"
            }
        ],
        "author_detail": {
            "name": "Vishal Manikanden"
        },
        "author": "Vishal Manikanden",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "FlowMorph：微流体视频中无标记单细胞力学的物理一致自我监督",
        "abstract_cn": "红细胞 (RBC) 的机械特性是血液学和全身性疾病的有希望的生物标志物，激发了微流体测定，在每次实验的细胞通量为 10^3$-10^6$ 的情况下探测变形能力。然而，现有的管道依赖于监督分割或手工制作的 kymographs，很少对控制 RBC 形状演化的层流斯托克斯流物理学进行编码。我们引入了 FlowMorph，这是一个物理一致的自监督框架，它从简短的明场微流体视频中为每个跟踪的红细胞学习无标签标量力学代理 $k$。 FlowMorph 通过低维参数轮廓对每个细胞进行建模，通过结合层流平流和曲率正则化弹性松弛的可微分“胶囊流”推进边界点，并仅使用自动导出的轮廓和光流来优化损耗耦合轮廓重叠、细胞内流动一致性、面积守恒、壁约束和时间平滑度。\n  在四个公共 RBC 微流体数据集中，FlowMorph 在提供速度场的物理丰富视频上实现了 0.905 美元的平均轮廓 IoU，并在纯数据驱动的基线上显着改善了区域保护和墙违规。在以 $\\sim 1.5\\times 10^5$ 为中心的序列上，标量 $k$ 单独将坦克行驶与翻转动力学分开，AUC 为 $0.863$。仅使用 200 美元的实时变形细胞计数 (RT-DC) 事件进行校准，单调图 $E=g(k)$ 可以预测表观杨氏模量，在 600 美元的保留细胞上平均绝对误差为 0.118 美元\\,MPa，并在通道几何、光学和帧速率的变化下优雅地降级。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17977v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17977v1",
        "title": "Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors"
        },
        "updated": "2026-01-25T20:16:25Z",
        "updated_parsed": [
            2026,
            1,
            25,
            20,
            16,
            25,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17977v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17977v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T20:16:25Z",
        "published_parsed": [
            2026,
            1,
            25,
            20,
            16,
            25,
            6,
            25,
            0
        ],
        "arxiv_comment": "4 pages; 3 figures; accepted by International Symposium on Biomedical Imaging (ISBI) 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jinchen Gu"
            },
            {
                "name": "Nan Zhao"
            },
            {
                "name": "Lei Qiu"
            },
            {
                "name": "Lu Zhang"
            }
        ],
        "author_detail": {
            "name": "Lu Zhang"
        },
        "author": "Lu Zhang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "领域专家引导的医疗人工智能混合专家：将数据驱动学习与临床先验相结合",
        "abstract_cn": "专家混合 (MoE) 模型以适度的计算成本提高了表征能力，但其在医学等专业领域的有效性受到小数据集的限制。相比之下，临床实践提供了丰富的专业知识，例如医生的注视模式和诊断启发法，而模型无法从有限的数据中可靠地学习。将捕获新颖模式的数据驱动专家与编码积累的临床见解的领域专家指导专家相结合，为稳健且具有临床意义的学习提供了互补的优势。为此，我们提出了领域知识引导混合 MoE (DKGH-MoE)，这是一个即插即用且可解释的模块，它将数据驱动学习与领域专业知识相结合。 DKGH-MoE 集成了数据驱动的 MoE，从原始成像数据中提取新特征，领域专家引导的 MoE 结合了临床先验，特别是临床医生的眼睛注视线索，以强调具有高诊断相关性的区域。通过将领域专家的见解与数据驱动的功能相集成，DKGH-MoE 提高了性能和可解释性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17987v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17987v1",
        "title": "Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization"
        },
        "updated": "2026-01-25T20:31:10Z",
        "updated_parsed": [
            2026,
            1,
            25,
            20,
            31,
            10,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17987v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17987v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification."
        },
        "tags": [
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T20:31:10Z",
        "published_parsed": [
            2026,
            1,
            25,
            20,
            31,
            10,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.LG"
        },
        "arxiv_journal_ref": "IEEE Conference on Artificial Intelligence 2026 (IEEE CAI 2026)",
        "authors": [
            {
                "name": "Ziwei Zheng"
            },
            {
                "name": "Huizhi Liang"
            },
            {
                "name": "Vaclav Snasel"
            },
            {
                "name": "Vito Latora"
            },
            {
                "name": "Panos Pardalos"
            },
            {
                "name": "Giuseppe Nicosia"
            },
            {
                "name": "Varun Ojha"
            }
        ],
        "author_detail": {
            "name": "Varun Ojha"
        },
        "author": "Varun Ojha",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "最小深度学习架构的系统表征：收敛、剪枝和量化的统一分析",
        "abstract_cn": "深度学习网络擅长分类，但识别可靠解决任务的最小架构仍然具有挑战性。我们提出了一种计算方法来系统地探索和分析收敛、剪枝和量化之间的关系。该工作流程首先对大量架构进行结构化设计扫描，然后评估代表性模型的收敛行为、剪枝敏感性和量化鲁棒性。着眼于复杂性不断增加的众所周知的图像分类，以及深度神经网络、卷积神经网络和视觉变换器，我们的初步结果表明，尽管架构多样化，但性能在很大程度上是不变的，并且学习动态始终表现出三种状态：不稳定、学习和过度拟合。我们进一步描述了稳定学习所需的最小可学习参数，揭示了不同的收敛和修剪阶段，并量化了数值精度降低对可训练参数的影响。与直觉一致，结果证实更深层次的架构比浅层架构更能适应剪枝，参数冗余高达 60%，量化对可学习参数较少的模型影响更严重，对更难的图像数据集影响更大。这些发现为在图像分类中的剪枝和低精度约束下选择紧凑、稳定的模型提供了可行的指导。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18008v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18008v1",
        "title": "Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection"
        },
        "updated": "2026-01-25T21:58:07Z",
        "updated_parsed": [
            2026,
            1,
            25,
            21,
            58,
            7,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18008v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18008v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1109/LRA.2026.3654448",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T21:58:07Z",
        "published_parsed": [
            2026,
            1,
            25,
            21,
            58,
            7,
            6,
            25,
            0
        ],
        "arxiv_comment": "This work has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Code available at: https://github.com/akanuasiegbu/stripfusion",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Asiegbu Miracle Kanu-Asiegbu"
            },
            {
                "name": "Nitin Jotwani"
            },
            {
                "name": "Xiaoxiao Du"
            }
        ],
        "author_detail": {
            "name": "Xiaoxiao Du"
        },
        "author": "Xiaoxiao Du",
        "arxiv_doi": "10.1109/LRA.2026.3654448",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Strip-Fusion：用于多光谱行人检测的时空融合",
        "abstract_cn": "行人检测是机器人感知中的一项关键任务。多光谱模式（可见光和热）可以通过提供互补的视觉信息来提高行人检测性能。多光谱行人检测方法仍存在一些差距。首先，现有方法主要关注空间融合，往往忽略时间信息。其次，多光谱基准中的 RGB 和热图像对可能并不总是完美对齐。由于不同的照明条件、遮挡等，行人的检测也具有挑战性。这项工作提出了 Strip-Fusion，这是一种时空融合网络，对于输入图像中的未对准以及不同的照明条件和严重遮挡具有鲁棒性。 Strip-Fusion 管道集成了时间自适应卷积来动态权衡时空特征，使我们的模型能够更好地捕捉行人运动和上下文随时间的变化。一种新颖的 Kullback-Leibler 散度损失旨在减轻可见光输入和热输入之间的模态不平衡，从而在训练期间引导特征对齐到信息更丰富的模态。此外，还开发了一种新颖的后处理算法来减少误报。大量的实验结果表明，我们的方法在 KAIST 和 CVC-14 基准测试中都具有竞争力。我们还观察到，与之前最先进的技术相比，在严重遮挡和错位等具有挑战性的条件下，我们还观察到了显着的改进。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18045v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18045v1",
        "title": "Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation"
        },
        "updated": "2026-01-25T23:51:45Z",
        "updated_parsed": [
            2026,
            1,
            25,
            23,
            51,
            45,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18045v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18045v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T23:51:45Z",
        "published_parsed": [
            2026,
            1,
            25,
            23,
            51,
            45,
            6,
            25,
            0
        ],
        "arxiv_comment": "Accepted by IEEE International Symposium on Biomedical Imaging (ISBI) 2026. 5 pages, 3 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zhuangzhi Gao"
            },
            {
                "name": "Feixiang Zhou"
            },
            {
                "name": "He Zhao"
            },
            {
                "name": "Xiuju Chen"
            },
            {
                "name": "Xiaoxin Li"
            },
            {
                "name": "Qinkai Yu"
            },
            {
                "name": "Yitian Zhao"
            },
            {
                "name": "Alena Shantsila"
            },
            {
                "name": "Gregory Y. H. Lip"
            },
            {
                "name": "Eduard Shantsila"
            },
            {
                "name": "Yalin Zheng"
            }
        ],
        "author_detail": {
            "name": "Yalin Zheng"
        },
        "author": "Yalin Zheng",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "利用余辉图像增强曲线结构分割的鲁棒性和性能",
        "abstract_cn": "分割医学图像中的曲线结构对于分析临床应用中的形态模式至关重要。集成拓扑属性（例如连通性）可以提高分割的准确性和一致性。然而，由于其不可微性和计算成本，提取和嵌入这些属性（尤其是从持久性图 (PD) 中）具有挑战性。现有的方法主要通过手工设计的损失函数来编码拓扑，这在任务中泛化性很差。在本文中，我们提出了 PIs-Regressor，这是一个简单而有效的模块，可以直接从数据中学习持久性图像（PI）——拓扑特征的有限、可微的表示。与在下采样和上采样阶段融合这些功能的 Topology SegNet 一起，我们的框架将拓扑集成到网络架构本身中，而不是辅助损失。与严重依赖手工损失函数的现有方法不同，我们的方法直接将拓扑信息合并到网络结构中，从而实现更稳健的分割。我们的设计非常灵活，可以与其他基于拓扑的方法无缝结合，以进一步增强分割性能。实验结果表明，集成拓扑特征可以增强模型的鲁棒性，有效应对医学成像中的过度曝光和模糊等挑战。我们在三个曲线基准上的方法在像素级精度和拓扑保真度方面展示了最先进的性能。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18049v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18049v1",
        "title": "Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling"
        },
        "updated": "2026-01-26T00:31:08Z",
        "updated_parsed": [
            2026,
            1,
            26,
            0,
            31,
            8,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18049v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18049v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T00:31:08Z",
        "published_parsed": [
            2026,
            1,
            26,
            0,
            31,
            8,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yunfei Qiu"
            },
            {
                "name": "Qiqiong Ma"
            },
            {
                "name": "Tianhua Lv"
            },
            {
                "name": "Li Fang"
            },
            {
                "name": "Shudong Zhou"
            },
            {
                "name": "Wei Yao"
            }
        ],
        "author_detail": {
            "name": "Wei Yao"
        },
        "author": "Wei Yao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "具有边缘感知超像素标签传播和自适应伪标签的半监督高光谱图像分类",
        "abstract_cn": "半监督高光谱图像（HSI）分类在特征提取和分类性能方面取得了重大进展。然而，由于标注成本较高和样本可用性有限，半监督学习仍然面临边界标签扩散和伪标签不稳定等挑战。为了解决这些问题，本文提出了一种新颖的半监督高光谱分类框架，将空间先验信息与动态学习机制相结合。首先，我们设计了边缘感知超像素标签传播（EASLP）模块。通过将边缘强度惩罚与邻域校正策略相结合，它可以减轻超像素分割的标签扩散，同时增强边界区域的分类鲁棒性。其次，我们介绍动态历史融合预测（DHP）方法。通过维护历史预测并用当前结果对其进行动态加权，DHP 可以平滑伪标签波动并提高时间一致性和抗噪声能力。同时，结合置信度和一致性度量，自适应三方样本分类（ATSC）策略实现了简单样本、模糊样本和困难样本的分层利用，从而提高了伪标签质量和学习效率。动态可靠性增强伪标签框架 (DREPL) 由 DHP 和 ATSC 组成，增强了跨时间域和样本域的伪标签稳定性。通过与EASLP的协同操作，实现时空一致性优化。对四个基准数据集的评估证明了其保持卓越分类性能的能力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18099v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18099v1",
        "title": "Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs"
        },
        "updated": "2026-01-26T03:21:26Z",
        "updated_parsed": [
            2026,
            1,
            26,
            3,
            21,
            26,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18099v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18099v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Following the earlier verification for Gaussian model in \\cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\\%$, obtained by applying the extracted defocus filters to less blurred images.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Following the earlier verification for Gaussian model in \\cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\\%$, obtained by applying the extracted defocus filters to less blurred images."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T03:21:26Z",
        "published_parsed": [
            2026,
            1,
            26,
            3,
            21,
            26,
            0,
            26,
            0
        ],
        "arxiv_comment": "9 pages, 14 input images, 3 TikZ images. arXiv admin note: substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Akbar Saadat"
            }
        ],
        "author_detail": {
            "name": "Akbar Saadat"
        },
        "author": "Akbar Saadat",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于估计图像对之间相对高斯模糊核的计算框架",
        "abstract_cn": "继\\cite{ASaa2026}中对高斯模型的早期验证之后，本文引入了模型的零训练前向计算框架以在实时应用中实现它。该框架基于针对高斯核的标准差的应用范围从较清晰的图像离散计算散焦图像的解析表达式并选择最佳匹配。分析表达式在某些图像点处产生多个解决方案，但使用相邻点的相似性度量将其过滤为单个解决方案。该框架的结构旨在处理两个给定图像彼此部分模糊版本的情况。对真实图像的实验评估表明，所提出的框架在估计合成模糊值时实现了低于 1.7\\%$ 的平均绝对误差 (MAE)。此外，实际模糊图像强度与其相应估计值之间的差异保持在 $2\\%$ 以下，这是通过将提取的散焦滤波器应用于不太模糊的图像而获得的。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18100v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18100v1",
        "title": "Spatial-Conditioned Reasoning in Long-Egocentric Videos",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Spatial-Conditioned Reasoning in Long-Egocentric Videos"
        },
        "updated": "2026-01-26T03:21:35Z",
        "updated_parsed": [
            2026,
            1,
            26,
            3,
            21,
            35,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18100v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18100v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T03:21:35Z",
        "published_parsed": [
            2026,
            1,
            26,
            3,
            21,
            35,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "James Tribble"
            },
            {
                "name": "Hao Wang"
            },
            {
                "name": "Si-En Hong"
            },
            {
                "name": "Chaoyi Zhou"
            },
            {
                "name": "Ashish Bastola"
            },
            {
                "name": "Siyu Huang"
            },
            {
                "name": "Abolfazl Razi"
            }
        ],
        "author_detail": {
            "name": "Abolfazl Razi"
        },
        "author": "Abolfazl Razi",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "以自我为中心的长视频中的空间条件推理",
        "abstract_cn": "由于视点漂移和缺乏持续的几何背景，长视距自我中心视频对视觉导航提出了重大挑战。尽管最近的视觉语言模型在图像和短视频推理方面表现良好，但它们在长自我中心序列中的空间推理能力仍然有限。在这项工作中，我们研究了显式空间信号如何影响基于 VLM 的视频理解，而无需修改模型架构或推理程序。我们引入了 Sanpo-D，这是 Google Sanpo 数据集的细粒度重新注释，并对面向导航的空间查询的多个 VLM 进行了基准测试。为了检查输入级归纳偏差，我们进一步将深度图与 RGB 帧融合并评估它们对空间推理的影响。我们的结果揭示了通用精度和空间专业化之间的权衡，表明深度感知和空间接地表示可以提高行人和障碍物检测等安全关键任务的性能。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18118v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18118v1",
        "title": "LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment"
        },
        "updated": "2026-01-26T04:03:50Z",
        "updated_parsed": [
            2026,
            1,
            26,
            4,
            3,
            50,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18118v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18118v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T04:03:50Z",
        "published_parsed": [
            2026,
            1,
            26,
            4,
            3,
            50,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Daeyoung Kim"
            }
        ],
        "author_detail": {
            "name": "Daeyoung Kim"
        },
        "author": "Daeyoung Kim",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "LungCRCT：基于因果表示的肺部 CT 处理用于肺癌治疗",
        "abstract_cn": "由于早期阶段的沉默，肺癌已成为全世界癌症患者死亡的主要原因之一。此外，肺癌的主要症状很难与慢性阻塞性肺病等其他呼吸道疾病症状区分开来，进一步导致患者忽视早期癌症的进展。因此，为了提高肺癌的生存率，持续主动呼吸系统监测的早期发现变得至关重要。低剂量计算机断层扫描（LDCT）胸部扫描是肺癌监测最普遍、最有效的方法之一，随着基于计算机视觉的人工智能模型（例如 EfficientNet 或 ResNet）在图像处理中的快速发展和应用，低剂量计算机断层扫描（LDCT）胸部扫描显着增强了肺癌检测或肿瘤分类任务。然而，尽管转移学习下的先进 CNN 模型或基于 ViT 的模型可以实现高性能的肺癌检测，但由于其在相关性依赖性方面的内在局限性以及由于复杂性导致的可解释性较低，深度学习模型在肺癌治疗分析或因果干预分析模拟中的扩展仍然受到限制。因此，本研究引入了 LungCRCT：一种基于潜在因果表征学习的肺癌分析框架，可检索肺癌进展的物理因果机制中因素的因果表征。通过使用基于先进图自动编码器的因果发现算法以及距离相关性解缠和基于熵的图像重建细化，LungCRCT不仅能够对肺癌治疗进行因果干预分析，而且还可以在恶性肿瘤分类任务中产生稳健且极其轻量的下游模型，AUC得分为93.91%。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18190v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18190v1",
        "title": "Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval"
        },
        "updated": "2026-01-26T06:16:53Z",
        "updated_parsed": [
            2026,
            1,
            26,
            6,
            16,
            53,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18190v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18190v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T06:16:53Z",
        "published_parsed": [
            2026,
            1,
            26,
            6,
            16,
            53,
            0,
            26,
            0
        ],
        "arxiv_comment": "7 pages, 3 figures. Code: https://github.com/Lcrucial1f/MPS-CLIP",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yifan Li"
            },
            {
                "name": "Shiying Wang"
            },
            {
                "name": "Jianqiang Huang"
            }
        ],
        "author_detail": {
            "name": "Jianqiang Huang"
        },
        "author": "Jianqiang Huang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "关键词引导的多视角子图像CLIP遥感图文检索",
        "abstract_cn": "像 CLIP 这样的视觉语言预训练 (VLP) 模型具有显着先进的遥感图像文本检索 (RSITR)。然而，现有的方法主要依赖于粗粒度的全局对齐，这常常忽略了俯视图像中固有的密集的、多尺度的语义。此外，通过全面微调来适应这些重型模型会带来高昂的计算成本，并可能导致灾难性遗忘。为了应对这些挑战，我们提出了 MPS-CLIP，这是一种参数高效的框架，旨在将检索范式从全局匹配转变为关键字引导的细粒度对齐。具体来说，我们利用大型语言模型（LLM）来提取核心语义关键词，指导分段任何模型（SamGeo）生成语义相关的子视角。为了有效地适应冻结的骨干网，我们引入了门控全局注意力（G^2A）适配器，它以最小的开销捕获全局上下文和远程依赖关系。此外，多视角表示（MPR）模块将这些本地线索聚合成强大的多视角嵌入。该框架通过结合多视角对比和加权三元组损失的混合目标进行优化，动态选择最大响应视角来抑制噪声并强制执行精确的语义匹配。 RSICD 和 RSITMD 基准的大量实验表明，MPS-CLIP 实现了最先进的性能，平均召回率 (mR) 分别为 35.18% 和 48.40%，显着优于完全微调基线和最近的竞争方法。代码可在 https://github.com/Lcrucial1f/MPS-CLIP 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18219v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18219v1",
        "title": "Automated HER2 scoring with uncertainty quantification using lensfree holography and deep learning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Automated HER2 scoring with uncertainty quantification using lensfree holography and deep learning"
        },
        "updated": "2026-01-26T07:09:08Z",
        "updated_parsed": [
            2026,
            1,
            26,
            7,
            9,
            8,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18219v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18219v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate assessment of human epidermal growth factor receptor 2 (HER2) expression is critical for breast cancer diagnosis, prognosis, and therapy selection; yet, most existing digital HER2 scoring methods rely on bulky and expensive optical systems. Here, we present a compact and cost-effective lensfree holography platform integrated with deep learning for automated HER2 scoring of immunohistochemically stained breast tissue sections. The system captures lensfree diffraction patterns of stained HER2 tissue sections under RGB laser illumination and acquires complex field information over a sample area of ~1,250 mm^2 at an effective throughput of ~84 mm^2 per minute. To enhance diagnostic reliability, we incorporated an uncertainty quantification strategy based on Bayesian Monte Carlo dropout, which provides autonomous uncertainty estimates for each prediction and supports reliable, robust HER2 scoring, with an overall correction rate of 30.4%. Using a blinded test set of 412 unique tissue samples, our approach achieved a testing accuracy of 84.9% for 4-class (0, 1+, 2+, 3+) HER2 classification and 94.8% for binary (0/1+ vs. 2+/3+) HER2 scoring with uncertainty quantification. Overall, this lensfree holography approach provides a practical pathway toward portable, high-throughput, and cost-effective HER2 scoring, particularly suited for resource-limited settings, where traditional digital pathology infrastructure is unavailable.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate assessment of human epidermal growth factor receptor 2 (HER2) expression is critical for breast cancer diagnosis, prognosis, and therapy selection; yet, most existing digital HER2 scoring methods rely on bulky and expensive optical systems. Here, we present a compact and cost-effective lensfree holography platform integrated with deep learning for automated HER2 scoring of immunohistochemically stained breast tissue sections. The system captures lensfree diffraction patterns of stained HER2 tissue sections under RGB laser illumination and acquires complex field information over a sample area of ~1,250 mm^2 at an effective throughput of ~84 mm^2 per minute. To enhance diagnostic reliability, we incorporated an uncertainty quantification strategy based on Bayesian Monte Carlo dropout, which provides autonomous uncertainty estimates for each prediction and supports reliable, robust HER2 scoring, with an overall correction rate of 30.4%. Using a blinded test set of 412 unique tissue samples, our approach achieved a testing accuracy of 84.9% for 4-class (0, 1+, 2+, 3+) HER2 classification and 94.8% for binary (0/1+ vs. 2+/3+) HER2 scoring with uncertainty quantification. Overall, this lensfree holography approach provides a practical pathway toward portable, high-throughput, and cost-effective HER2 scoring, particularly suited for resource-limited settings, where traditional digital pathology infrastructure is unavailable."
        },
        "tags": [
            {
                "term": "physics.med-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T07:09:08Z",
        "published_parsed": [
            2026,
            1,
            26,
            7,
            9,
            8,
            0,
            26,
            0
        ],
        "arxiv_comment": "23 Pages, 6 Figures, 1 Table",
        "arxiv_primary_category": {
            "term": "physics.med-ph"
        },
        "authors": [
            {
                "name": "Che-Yung Shen"
            },
            {
                "name": "Xilin Yang"
            },
            {
                "name": "Yuzhu Li"
            },
            {
                "name": "Leon Lenk"
            },
            {
                "name": "Aydogan Ozcan"
            }
        ],
        "author_detail": {
            "name": "Aydogan Ozcan"
        },
        "author": "Aydogan Ozcan",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用无透镜全息术和深度学习进行不确定性量化的自动 HER2 评分",
        "abstract_cn": "准确评估人表皮生长因子受体2（HER2）表达对于乳腺癌的诊断、预后和治疗选择至关重要；然而，大多数现有的数字 HER2 评分方法依赖于笨重且昂贵的光学系统。在这里，我们提出了一个紧凑且经济高效的无透镜全息平台，该平台与深度学习集成，用于对免疫组织化学染色的乳腺组织切片进行自动 HER2 评分。该系统在 RGB 激光照明下捕获染色 HER2 组织切片的无透镜衍射图案，并以每分钟约 84 mm^2 的有效吞吐量在约 1,250 mm^2 的样本区域内获取复杂的场信息。为了提高诊断可靠性，我们采用了基于贝叶斯蒙特卡罗 dropout 的不确定性量化策略，该策略为每个预测提供自主不确定性估计，并支持可靠、稳健的 HER2 评分，总体校正率为 30.4%。使用包含 412 个独特组织样本的盲法测试集，我们的方法在 4 级（0、1+、2+、3+）HER2 分类中实现了 84.9% 的测试准确度，在不确定性量化的二元（0/1+ 与 2+/3+）HER2 评分中实现了 94.8% 的测试准确度。总体而言，这种无透镜全息方法为便携式、高通量和经济高效的 HER2 评分提供了一条实用途径，特别适合资源有限、传统数字病理学基础设施不可用的环境。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18228v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18228v1",
        "title": "Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach"
        },
        "updated": "2026-01-26T07:29:50Z",
        "updated_parsed": [
            2026,
            1,
            26,
            7,
            29,
            50,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18228v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18228v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T07:29:50Z",
        "published_parsed": [
            2026,
            1,
            26,
            7,
            29,
            50,
            0,
            26,
            0
        ],
        "arxiv_comment": "6 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sahil Naik"
            },
            {
                "name": "Soham Bagayatkar"
            },
            {
                "name": "Pavankumar Singh"
            }
        ],
        "author_detail": {
            "name": "Pavankumar Singh"
        },
        "author": "Pavankumar Singh",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用基于 EfficientNetB2 的方法在 FER-2013 上进行面部情绪识别",
        "abstract_cn": "正如在 48x48 灰度图像的 FER-2013 数据集中观察到的那样，由于图像质量低、光照变化、姿势变化、背景干扰、类间变化小、众包噪声标签和严重的类不平衡，在现实场景中基于面部图像检测人类情绪是一项艰巨的任务。尽管最近使用 VGG 和 ResNet 等大型 CNN 的方法实现了合理的精度，但它们的计算成本高昂且内存密集，限制了它们在实时应用中的实用性。我们使用基于 EfficientNetB2 的轻量级且高效的面部情绪识别管道来应对这些挑战，并使用两阶段预热和微调策略进行训练。该模型通过 AdamW 优化、解耦权重衰减、标签平滑 (epsilon = 0.06) 来增强，以减少注释噪声，并通过剪裁类权重来减轻类不平衡，以及 dropout、混合精度训练和广泛的实时数据增强。该模型使用分层的 87.5%/12.5% 训练验证分割进行训练，同时保持官方测试集完整，实现了 68.78% 的测试准确率，参数比基于 VGG16 的基线少了近十倍。实验结果，包括每类指标和学习动态，证明了稳定的训练和强大的泛化能力，使得所提出的方法适合实时和基于边缘的应用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18238v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18238v1",
        "title": "TechING: Towards Real World Technical Image Understanding via VLMs",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "TechING: Towards Real World Technical Image Understanding via VLMs"
        },
        "updated": "2026-01-26T07:43:55Z",
        "updated_parsed": [
            2026,
            1,
            26,
            7,
            43,
            55,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18238v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18238v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x."
        },
        "tags": [
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T07:43:55Z",
        "published_parsed": [
            2026,
            1,
            26,
            7,
            43,
            55,
            0,
            26,
            0
        ],
        "arxiv_comment": "Accepted at Findings of EACL 2026, 30 Pages (9 Pages main paper + 4 pages references + 17 pages appendix)",
        "arxiv_primary_category": {
            "term": "cs.CL"
        },
        "authors": [
            {
                "name": "Tafazzul Nadeem"
            },
            {
                "name": "Bhavik Shangari"
            },
            {
                "name": "Manish Rai"
            },
            {
                "name": "Gagan Raj Gupta"
            },
            {
                "name": "Ashutosh Modi"
            }
        ],
        "author_detail": {
            "name": "Ashutosh Modi"
        },
        "author": "Ashutosh Modi",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "TechING：通过 VLM 实现现实世界的技术图像理解",
        "abstract_cn": "技术领域的专业人员在讨论过程中通常会手绘（在白板、纸张等上）技术图表（例如流程图、框图等）；但是，如果他们想稍后编辑这些，则需要从头开始绘制。现代 VLM 在图像理解方面取得了巨大进步，但在理解技术图表方面却遇到了困难。克服这个问题的一种方法是对现实世界的手绘图像进行微调，但实际上不可能生成大量此类图像。在本文中，我们引入了一个大型综合生成的语料库（反映现实世界图像）用于训练 VLM，并随后在较小的手绘图像语料库上评估 VLM（在人类的帮助下）。我们引入了几个新的自监督任务进行训练，并使用各种基线模型进行了广泛的实验，并在这些任务上对合成图像上的 Llama 3.2 11B-instruct 模型进行了微调，以获得 LLama-VL-TUG，这将 Llama 3.2 11B-instruct 的 ROUGE-L 性能显着提高了 2.14 倍，并在所有基线模型中实现了最佳的全面性能。在真实图像上，人工评估表明，我们在 8 种图表类型中的 7 种中实现了所有基线的最小编译错误，并将 Llama 3.2 11B-instruct 的平均 F1 分数提高了 6.97 倍。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18240v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18240v1",
        "title": "V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering"
        },
        "updated": "2026-01-26T07:46:41Z",
        "updated_parsed": [
            2026,
            1,
            26,
            7,
            46,
            41,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18240v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18240v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T07:46:41Z",
        "published_parsed": [
            2026,
            1,
            26,
            7,
            46,
            41,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Mengyuan Jin"
            },
            {
                "name": "Zehui Liao"
            },
            {
                "name": "Yong Xia"
            }
        ],
        "author_detail": {
            "name": "Yong Xia"
        },
        "author": "Yong Xia",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "V-Loop：医学视觉问答中幻觉检测的视觉逻辑循环验证",
        "abstract_cn": "多模态大语言模型（MLLM）在医学视觉问答（VQA）中辅助疾病诊断方面表现出了卓越的能力。然而，他们的输出仍然容易产生幻觉（即与视觉事实相矛盾的反应），在高风险的医疗场景中构成重大风险。最近的内省检测方法，特别是基于不确定性的方法，提供了计算效率，但基本上是间接的，因为它们估计图像-问题对的预测不确定性，而不是验证特定答案的事实正确性。为了解决这一限制，我们提出了视觉逻辑循环验证（V-Loop），这是一种用于医学 VQA 中幻觉检测的免训练且即插即用的框架。 V-Loop 引入了双向推理过程，形成视觉基础的逻辑循环来验证事实的正确性。给定一个输入，MLLM 会生成主要输入对的答案。 V-Loop 从主要 QA 对中提取语义单元，通过以答案单元为条件重新查询问题单元来生成验证问题，并强制视觉注意一致性以确保回答主要问题和验证问题都依赖于相同的图像证据。如果验证答案与预期的语义内容相匹配，则逻辑循环关闭，表明有事实依据；否则，主要答案将被标记为幻觉。对多个医学 VQA 基准和 MLLM 的大量实验表明，V-Loop 始终优于现有的内省方法，保持高效，并在组合使用时进一步增强基于不确定性的方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18242v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18242v1",
        "title": "Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation"
        },
        "updated": "2026-01-26T07:54:53Z",
        "updated_parsed": [
            2026,
            1,
            26,
            7,
            54,
            53,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18242v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18242v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\\times$ faster convergence and 10-100$\\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\\times$ faster convergence and 10-100$\\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.NI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T07:54:53Z",
        "published_parsed": [
            2026,
            1,
            26,
            7,
            54,
            53,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zerui Kang"
            },
            {
                "name": "Yishen Lim"
            },
            {
                "name": "Zhouyou Gu"
            },
            {
                "name": "Seung-Woo Ko"
            },
            {
                "name": "Tony Q. S. Quek"
            },
            {
                "name": "Jihong Park"
            }
        ],
        "author_detail": {
            "name": "Jihong Park"
        },
        "author": "Jihong Park",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "视觉语言模型引导的可微分射线追踪，用于快速准确的多材料射频参数估计",
        "abstract_cn": "准确的射频 (RF) 材料参数对于 6G 系统中的电磁数字孪生至关重要，但基于梯度的逆射线追踪 (RT) 仍然对初始化敏感，并且在测量有限的情况下成本高昂。本文提出了一种视觉语言模型 (VLM) 引导框架，可加速和稳定可微 RT (DRT) 引擎中的多材料参数估计。 VLM 解析场景图像以推断材料类别，并通过 ITU-R 材料表将它们映射到定量先验，从而产生知情的电导率初始化。 VLM 进一步选择信息丰富的发射器/接收器放置，以促进多样化、材料区分的路径。从这些先验开始，DRT 使用测量的接收信号强度执行基于梯度的细化。在 NVIDIA Sionna 上进行的室内场景实验表明，与均匀或随机初始化和随机放置基线相比，收敛速度提高了 2-4$\\times$，最终参数误差降低了 10-100$\\times$，仅用少数接收器即可实现低于 0.1\\% 的平均相对误差。复杂性分析表明每次迭代的时间尺度与材料和测量设置的数量几乎呈线性关系，而 VLM 引导的放置减少了精确恢复所需的测量。 RT 深度和光线计数的消融证实了精度的进一步提高，而无需显着的每次迭代开销。结果表明，VLM 的语义先验有效指导基于物理的优化，以实现快速可靠的射频材料估计。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18250v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18250v1",
        "title": "A multimodal vision foundation model for generalizable knee pathology",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A multimodal vision foundation model for generalizable knee pathology"
        },
        "updated": "2026-01-26T08:14:51Z",
        "updated_parsed": [
            2026,
            1,
            26,
            8,
            14,
            51,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18250v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18250v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T08:14:51Z",
        "published_parsed": [
            2026,
            1,
            26,
            8,
            14,
            51,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Kang Yu"
            },
            {
                "name": "Dingyu Wang"
            },
            {
                "name": "Zimu Yuan"
            },
            {
                "name": "Nan Zhou"
            },
            {
                "name": "Jiajun Liu"
            },
            {
                "name": "Jiaxin Liu"
            },
            {
                "name": "Shanggui Liu"
            },
            {
                "name": "Yaoyan Zheng"
            },
            {
                "name": "Huishu Yuan"
            },
            {
                "name": "Di Huang"
            },
            {
                "name": "Dong Jiang"
            }
        ],
        "author_detail": {
            "name": "Dong Jiang"
        },
        "author": "Dong Jiang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通用膝关节病理学的多模态视觉基础模型",
        "abstract_cn": "肌肉骨骼疾病是全球残疾的主要原因，迫切需要精确解释医学成像。当前骨科领域的人工智能 (AI) 方法主要依赖于特定任务的监督学习范式。这些方法本质上是分散的，需要大量带注释的数据集，并且通常缺乏跨不同模式和临床场景的通用性。该领域基础模型的发展受到大规模、精选和开源肌肉骨骼数据集稀缺的限制。为了应对这些挑战，我们引入了 OrthoFoundation，这是一种针对肌肉骨骼病理学优化的多模态视觉基础模型。我们构建了一个预训练数据集，其中包含来自内部和公共数据库的 120 万张未标记的膝盖 X 射线和 MRI 图像。该模型利用 Dinov3 主干网络，通过自我监督对比学习进行训练，以捕获稳健的放射学表征。 OrthoFoundation 在 14 个下游任务中实现了最先进的 (SOTA) 性能。它在X射线骨关节炎诊断中取得了优异的准确率，在MRI结构损伤检测中排名第一。该模型展示了卓越的标签效率，仅使用 50% 的标签数据即可匹配监督基线。此外，尽管在膝盖图像上进行了预先训练，OrthoFoundation 仍表现出了对臀部、肩部和脚踝的出色的跨解剖学概括。 OrthoFoundation 代表了肌肉骨骼成像通用人工智能的重大进步。通过从大规模多模态数据中学习基本的、与关节无关的放射学语义，它克服了传统模型的局限性，为减少注释负担和提高临床实践中的诊断准确性提供了一个强大的框架。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18260v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18260v1",
        "title": "Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images"
        },
        "updated": "2026-01-26T08:33:11Z",
        "updated_parsed": [
            2026,
            1,
            26,
            8,
            33,
            11,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18260v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18260v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T08:33:11Z",
        "published_parsed": [
            2026,
            1,
            26,
            8,
            33,
            11,
            0,
            26,
            0
        ],
        "arxiv_comment": "preprint",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Eytan Kats"
            },
            {
                "name": "Kai Geissler"
            },
            {
                "name": "Daniel Mensing"
            },
            {
                "name": "Jochen G. Hirsch"
            },
            {
                "name": "Stefan Heldman"
            },
            {
                "name": "Mattias P. Heinrich"
            }
        ],
        "author_detail": {
            "name": "Mattias P. Heinrich"
        },
        "author": "Mattias P. Heinrich",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "深度解剖：从表面深度图像学习内脏器官位置",
        "abstract_cn": "自动患者定位在优化扫描程序和提高患者吞吐量方面发挥着重要作用。利用 RGB-D 相机捕获的深度信息为估计内部器官位置提供了一种有前景的方法，从而实现更准确、更高效的定位。在这项工作中，我们提出了一个基于学习的框架，可以根据身体表面的单个 2D 深度图像直接预测多个内脏器官的 3D 位置和形状。利用全身 MRI 扫描的大规模数据集，我们合成与相应解剖分割配对的深度图像，以训练统一的卷积神经网络架构。我们的方法可以准确定位多种解剖结构，包括骨骼和软组织，而不需要显式的表面重建。实验结果证明了将深度传感器集成到放射学工作流程中的潜力，可以简化扫描程序并通过自动患者定位增强患者体验。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18263v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18263v1",
        "title": "Revisiting Aerial Scene Classification on the AID Benchmark",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Revisiting Aerial Scene Classification on the AID Benchmark"
        },
        "updated": "2026-01-26T08:39:02Z",
        "updated_parsed": [
            2026,
            1,
            26,
            8,
            39,
            2,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18263v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18263v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T08:39:02Z",
        "published_parsed": [
            2026,
            1,
            26,
            8,
            39,
            2,
            0,
            26,
            0
        ],
        "arxiv_comment": "Presented at the IEEE India Geoscience and Remote Sensing Symposium 2025 and accepted for publication in IEEE Xplore",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Subhajeet Das"
            },
            {
                "name": "Susmita Ghosh"
            },
            {
                "name": "Abhiroop Chatterjee"
            }
        ],
        "author_detail": {
            "name": "Abhiroop Chatterjee"
        },
        "author": "Abhiroop Chatterjee",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "重新审视 AID 基准上的空中场景分类",
        "abstract_cn": "航空图像在城市规划和环境保护中发挥着至关重要的作用，因为它们由各种结构组成，代表不同类型的建筑物、森林、山脉和无人居住的土地。由于其异构性，开发强大的场景分类模型仍然是一个挑战。在本研究中，我们对航空图像分类的各种机器学习方法进行了文献综述。我们的调查涵盖了从手工特征（例如 SIFT、LBP）到传统 CNN（例如 VGG、GoogLeNet）以及先进的深度混合网络的一系列方法。在这方面，我们还设计了Aerial-Y-Net，一种具有多尺度特征融合机制的空间注意力增强CNN，它作为基于注意力的模型，帮助我们更好地理解航空图像的复杂性。在 AID 数据集上进行评估，我们的模型达到了 91.72% 的准确率，优于多个基线架构。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18301v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18301v1",
        "title": "Contextual Range-View Projection for 3D LiDAR Point Clouds",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Contextual Range-View Projection for 3D LiDAR Point Clouds"
        },
        "updated": "2026-01-26T09:30:43Z",
        "updated_parsed": [
            2026,
            1,
            26,
            9,
            30,
            43,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18301v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18301v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \\textit{Centerness-Aware Projection (CAP)} and \\textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \\textit{Centerness-Aware Projection (CAP)} and \\textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T09:30:43Z",
        "published_parsed": [
            2026,
            1,
            26,
            9,
            30,
            43,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Seyedali Mousavi"
            },
            {
                "name": "Seyedhamidreza Mousavi"
            },
            {
                "name": "Masoud Daneshtalab"
            }
        ],
        "author_detail": {
            "name": "Masoud Daneshtalab"
        },
        "author": "Masoud Daneshtalab",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "3D LiDAR 点云的上下文距离视图投影",
        "abstract_cn": "范围视图投影提供了一种将 3D LiDAR 点云转换为 2D 范围图像表示的有效方法，从而能够使用 2D 深度学习模型进行有效处理。然而，该投影的一个主要挑战是多对一冲突，其中多个 3D 点被映射到距离图像中的同一像素上，需要一种选择策略。现有的方法通常保留最小深度的点（最接近激光雷达），而忽略语义相关性和对象结构，这会导致重要上下文信息的丢失。在本文中，我们通过合并来自实例中心和类标签的上下文信息来扩展基于深度的选择规则，引入两种机制：\\textit{中心感知投影（CAP）}和\\textit{类加权感知投影（CWAP）}。在 CAP 中，点深度根据距实例中心的距离进行调整，从而优先考虑中心实例点而不是噪声边界和背景点。在 CWAP 中，对象类通过用户定义的权重确定优先级，从而提供投影策略的灵活性。我们对 SemanticKITTI 数据集的评估表明，CAP 在投影过程中保留了更多实例点，与基线相比，实现了高达 3.1\\% mIoU 的改进。此外，CWAP 提高了目标类别的性能，同时对其他类别的性能影响可以忽略不计"
    },
    {
        "id": "http://arxiv.org/abs/2601.18330v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18330v1",
        "title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification"
        },
        "updated": "2026-01-26T10:14:57Z",
        "updated_parsed": [
            2026,
            1,
            26,
            10,
            14,
            57,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18330v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18330v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T10:14:57Z",
        "published_parsed": [
            2026,
            1,
            26,
            10,
            14,
            57,
            0,
            26,
            0
        ],
        "arxiv_comment": "33 Pages, 8 Tables, Figures 16",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Muhammad Ali Shah"
            },
            {
                "name": "Muhammad Mansoor Alam"
            },
            {
                "name": "Saddam Hussain Khan"
            }
        ],
        "author_detail": {
            "name": "Saddam Hussain Khan"
        },
        "arxiv_affiliation": "University of Engineering and Applied Sciences, Swat, Kanju Township, Pakistan",
        "author": "Saddam Hussain Khan",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "具有增强和分层特征空间的肿瘤感知 DenseNet Swin 混合学习，用于大规模脑 MRI 分类",
        "abstract_cn": "本研究提出了一种用于脑肿瘤 MRI 分析的高效 Densely Swin Hybrid (EDSH) 框架，旨在联合捕获细粒度纹理模式和长范围上下文依赖性。引入了两种肿瘤感知实验装置来解决特定类别的诊断挑战。第一个设置采用增强特征空间（BFS），其中独立定制的 DenseNet 和 Swint 分支学习维度对齐、融合和增强的互补局部和全局表示，通过成功学习不规则形状、定义不明确的质量和异质纹理的特征，实现弥漫性神经胶质瘤模式的高灵敏度检测。第二种设置采用具有深度特征提取的分层 DenseNet Swint 架构，具有双残差连接（DFE 和 DR），其中 DenseNet 用作结构化局部特征学习的干 CNN，而 Swin_t 建模全局肿瘤形态，通过学习明确定义的肿块、位置（脑外）和肿瘤增大（硬脑膜尾或向上延伸）的特征，有效抑制脑膜瘤和垂体瘤分类中的假阴性。 DenseNet 在输入级别进行定制，以匹配 MRI 空间特征，利用密集残差连接来保留纹理信息并减轻梯度消失效应。同时，Swint 通过任务对齐补丁嵌入和移动窗口自注意力进行定制，以有效捕获分层全局依赖关系。对大规模 MRI 数据集（涵盖四个肿瘤类别的严格 40,260 个图像）的广泛评估表明，与独立 CNN、Vision Transformers 和混合数据集相比，其具有一致的优越性，在测试未见过的数据集上实现了 98.50 的准确率和召回率。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18336v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18336v1",
        "title": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction"
        },
        "updated": "2026-01-26T10:23:43Z",
        "updated_parsed": [
            2026,
            1,
            26,
            10,
            23,
            43,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18336v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18336v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T10:23:43Z",
        "published_parsed": [
            2026,
            1,
            26,
            10,
            23,
            43,
            0,
            26,
            0
        ],
        "arxiv_comment": "For more details and updates, please visit our project website: https://research.nvidia.com/labs/sil/projects/ppisp/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Isaac Deutsch"
            },
            {
                "name": "Nicolas Moënne-Loccoz"
            },
            {
                "name": "Gavriel State"
            },
            {
                "name": "Zan Gojcic"
            }
        ],
        "author_detail": {
            "name": "Zan Gojcic"
        },
        "author": "Zan Gojcic",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "PPISP：辐射场重建中光度变化的物理合理补偿和控制",
        "abstract_cn": "多视图 3D 重建方法对相机光学特性和图像信号处理 (ISP) 变化引起的光度不一致仍然高度敏感。现有的缓解策略（例如每帧潜在变量或仿射颜色校正）缺乏物理基础，并且很难推广到新颖的视图。我们提出了物理上合理的 ISP (PPISP)​​ 校正模块，该模块通过基于物理和可解释的转换来解开相机固有的和与捕获相关的效果。专用 PPISP 控制器根据输入视图进行训练，预测新视点的 ISP 参数，类似于真实相机中的自动曝光和自动白平衡。这种设计可以在不访问真实图像的情况下对新颖的视图进行现实和公平的评估。 PPISP 在标准基准上实现了 SoTA 性能，同时提供直观的控制并支持元数据的集成（如果可用）。源代码位于：https://github.com/nv-tlabs/ppisp"
    },
    {
        "id": "http://arxiv.org/abs/2601.18346v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18346v1",
        "title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception"
        },
        "updated": "2026-01-26T10:37:20Z",
        "updated_parsed": [
            2026,
            1,
            26,
            10,
            37,
            20,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18346v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18346v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T10:37:20Z",
        "published_parsed": [
            2026,
            1,
            26,
            10,
            37,
            20,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sijing Wu"
            },
            {
                "name": "Yunhao Li"
            },
            {
                "name": "Zicheng Zhang"
            },
            {
                "name": "Qi Jia"
            },
            {
                "name": "Xinyue Li"
            },
            {
                "name": "Huiyu Duan"
            },
            {
                "name": "Xiongkuo Min"
            },
            {
                "name": "Guangtao Zhai"
            }
        ],
        "author_detail": {
            "name": "Guangtao Zhai"
        },
        "author": "Guangtao Zhai",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Q-Bench-Portrait：对肖像图像质量感知的多模态大语言模型进行基准测试",
        "abstract_cn": "多模态大语言模型 (MLLM) 的最新进展在现有的低级视觉基准上展示了令人印象深刻的性能，这些基准主要关注通用图像。然而，它们感知和评估肖像图像的能力（一个以独特的结构和感知特性为特征的领域）在很大程度上仍未得到充分开发。为此，我们推出了 Q-Bench-Portrait，这是第一个专门针对人像图像质量感知而设计的整体基准测试，由 2,765 个图像-问题-答案三元组组成，并具有以下特点：(1) 多样化的人像图像源，包括自然、合成失真、人工智能生成、艺术和计算机图形图像； (2) 综合质量维度，涵盖技术扭曲、AIGC特有扭曲和美观； (3) 全球和本地层面的一系列问题格式，包括单选题、多项选择题、对错题和开放式问题。基于 Q-Bench-Portrait，我们评估了 20 个开源和 5 个闭源 MLLM，结果表明，尽管当前模型在肖像图像感知方面表现出一定的能力，但它们的性能仍然有限且不精确，与人类判断相比存在明显差距。我们希望所提出的基准能够促进进一步研究，以增强通用和特定领域 MLLM 的肖像图像感知能力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18368v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18368v1",
        "title": "OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI"
        },
        "updated": "2026-01-26T11:19:21Z",
        "updated_parsed": [
            2026,
            1,
            26,
            11,
            19,
            21,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18368v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18368v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.\n  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.\n  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.\n  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.\n  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T11:19:21Z",
        "published_parsed": [
            2026,
            1,
            26,
            11,
            19,
            21,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Caterina Fuster-Barceló"
            },
            {
                "name": "Claudia Castrillón"
            },
            {
                "name": "Laura Rodrigo-Muñoz"
            },
            {
                "name": "Victor Manuel Vega-Suárez"
            },
            {
                "name": "Nicolás Pérez-Fernández"
            },
            {
                "name": "Gorka Bastarrika"
            },
            {
                "name": "Arrate Muñoz-Barrutia"
            }
        ],
        "author_detail": {
            "name": "Arrate Muñoz-Barrutia"
        },
        "author": "Arrate Muñoz-Barrutia",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "OREHAS：用于 MRI 中体积内淋巴积水定量的全自动深度学习管道",
        "abstract_cn": "我们推出了 OREHAS（听觉系统体积积水的优化识别和评估），这是第一个通过常规 3D-SPACE-MRC 和 3D-REAL-IR MRI 对内淋巴积水 (EH) 进行体积量化的全自动管道。该系统将切片分类、内耳定位和序列特定分割这三个组件集成到一个工作流程中，直接根据整个 MRI 体积计算每耳内淋巴与前庭体积比率 (ELR)，从而无需手动干预。\n  每个患者仅使用 3 到 6 个带注释的切片进行训练，OREHAS 可以有效地推广到完整的 3D 体积，SPACE-MRC 的 Dice 分数为 0.90，REAL-IR 的 Dice 分数为 0.75。在具有完整手动注释的外部验证队列中，OREHAS 与专家的基本事实密切匹配 (VSI = 74.3%)，并且大大优于临床 syngo.via 软件 (VSI = 42.5%)，该软件往往会高估内淋巴体积。在 19 名测试患者中，OREHAS 的前庭测量结果与 syngo.via 一致，而内淋巴体积更小且更符合生理学现实。\n  这些结果表明，使用有限的监督，可以通过标准 MRI 实现可靠且可重复的 EH 量化。通过将基于深度学习的高效分割与临床一致的体积工作流程相结合，OREHAS 减少了操作员依赖，确保方法的一致性。此外，结果与既定的成像协议兼容。该方法为大规模研究和基于内耳精确体积测量重新校准临床诊断阈值提供了坚​​实的基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18385v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18385v1",
        "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Estimation of geometric transformation matrices using grid-shaped pilot signals"
        },
        "updated": "2026-01-26T11:33:01Z",
        "updated_parsed": [
            2026,
            1,
            26,
            11,
            33,
            1,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18385v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18385v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1561/116.20250067",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T11:33:01Z",
        "published_parsed": [
            2026,
            1,
            26,
            11,
            33,
            1,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "arxiv_journal_ref": "APSIPA Transactions on Signal and Information Processing (2025) 14 (1)",
        "authors": [
            {
                "name": "Rinka Kawano"
            },
            {
                "name": "Masaki Kawamura"
            }
        ],
        "author_detail": {
            "name": "Masaki Kawamura"
        },
        "author": "Masaki Kawamura",
        "arxiv_doi": "10.1561/116.20250067",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用网格状导频信号估计几何变换矩阵",
        "abstract_cn": "数字水印技术对于防止未经授权使用图像至关重要。由于盗版图像通常会因缩放和裁剪等操作而产生几何扭曲，因此准确的同步（检测水印的嵌入位置）对于正确提取至关重要。特别是，裁剪会改变图像的来源，使同步变得困难。然而，现有的方法很少对裁剪具有鲁棒性。为了解决这个问题，我们提出了一种水印方法，该方法使用导频信号估计应用于隐写图像的几何变换，即使在裁剪后也允许同步。图像中嵌入了具有不同水平和垂直值的网格状导频信号。当图像变换时，网格也会变形。通过分析这种失真，可以估计变换矩阵。将氡变换应用于扭曲图像可以估计网格角度和间隔。此外，由于水平和垂直网格线的编码不同，因此可以确定网格方向，从而减少模糊性。为了验证我们的方法，我们通过各向异性缩放、旋转、剪切和裁剪进行了模拟。结果表明，该方法在单一攻击和复合攻击下都能准确估计变换矩阵，且误差较低。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18386v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18386v1",
        "title": "ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks"
        },
        "updated": "2026-01-26T11:36:34Z",
        "updated_parsed": [
            2026,
            1,
            26,
            11,
            36,
            34,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18386v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18386v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk\". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk\". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T11:36:34Z",
        "published_parsed": [
            2026,
            1,
            26,
            11,
            36,
            34,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Gabriel Lee Jun Rong"
            },
            {
                "name": "Christos Korgialas"
            },
            {
                "name": "Dion Jia Xu Ho"
            },
            {
                "name": "Pai Chet Ng"
            },
            {
                "name": "Xiaoxiao Miao"
            },
            {
                "name": "Konstantinos N. Plataniotis"
            }
        ],
        "author_detail": {
            "name": "Konstantinos N. Plataniotis"
        },
        "author": "Konstantinos N. Plataniotis",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ARMOR：用于鲁棒对抗攻击的方法编排和重新参数化的代理推理",
        "abstract_cn": "现有的自动化攻击套件作为具有固定序列的静态整体运行，缺乏策略适应和语义意识。本文介绍了方法编排和重新参数化的代理推理 (ARMOR) 框架来解决这些限制。 ARMOR 通过视觉语言模型 (VLM) 引导的代理协调三个规范的对抗原语：Carlini-Wagner (CW)、基于雅可比的显着图攻击 (JSMA) 和空间转换攻击 (STA)，这些代理通过共享的“混合台”协作生成和合成扰动。大型语言模型 (LLM) 在实时、闭环中自适应地调整和重新参数化并行攻击代理在标准基准测试中，ARMOR 实现了改进的跨架构传输并可靠地愚弄了这两种设置，为盲目标提供混合输出，并使用置信度和 SSIM 分数为白盒目标选择最佳攻击或混合攻击。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18392v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18392v1",
        "title": "Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space"
        },
        "updated": "2026-01-26T11:50:52Z",
        "updated_parsed": [
            2026,
            1,
            26,
            11,
            50,
            52,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18392v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18392v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T11:50:52Z",
        "published_parsed": [
            2026,
            1,
            26,
            11,
            50,
            52,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Moritz Rempe"
            },
            {
                "name": "Lukas T. Rotkopf"
            },
            {
                "name": "Marco Schlimbach"
            },
            {
                "name": "Helmut Becker"
            },
            {
                "name": "Fabian Hörst"
            },
            {
                "name": "Johannes Haubold"
            },
            {
                "name": "Philipp Dammann"
            },
            {
                "name": "Kevin Kröninger"
            },
            {
                "name": "Jens Kleesiek"
            }
        ],
        "author_detail": {
            "name": "Jens Kleesiek"
        },
        "author": "Jens Kleesiek",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "直接从 k 空间进行 MRI 分类的高效复值视觉转换器",
        "abstract_cn": "磁共振成像 (MRI) 中的深度学习应用主要对重建的震级图像进行操作，该过程会丢弃相位信息并需要计算成本高昂的变换。标准神经网络架构依赖于局部操作（卷积或网格补丁），这些操作不适合原始频域（k 空间）数据的全局、非局部性质。在这项工作中，我们提出了一种新颖的复值视觉变换器（kViT），旨在直接对 k 空间数据执行分类。为了弥合当前架构和 MRI 物理之间的几何脱节，我们引入了一种尊重频域频谱能量分布的径向 k 空间修补策略。在 fastMRI 和内部数据集上进行的大量实验表明，我们的方法实现了与最先进的图像域基线（ResNet、EfficientNet、ViT）相媲美的分类性能。至关重要的是，kViT 对高加速因子表现出卓越的鲁棒性，并提供计算效率的范式转变，与标准方法相比，训练期间的 VRAM 消耗减少高达 68 美元\\倍$。这为资源高效、直接来自扫描仪的人工智能分析建立了一条途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18407v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18407v1",
        "title": "Larger than memory image processing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Larger than memory image processing"
        },
        "updated": "2026-01-26T12:02:41Z",
        "updated_parsed": [
            2026,
            1,
            26,
            12,
            2,
            41,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18407v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18407v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T12:02:41Z",
        "published_parsed": [
            2026,
            1,
            26,
            12,
            2,
            41,
            0,
            26,
            0
        ],
        "arxiv_comment": "10 pages",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jon Sporring"
            },
            {
                "name": "David Stansby"
            }
        ],
        "author_detail": {
            "name": "David Stansby"
        },
        "author": "David Stansby",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "大于内存的图像处理",
        "abstract_cn": "该报告针对千万亿级数据集（例如 1.4 PB 电子显微镜体积和 150 TB 人体器官图集）进行大于内存的图像分析。我们认为性能从根本上来说是受 I/O 限制的。我们表明，流式传输数据时的结构化分析至关重要。对于 3D 体积，两种表示形式很流行：2D 切片堆栈（例如目录或多页 TIFF）和 3D 分块布局（例如 Zarr/HDF5）。虽然对于一些算法来说，磁盘上的分块布局对于将磁盘 I/O 保持在最低水平至关重要，但我们展示了如何以最小化磁盘 I/O 的方式在任一图像表示之上构建基于切片的流架构。这对于依赖于相邻值的算法特别有利，因为切片流架构是一维的，这意味着只有 2 个可能的扫描顺序，这两个顺序都与从磁盘读取图像的顺序对齐。这与 3D 块形成对比，在 3D 块中，如果不访问每个块至少 9 次，就无法完成任何扫描。我们通过基于扫描的执行（自然 2D/3D 顺序）、窗口操作和重叠感知平铺将其形式化，以最大限度地减少冗余访问。基于这些原则，我们引入了一种特定领域语言（DSL），该语言使用其最佳流和内存使用的内在知识对算法进行编码； DSL 执行编译时和运行时管道分析，以自动选择窗口大小、熔断阶段、tee 和 zip 流以及有限 RAM 机器的调度通道，从而产生近线性 I/O 扫描和可预测的内存占用。该方法与现有的分割和形态学工具集成，但将预处理/后处理重新构建为优先顺序读/写模式的管道，为极大的图像提供可观的吞吐量增益，而不需要在内存中全卷驻留。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18464v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18464v1",
        "title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System"
        },
        "updated": "2026-01-26T13:12:24Z",
        "updated_parsed": [
            2026,
            1,
            26,
            13,
            12,
            24,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18464v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18464v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T13:12:24Z",
        "published_parsed": [
            2026,
            1,
            26,
            13,
            12,
            24,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Wenbin Wei"
            },
            {
                "name": "Suyuan Yao"
            },
            {
                "name": "Cheng Huang"
            },
            {
                "name": "Xiangyu Gao"
            }
        ],
        "author_detail": {
            "name": "Xiangyu Gao"
        },
        "author": "Xiangyu Gao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "慧眼网：公平、可信、多模态一体化的青光眼全链人工智能系统",
        "abstract_cn": "青光眼是全球不可逆失明的首要原因，因此早期发现和纵向随访对于预防永久性视力丧失至关重要。然而，目前的筛查和进展评估依赖于单一测试或松散联系的检查，引入了主观性和分散的护理。获得高质量成像工具和专业知识的机会有限，进一步损害了实际使用中的一致性和公平性。为了弥补这些差距，我们开发了 Fair-Eye Net，这是一个公平、可靠的多模式人工智能系统，关闭了从青光眼筛查到随访和风险警报的临床循环。它通过双流异构融合架构集成了眼底照片、OCT 结构指标、VF 功能指数和人口统计因素，以及用于选择性预测和安全转诊的不确定性感知分层门控策略。公平约束减少了弱势群体的漏诊。实验结果显示，其 AUC 为 0.912（特异性为 96.7%），种族假阴性差异缩小了 73.4%（12.31% 至 3.28%），保持了稳定的跨域性能，并实现了 3-12 个月的早期风险警报（敏感性为 92%，特异性为 88%）。与事后公平性调整不同，Fair-Eye Net 通过多任务学习以临床可靠性优化公平性作为主要目标，为临床转化和大规模部署提供可重复的路径，以促进全球眼健康公平性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18493v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18493v1",
        "title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment"
        },
        "updated": "2026-01-26T13:48:11Z",
        "updated_parsed": [
            2026,
            1,
            26,
            13,
            48,
            11,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18493v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18493v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.\n  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.\n  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T13:48:11Z",
        "published_parsed": [
            2026,
            1,
            26,
            13,
            48,
            11,
            0,
            26,
            0
        ],
        "arxiv_comment": "Under review at ICPR 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sara Tehrani"
            },
            {
                "name": "Yonghao Xu"
            },
            {
                "name": "Leif Haglund"
            },
            {
                "name": "Amanda Berg"
            },
            {
                "name": "Michael Felsberg"
            }
        ],
        "author_detail": {
            "name": "Michael Felsberg"
        },
        "author": "Michael Felsberg",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DisasterInsight：功能感知和扎根灾害评估的多模式基准",
        "abstract_cn": "卫星图像的及时解释对于灾难响应至关重要，但现有的遥感视觉语言基准主要集中在粗略标签和图像级识别上，忽视了实际人道主义工作流程中所需的功能理解和指令稳健性。我们推出了 DisasterInsight，这是一个多模式基准测试，旨在评估现实灾难分析任务的视觉语言模型 (VLM)。 DisasterInsight 将 xBD 数据集重组为大约 112K 以建筑为中心的实例，并支持跨多个任务的指令多样化评估，包括建筑功能分类、损坏级别和灾难类型分类、计数以及符合人道主义评估指南的结构化报告生成。\n  为了建立域适应基线，我们提出了 DI-Chat，它是通过使用参数高效的低阶适应 (LoRA) 对灾难特定指令数据上的现有 VLM 主干进行微调而获得的。对最先进的通用和遥感 VLM 进行的广泛实验揭示了任务之间的巨大性能差距，特别是在损伤理解和结构化报告生成方面。 DI-Chat 在损害级别和灾害类型分类以及报告生成质量方面取得了显着改进，而建筑功能分类对于所有评估模型来说仍然具有挑战性。 DisasterInsight 为研究灾难图像中的扎根多模态推理提供了统一的基准。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18543v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18543v1",
        "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning"
        },
        "updated": "2026-01-26T14:49:04Z",
        "updated_parsed": [
            2026,
            1,
            26,
            14,
            49,
            4,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18543v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18543v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T14:49:04Z",
        "published_parsed": [
            2026,
            1,
            26,
            14,
            49,
            4,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Kaixun Jiang"
            },
            {
                "name": "Yuzheng Wang"
            },
            {
                "name": "Junjie Zhou"
            },
            {
                "name": "Pandeng Li"
            },
            {
                "name": "Zhihang Liu"
            },
            {
                "name": "Chen-Wei Xie"
            },
            {
                "name": "Zhaoyu Chen"
            },
            {
                "name": "Yun Zheng"
            },
            {
                "name": "Wenqiang Zhang"
            }
        ],
        "author_detail": {
            "name": "Wenqiang Zhang"
        },
        "author": "Wenqiang Zhang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "GenAgent：通过代理多模态推理扩展文本到图像的生成",
        "abstract_cn": "我们引入 GenAgent，通过代理多模态模型统一视觉理解和生成。与面临昂贵的训练成本和理解生成权衡的统一模型不同，GenAgent 通过代理框架将这些功能解耦：理解由多模态模型本身处理，而生成是通过将图像生成模型视为可调用工具来实现的。至关重要的是，与受静态管道约束的现有模块化系统不同，这种设计可以实现自主多轮交互，其中代理生成包含推理、工具调用、判断和反射的多模式思想链，以迭代地完善输出。我们采用两阶段训练策略：首先，冷启动，对高质量工具调用和反射数据进行监督微调，以引导代理行为；其次，端到端代理强化学习结合了点式奖励（最终图像质量）和成对奖励（反射准确性），并通过轨迹重采样来增强多轮探索。 GenAgent 显着提高了 GenEval++ (+23.6\\%) 和 WISE (+14\\%) 上的基本生成器 (FLUX.1-dev) 性能。除了性能提升之外，我们的框架还展示了三个关键属性：1）对具有不同功能的生成器的跨工具泛化，2）测试时间扩展，在交互轮次中具有一致的改进，以及3）自动调整以适应不同任务的任务自适应推理。我们的代码可以在 \\href{https://github.com/deep-kaixun/GenAgent}{this url} 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18547v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18547v1",
        "title": "REMAC: Reference-Based Martian Asymmetrical Image Compression",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "REMAC: Reference-Based Martian Asymmetrical Image Compression"
        },
        "updated": "2026-01-26T14:55:17Z",
        "updated_parsed": [
            2026,
            1,
            26,
            14,
            55,
            17,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18547v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18547v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1109/TGRS.2025.3649222",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \\textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \\textit{intra-} and \\textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \\textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \\textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \\textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \\textit{intra-} and \\textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \\textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \\textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.MM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T14:55:17Z",
        "published_parsed": [
            2026,
            1,
            26,
            14,
            55,
            17,
            0,
            26,
            0
        ],
        "arxiv_comment": "Accepted for publication in IEEE Transactions on Geoscience and Remote Sensing (TGRS). 2025 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. 18 pages, 20 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "arxiv_journal_ref": "Year: 2025, Volume: 64, Article Sequence Number: 5601018",
        "authors": [
            {
                "name": "Qing Ding"
            },
            {
                "name": "Mai Xu"
            },
            {
                "name": "Shengxi Li"
            },
            {
                "name": "Xin Deng"
            },
            {
                "name": "Xin Zou"
            }
        ],
        "author_detail": {
            "name": "Xin Zou"
        },
        "author": "Xin Zou",
        "arxiv_doi": "10.1109/TGRS.2025.3649222",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "REMAC：基于参考的火星非对称图像压缩",
        "abstract_cn": "为了加快火星太空探索，必须开发一种有效的火星图像压缩方法，通过受限的火星地地通信通道传输图像。尽管现有的学习压缩方法在处理来自地球的自然图像方面取得了可喜的结果，但仍然存在两个阻碍其在火星图像压缩方面的有效性的关键问题：1）它们忽视了火星上高度有限的计算资源； 2）他们没有利用火星图像之间强大的\\textit{图像间}相似性来提高图像压缩性能。受我们从纹理、颜色和语义的角度对 \\textit{intra-} 和 \\textit{inter-image} 相似性进行实证分析的启发，我们提出了一种基于参考的火星非对称图像压缩（REMAC）方法，该方法将计算复杂性从编码器转移到资源丰富的解码器，同时提高了压缩性能。为了利用 \\textit{图像间} 相似性，我们提出了一个参考引导熵模块和一个参考解码器，它利用参考图像中的有用信息，减少编码器的冗余操作并实现卓越的压缩性能。为了利用 \\textit{intra-image} 相似性，ref-decoder 采用了一种具有扩大的感受野大小的深度、多尺度架构来模拟远程空间依赖性。此外，我们开发了一种潜在特征回收机制，以进一步缓解火星上的极端计算限制。实验结果表明，与最先进的方法相比，REMAC 将编码器复杂度降低了 43.51%，同时实现了 0.2664 dB 的 BD-PSNR 增益。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18556v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18556v1",
        "title": "Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis"
        },
        "updated": "2026-01-26T15:05:19Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            5,
            19,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18556v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18556v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:05:19Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            5,
            19,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jingsong Xia"
            },
            {
                "name": "Siqi Wang"
            }
        ],
        "author_detail": {
            "name": "Siqi Wang"
        },
        "author": "Siqi Wang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于医学图像诊断的量子增强辨别生成扩散增强",
        "abstract_cn": "在生物医学工程中，人工智能已成为增强医疗诊断的关键工具，特别是在医学图像分类任务中，例如通过胸部 X 光检测肺炎和乳腺癌筛查。然而，现实世界的医学数据集经常表现出严重的类别不平衡，其中正样本大大超过负样本，导致少数类别的召回率较低的有偏差模型。这种不平衡不仅会影响诊断准确性，还会带来临床误诊风险。为了应对这一挑战，我们提出了 SDA-QEC（具有量子增强分类的简化扩散增强），这是一种创新框架，它将基于简化扩散的数据增强与量子增强特征辨别相结合。我们的方法采用轻量级扩散增强器为少数类别生成高质量的合成样本，重新平衡训练分布。随后，MobileNetV2 架构中嵌入的量子特征层通过希尔伯特空间中的高维特征映射增强了模型的判别能力。冠状动脉造影图像分类的综合实验表明，SDA-QEC 的准确率达到 98.33%，AUC 达到 98.78%，F1 分数达到 98.33%，显着优于 ResNet18、MobileNetV2、DenseNet121 和 VGG16 等经典基线。值得注意的是，我们的框架同时达到了 98.33% 的敏感性和 98.33% 的特异性，实现了对临床部署至关重要的平衡性能。该方法验证了在现实世界的医学成像任务中将生成增强与量子增强建模相结合的可行性，为在小样本、高度不平衡和高风险的诊断场景中开发高可靠的医疗人工智能系统提供了一种新的研究途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18560v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18560v1",
        "title": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging"
        },
        "updated": "2026-01-26T15:07:31Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            7,
            31,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18560v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18560v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:07:31Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            7,
            31,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Li Fang"
            },
            {
                "name": "Tianyu Li"
            },
            {
                "name": "Yanghong Lin"
            },
            {
                "name": "Shudong Zhou"
            },
            {
                "name": "Wei Yao"
            }
        ],
        "author_detail": {
            "name": "Wei Yao"
        },
        "author": "Wei Yao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "支持人工智能的卫星边缘计算：基于单像素特征的高光谱成像浅层分类模型",
        "abstract_cn": "高光谱成像卫星作为对地观测系统的重要组成部分，凭借强大的光谱测量能力，为相关政策的制定提供高保真、丰富的信息。然而，在灾害监测、应急测绘等需要快速响应能力的应用中，卫星下行链路的传输速度已成为主要瓶颈。我们提出了一种高效的人工智能卫星边缘计算范例，用于高光谱图像分类，促进卫星实现自主决策。为了适应卫星平台的资源限制，该方法采用了轻量级的非深度学习框架，并结合了少量学习策略。此外，卫星上的机载处理可能会面临传感器故障和扫描模式错误，从而导致图像质量下降、像素不良/未对齐和混合噪声。为了解决这些挑战，我们开发了一种新颖的两阶段像素级标签传播方案，该方案仅利用单像素级别的固有光谱特征，而无需考虑深度神经网络所要求的空间结构信息。在第一阶段，通过构建的锚像素亲和力矩阵传播选定的锚标签来获得初始像素标签。随后，通过直接计算像素级相似度来生成 top-k 剪枝稀疏图。在第二阶段，采用从稀疏图导出的封闭式解来代替迭代计算。此外，我们开发了一种基于排序约束的图聚类算法来确定锚标签。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18585v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18585v1",
        "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization"
        },
        "updated": "2026-01-26T15:32:16Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            32,
            16,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18585v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18585v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:32:16Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            32,
            16,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chenxi Liu"
            },
            {
                "name": "Selena Ling"
            },
            {
                "name": "Alec Jacobson"
            }
        ],
        "author_detail": {
            "name": "Alec Jacobson"
        },
        "author": "Alec Jacobson",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "GimmBO：通过贝叶斯优化的交互式生成图像模型合并",
        "abstract_cn": "基于微调的适应广泛用于定制基于扩散的图像生成，从而产生了大量社区创建的适配器，可以捕捉不同的主题和风格。来自同一基本模型的适配器可以与权重合并，从而能够在广阔且连续的设计空间中合成新的视觉结果。为了探索这个空间，当前的工作流程依赖于基于滑块的手动调整，这种方法扩展性很差，并且使得权重选择变得困难，即使候选集仅限于 20-30 个适配器也是如此。我们建议 GimmBO 通过优先贝叶斯优化（PBO）支持适配器合并的交互式探索，以生成图像。受现实世界使用观察（包括稀疏性和约束权重范围）的启发，我们引入了两阶段 BO 后端，可以提高高维空间中的采样效率和收敛性。我们通过模拟用户和用户研究来评估我们的方法，证明了改进的收敛性、高成功率以及相对于 BO 和线搜索基线的一致增益，并通过多个扩展进一步展示了该框架的灵活性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18589v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18589v1",
        "title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment"
        },
        "updated": "2026-01-26T15:35:03Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            35,
            3,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18589v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18589v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.MM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:35:03Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            35,
            3,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "KV Karthikeya"
            },
            {
                "name": "Ashok Kumar Das"
            },
            {
                "name": "Shantanu Pal"
            },
            {
                "name": "Vivekananda Bhat K"
            },
            {
                "name": "Arun Sekar Rajasekaran"
            }
        ],
        "author_detail": {
            "name": "Arun Sekar Rajasekaran"
        },
        "author": "Arun Sekar Rajasekaran",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "AGSP-DSA：具有动态语义对齐的鲁棒多模态融合的自适应图形信号处理框架",
        "abstract_cn": "在本文中，我们介绍了一种具有动态语义对齐的自适应图信号处理（AGSP DSA）框架，用于对文本、音频和图像等异构源执行鲁棒的多模态数据融合。所请求的方法使用双图构造来学习模内和模间关系，使用谱图过滤来增强信息信号，以及使用多尺度图卷积网络（GCN）进行有效的节点嵌入。语义感知注意机制：每种模态都可以动态地对上下文相关性做出贡献。三个基准数据集（包括 CMU-MOSEI、AVE 和 MM-IMDB）上的实验结果表明 AGSP-DSA 的性能达到了最先进的水平。更准确地说，它在 CMU-MOSEI 上实现了 95.3% 的准确率、0.936 F1 分数和 0.924 mAP，将 MM-GNN 的准确率提高了 2.6%。它在 AVE 上获得了 93.4% 的准确率和 0.911 F1 分数，在 MM-IMDB 上获得了 91.8% 的准确率和 0.886 F1 分数，这在缺失模态设置中表现出了良好的泛化性和鲁棒性。这些发现验证了 AGSP-DSA 在促进情感分析、事件识别和多媒体分类等多模态学习方面的效率。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18619v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18619v1",
        "title": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures"
        },
        "updated": "2026-01-26T15:58:04Z",
        "updated_parsed": [
            2026,
            1,
            26,
            15,
            58,
            4,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18619v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18619v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T15:58:04Z",
        "published_parsed": [
            2026,
            1,
            26,
            15,
            58,
            4,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jorge Quesada"
            },
            {
                "name": "Ghassan AlRegib"
            }
        ],
        "author_detail": {
            "name": "Ghassan AlRegib"
        },
        "author": "Ghassan AlRegib",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于分割小型稀疏结构的尺度感知自监督学习",
        "abstract_cn": "自监督学习（SSL）已成为有限注释机制下表示学习的强大策略，但其有效性对许多因素仍然高度敏感，尤其是目标任务的性质。在分割中，现有的管道通常调整为大的、均匀的区域，但当对象小、稀疏或局部不规则时，它们的性能会下降。在这项工作中，我们提出了一种尺度感知 SSL 适配，它将小窗口裁剪集成到增强管道中，在预训练期间放大精细尺度结构。我们在两个具有明显不同数据模式的领域评估这种方法：地震成像，其目标是分割稀疏断层，以及神经成像，其任务是描绘小细胞结构。在这两种设置中，我们的方法在标签约束下比标准和最先进的基线产生了一致的改进，将故障分割的准确性提高了 13%，将细胞描绘的准确性提高了 5%。相比之下，地震相或组织区域等大尺度特征几乎没有什么好处，这强调了 SSL 的价值关键取决于目标物体的规模。我们的研究结果强调了将 SSL 设计与对象大小和稀疏性保持一致的必要性，为跨科学成像领域构建更有效的表示学习管道提供了一般原则。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18623v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18623v1",
        "title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation"
        },
        "updated": "2026-01-26T16:00:36Z",
        "updated_parsed": [
            2026,
            1,
            26,
            16,
            0,
            36,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18623v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18623v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T16:00:36Z",
        "published_parsed": [
            2026,
            1,
            26,
            16,
            0,
            36,
            0,
            26,
            0
        ],
        "arxiv_comment": "Paper accepted as a conference paper at ICLR 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zihao Wang"
            },
            {
                "name": "Yuzhou Chen"
            },
            {
                "name": "Shaogang Ren"
            }
        ],
        "author_detail": {
            "name": "Shaogang Ren"
        },
        "author": "Shaogang Ren",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "跨模态图像翻译扩散模型中的自适应域转移",
        "abstract_cn": "跨模式图像翻译仍然脆弱且低效。标准扩散方法通常依赖于域之间的单一全局线性传输。我们发现这种捷径迫使采样器遍历非流形的高成本区域，增加了校正负担并引发语义漂移。我们将这种共享故障模式称为固定计划域传输。在本文中，我们将域转移动力学直接嵌入到生成过程中。我们的模型预测每个反向步骤中空间变化的混合场，并将一个明确的、目标一致的恢复项注入到漂移中。这种逐步指导使流形上保持大量更新，并将模型的角色从全局对齐转变为局部残差校正。我们提供了具有精确解形式的连续时间公式，并导出了保持边际一致性的实用一阶采样器。根据经验，在医学成像、遥感和电致发光语义映射的翻译任务中，我们的框架提高了结构保真度和语义一致性，同时以更少的去噪步骤收敛。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18625v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18625v1",
        "title": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search"
        },
        "updated": "2026-01-26T16:01:33Z",
        "updated_parsed": [
            2026,
            1,
            26,
            16,
            1,
            33,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18625v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18625v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T16:01:33Z",
        "published_parsed": [
            2026,
            1,
            26,
            16,
            1,
            33,
            0,
            26,
            0
        ],
        "arxiv_comment": "Accepted by ICASSP 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zequn Xie"
            }
        ],
        "author_detail": {
            "name": "Zequn Xie"
        },
        "author": "Zequn Xie",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "CONQUER：具有基于文本人员搜索的查询增强功能的上下文感知表示",
        "abstract_cn": "基于文本的人物搜索（TBPS）旨在使用自然语言描述从大型画廊中检索行人图像。这项对于公共安全应用至关重要的任务受到跨模式差异和模糊用户查询的阻碍。我们引入了 CONQUER，这是一个两阶段框架，旨在通过增强训练期间的跨模式对齐和在推理时自适应地细化查询来应对这些挑战。在训练过程中，CONQUER 采用多粒度编码、互补对挖掘和基于最佳传输的上下文引导最佳匹配来学习稳健的嵌入。在推理时，即插即用查询增强模块通过锚点选择和属性驱动的丰富来细化模糊或不完整的查询，而不需要重新训练主干网。在 CUHK-PEDES、ICFG-PEDES 和 RSTPReid 上进行的大量实验表明，CONQUER 在 Rank-1 准确率和 mAP 方面始终优于强大的基线，在跨域和不完整查询场景中产生了显着的改进。这些结果凸显了 CONQUER 作为现实世界 TBPS 部署的实用且有效的解决方案。源代码可在 https://github.com/zqxie77/CONQUER 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18633v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18633v1",
        "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting"
        },
        "updated": "2026-01-26T16:06:57Z",
        "updated_parsed": [
            2026,
            1,
            26,
            16,
            6,
            57,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18633v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18633v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T16:06:57Z",
        "published_parsed": [
            2026,
            1,
            26,
            16,
            6,
            57,
            0,
            26,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Tong Shi"
            },
            {
                "name": "Melonie de Almeida"
            },
            {
                "name": "Daniela Ivanova"
            },
            {
                "name": "Nicolas Pugeault"
            },
            {
                "name": "Paul Henderson"
            }
        ],
        "author_detail": {
            "name": "Paul Henderson"
        },
        "author": "Paul Henderson",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Splat-Portrait：用高斯泼溅概括说话的头像",
        "abstract_cn": "Talking Head Generation 旨在从语音和单个肖像图像合成看起来自然的谈话视频。以前的 3D 头部说话生成方法依赖于特定领域的启发法，例如在动画说话动作之前基于扭曲的面部运动表示，但仍然会产生不准确的 3D 头像重建，从而破坏了生成动画的真实感。我们引入了 Splat-Portrait，这是一种基于高斯分布的方法，可解决 3D 头部重建和嘴唇运动合成的挑战。我们的方法自动学习将单个肖像图像分解为静态 3D 重建（表示为静态高斯分布）和预测的整个图像 2D 背景。然后，它会根据输入音频生成自然的嘴唇运动，而无需任何运动驱动的先验。训练纯粹由 2D 重建和分数蒸馏损失驱动，没有 3D 监督或地标。实验结果表明，Splat-Portrait 在头像生成和新颖的视图合成方面表现出优越的性能，与之前的作品相比，实现了更好的视觉质量。我们的项目代码和补充文档可在 https://github.com/stonewalking/Splat-portrait 上公开获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.18739v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.18739v1",
        "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification"
        },
        "updated": "2026-01-26T18:01:46Z",
        "updated_parsed": [
            2026,
            1,
            26,
            18,
            1,
            46,
            0,
            26,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.18739v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.18739v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-26T18:01:46Z",
        "published_parsed": [
            2026,
            1,
            26,
            18,
            1,
            46,
            0,
            26,
            0
        ],
        "arxiv_comment": "28 pages",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ignacio Antequera-Sánchez"
            },
            {
                "name": "Juan Luis Suárez-Díaz"
            },
            {
                "name": "Rosana Montes"
            },
            {
                "name": "Francisco Herrera"
            }
        ],
        "author_detail": {
            "name": "Francisco Herrera"
        },
        "author": "Francisco Herrera",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "SeNeDiF-OOD：开放世界分类中分布外检测方法的语义嵌套二分法融合。纪念碑风格分类案例研究",
        "abstract_cn": "分布外 (OOD) 检测是在开放世界环境中可靠部署人工智能应用程序的基本要求。然而，解决 OOD 数据的异构性质（从低级损坏到语义转换）仍然是单级检测器通常无法解决的复杂挑战。为了解决这个问题，我们提出了 SeNeDiF-OOD，一种基于语义嵌套二分法融合的新颖方法。该框架将检测任务分解为二进制融合节点的层次结构，其中每一层都旨在集成与特定语义抽象级别对齐的决策边界。为了验证所提出的框架，我们使用 MonuMAI 进行了全面的案例研究，MonuMAI 是一个暴露在开放环境中的真实建筑风格识别系统。该应用程序面临各种输入，包括非纪念碑图像、未知的建筑风格和对抗性攻击，使其成为我们提案的理想测试平台。通过该领域的广泛实验评估，结果表明我们的分层融合方法显着优于传统基线，有效过滤这些不同的 OOD 类别，同时保持分布内性能。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17689v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17689v1",
        "title": "REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization"
        },
        "updated": "2026-01-25T04:28:21Z",
        "updated_parsed": [
            2026,
            1,
            25,
            4,
            28,
            21,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17689v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17689v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data."
        },
        "tags": [
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T04:28:21Z",
        "published_parsed": [
            2026,
            1,
            25,
            4,
            28,
            21,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.LG"
        },
        "authors": [
            {
                "name": "Shanu Saklani"
            },
            {
                "name": "Tushar M. Athawale"
            },
            {
                "name": "Nairita Pal"
            },
            {
                "name": "David Pugmire"
            },
            {
                "name": "Christopher R. Johnson"
            },
            {
                "name": "Soumya Dutta"
            }
        ],
        "author_detail": {
            "name": "Soumya Dutta"
        },
        "author": "Soumya Dutta",
        "journal": "arXiv: Nerf",
        "title_cn": "REV-INR：用于不确定性感知体积可视化的正则化证据隐式神经表示",
        "abstract_cn": "隐式神经表示（INR）的应用已成为一种有前途的深度学习方法，用于紧凑地表示大型体积数据集。这些模型可以充当体数据的替代品，通过模型预测实现高效存储和按需重建。然而，传统的确定性 INR 仅提供价值预测，无法深入了解模型的预测不确定性或数据中固有噪声的影响。由于重建体积中的预测不准确，这种限制可能导致不可靠的数据解释和可视化。识别从模型预测数据中提取的错误结果可能是不可行的，因为原始数据可能由于其规模较大而无法获得。为了应对这一挑战，我们引入了 REV-INR（正则化证据隐式神经表示），它在推理过程中仅使用经过训练的 REV-INR 的单次前向传递来学习准确预测数据值以及相关的坐标级数据不确定性和模型不确定性。通过对 REV-INR 与现有完善的深度不确定性估计方法进行全面比较和对比，我们表明 REV-INR 使用最快的推理时间，通过稳健的数据（任意）和模型（认知）不确定性估计实现了最佳体积重建质量。因此，我们证明 REV-INR 有助于评估提取的等值面和体积可视化结果的可靠性和可信度，使分析能够仅由模型预测数据驱动。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17693v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17693v1",
        "title": "DDFKs: Fluid Simulation with Dynamic Divergence-Free Kernels",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DDFKs: Fluid Simulation with Dynamic Divergence-Free Kernels"
        },
        "updated": "2026-01-25T04:48:24Z",
        "updated_parsed": [
            2026,
            1,
            25,
            4,
            48,
            24,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17693v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17693v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Fluid simulations based on memory-efficient spatial representations like implicit neural spatial representations (INSRs) and Gaussian spatial representation (GSR), where the velocity fields are parameterized by neural networks or weighted Gaussian functions, has been an emerging research area. Though advantages over traditional discretizations like spatial adaptivity and continuous differentiability of these spatial representations are leveraged by fluid solvers, solving the time-dependent PDEs that governs the fluid dynamics remain challenging, especially in incompressible fluids where the divergence-free constraint is enforced. In this paper, we propose a grid-free solver Dynamic Divergence-Free Kernels (DDFKs) for incompressible flows based on divergence-free kernels (DFKs). Each DFK is incorporated with a matrix-valued radial basis function and a vector-valued weight, yielding a divergence-free vector field. We model the continuous flow velocity as the sum of multiple DFKs, thus enforcing incompressibility while being able to preserve different level of details. Quantitative and qualitative results show that our method achieves comparable accuracy, robustness, ability to preserve vortices, time and memory efficiency and generality across diverse phenomena to state-of-the-art methods using memory-efficient spatial representations, while excels at maintaining incompressibility. Though our first-order solver are slower than fluid solvers with traditional discretizations, our approach exhibits significantly lower numerical dissipation due to reduced discretization error. We demonstrate our method on diverse incompressible flow examples with rich vortices and various solid boundary conditions.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fluid simulations based on memory-efficient spatial representations like implicit neural spatial representations (INSRs) and Gaussian spatial representation (GSR), where the velocity fields are parameterized by neural networks or weighted Gaussian functions, has been an emerging research area. Though advantages over traditional discretizations like spatial adaptivity and continuous differentiability of these spatial representations are leveraged by fluid solvers, solving the time-dependent PDEs that governs the fluid dynamics remain challenging, especially in incompressible fluids where the divergence-free constraint is enforced. In this paper, we propose a grid-free solver Dynamic Divergence-Free Kernels (DDFKs) for incompressible flows based on divergence-free kernels (DFKs). Each DFK is incorporated with a matrix-valued radial basis function and a vector-valued weight, yielding a divergence-free vector field. We model the continuous flow velocity as the sum of multiple DFKs, thus enforcing incompressibility while being able to preserve different level of details. Quantitative and qualitative results show that our method achieves comparable accuracy, robustness, ability to preserve vortices, time and memory efficiency and generality across diverse phenomena to state-of-the-art methods using memory-efficient spatial representations, while excels at maintaining incompressibility. Though our first-order solver are slower than fluid solvers with traditional discretizations, our approach exhibits significantly lower numerical dissipation due to reduced discretization error. We demonstrate our method on diverse incompressible flow examples with rich vortices and various solid boundary conditions."
        },
        "tags": [
            {
                "term": "physics.flu-dyn",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.GR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T04:48:24Z",
        "published_parsed": [
            2026,
            1,
            25,
            4,
            48,
            24,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.flu-dyn"
        },
        "authors": [
            {
                "name": "Jingrui Xing"
            },
            {
                "name": "Yizao Tang"
            },
            {
                "name": "Mengyu Chu"
            },
            {
                "name": "Baoquan Chen"
            }
        ],
        "author_detail": {
            "name": "Baoquan Chen"
        },
        "author": "Baoquan Chen",
        "journal": "arXiv: Nerf",
        "title_cn": "DDFK：使用动态无散核的流体模拟",
        "abstract_cn": "基于内存高效空间表示（例如隐式神经空间表示（INSR）和高斯空间表示（GSR））的流体模拟一直是一个新兴的研究领域，其中速度场由神经网络或加权高斯函数参数化。尽管流体求解器利用了这些空间表示的空间自适应性和连续可微分性等传统离散化的优势，但求解控制流体动力学的时间相关偏微分方程仍然具有挑战性，特别是在强制执行无散度约束的不可压缩流体中。在本文中，我们提出了一种基于无散核（DFK）的不可压缩流的无网格求解器动态无散核（DDFK）。每个 DFK 都包含一个矩阵值径向基函数和一个向量值权重，产生一个无散向量场。我们将连续流速建模为多个 DFK 的总和，从而增强不可压缩性，同时能够保留不同级别的细节。定量和定性结果表明，我们的方法在不同现象中实现了与使用内存高效空间表示的最先进方法相当的准确性、鲁棒性、保留涡流的能力、时间和内存效率以及通用性，同时擅长保持不可压缩性。尽管我们的一阶求解器比传统离散化的流体求解器慢，但由于离散化误差减少，我们的方法表现出显着较低的数值耗散。我们在具有丰富涡流和各种固体边界条件的各种不可压缩流示例中展示了我们的方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17741v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17741v1",
        "title": "Frequency-aware Neural Representation for Videos",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Frequency-aware Neural Representation for Videos"
        },
        "updated": "2026-01-25T08:19:13Z",
        "updated_parsed": [
            2026,
            1,
            25,
            8,
            19,
            13,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17741v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17741v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T08:19:13Z",
        "published_parsed": [
            2026,
            1,
            25,
            8,
            19,
            13,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jun Zhu"
            },
            {
                "name": "Xinfeng Zhang"
            },
            {
                "name": "Lv Tang"
            },
            {
                "name": "Junhao Jiang"
            },
            {
                "name": "Gai Zhang"
            },
            {
                "name": "Jia Wang"
            }
        ],
        "author_detail": {
            "name": "Jia Wang"
        },
        "author": "Jia Wang",
        "journal": "arXiv: Nerf",
        "title_cn": "视频的频率感知神经表示",
        "abstract_cn": "隐式神经表示（INR）已成为视频压缩的一种有前途的范例。然而，现有的基于 INR 的框架通常受到固有的频谱偏差的影响，这有利于低频分量并导致过度平滑的重建和次优的率失真性能。在本文中，我们提出了 FaNeRV，一种视频的频率感知神经表示，它明确地解耦低频和高频分量，以实现高效和忠实的视频重建。 FaNeRV 引入了多分辨率监督策略，引导网络通过分阶段监督逐步捕获全局结构和细粒度纹理。为了进一步增强高频重建，我们提出了一种动态高频注入机制，自适应地强调具有挑战性的区域。此外，我们设计了一个频率分解网络模块来改进跨不同频段的特征建模。对标准基准的大量实验表明，FaNeRV 显着优于最先进的 INR 方法，并实现了与传统编解码器相比具有竞争力的速率失真性能。"
    },
    {
        "id": "http://arxiv.org/abs/2601.17743v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.17743v1",
        "title": "Video Compression with Hierarchical Temporal Neural Representation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Video Compression with Hierarchical Temporal Neural Representation"
        },
        "updated": "2026-01-25T08:26:12Z",
        "updated_parsed": [
            2026,
            1,
            25,
            8,
            26,
            12,
            6,
            25,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.17743v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.17743v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-25T08:26:12Z",
        "published_parsed": [
            2026,
            1,
            25,
            8,
            26,
            12,
            6,
            25,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jun Zhu"
            },
            {
                "name": "Xinfeng Zhang"
            },
            {
                "name": "Lv Tang"
            },
            {
                "name": "Junhao Jiang"
            },
            {
                "name": "Gai Zhang"
            },
            {
                "name": "Jia Wang"
            }
        ],
        "author_detail": {
            "name": "Jia Wang"
        },
        "author": "Jia Wang",
        "journal": "arXiv: Nerf",
        "title_cn": "具有分层时间神经表示的视频压缩",
        "abstract_cn": "视频压缩最近受益于隐式神经表示（INR），它将视频建模为连续函数。 INR 提供紧凑的存储和灵活的重建，为传统编解码器提供了一种有前景的替代方案。然而，大多数现有的基于 INR 的方法将时间维度视为独立输入，限制了它们捕获复杂时间依赖性的能力。为了解决这个问题，我们提出了视频的分层时间神经表示 TeNeRV。 TeNeRV 通过两个关键组件集成了短期和长期依赖关系。首先，帧间特征融合 (IFF) 模块聚合相邻帧的特征，增强局部时间一致性并捕获细粒度运动。其次，GoP 自适应调制 (GAM) 机制将视频划分为图片组并学习特定于组的先验。该机制调节网络参数，从而实现跨不同 GoP 的自适应表示。大量实验表明，TeNeRV 在率失真性能方面始终优于现有的基于 INR 的方法，验证了我们提出的方法的有效性。"
    }
]