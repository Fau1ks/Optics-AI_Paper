[
    {
        "id": "https://doi.org/10.1364/prj.582266",
        "title": "40 Tbps classical communication coexistence with quantum key distribution over hundred-kilometer hollow-core fiber",
        "link": "https://doi.org/10.1364/prj.582266",
        "published": "2026-01-21",
        "author": "Tianqi Dou, Song Gao, Zhenhua Li, Jianjun Tang, Yuheng Xie, Lipeng Feng, Zhang Lei, Peng Li, Nan Lu, Xuewei Kan, Hai-Qiang Ma, Weiwen Kong, Shi-biao Tang",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Photonics Research",
        "title_cn": "40 Tbps 经典通信与百公里空心光纤量子密钥分发共存",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/prj.583471",
        "title": "Tunable band-pass microwave photonic filter with ultra-wide bandwidth and frequency tuning range based on cascaded tunable high-Q silicon micro-ring resonators",
        "link": "https://doi.org/10.1364/prj.583471",
        "published": "2026-01-21",
        "author": "Pengfei Wang, Cheng Wei, shangqing shi, Nuoyi Zhou, Chen Guo, Rui Ma, Junhao Ni, Guohua Hu, Bin Yun",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Photonics Research",
        "title_cn": "基于级联可调谐高Q硅微环谐振器的具有超宽带宽和频率调谐范围的可调谐带通微波光子滤波器",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/prj.579096",
        "title": "Hertz-Integral-Linewidth Lasers based on Portable Solid-state Microresonators",
        "link": "https://doi.org/10.1364/prj.579096",
        "published": "2026-01-21",
        "author": "Xing Jin, Xuanyi Zhang, Fangxing Zhang, Zhenyu Xie, Shuijing Tang, Qi-Fan Yang",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Photonics Research",
        "title_cn": "基于便携式固态微谐振器的赫兹积分线宽激光器",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "title": "PGgraf: Pose-Guided generative radiance field for novel-views on X-ray",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PGgraf: Pose-Guided generative radiance field for novel-views on X-ray"
        },
        "summary": "Publication date: April 2026Source: Displays, Volume 92Author(s): Hangyu Li, Moquan Liu, Nan Wang, Mengcheng Sun, Yu Zhu",
        "summary_detail": {
            "type": "text/html",
            "language": null,
            "base": "",
            "value": "<p>Publication date: April 2026</p><p><b>Source:</b> Displays, Volume 92</p><p>Author(s): Hangyu Li, Moquan Liu, Nan Wang, Mengcheng Sun, Yu Zhu</p>"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.sciencedirect.com/science/article/pii/S014193822600017X?dgcid=rss_sd_all"
            }
        ],
        "link": "https://www.sciencedirect.com/science/article/pii/S014193822600017X",
        "id": "https://www.sciencedirect.com/science/article/pii/S014193822600017X",
        "guidislink": false,
        "journal": "Displays",
        "title_cn": "PGgraf：用于 X 射线新颖视图的姿势引导生成辐射场",
        "abstract_cn": "Publication date: April 2026Source: Displays, Volume 92Author(s): Hangyu Li, Moquan Liu, Nan Wang, Mengcheng Sun, Yu Zhu"
    },
    {
        "title": "Harnessing differentiable geometry and orientation attention for semi-supervised vessel segmentation with limited annotations",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Harnessing differentiable geometry and orientation attention for semi-supervised vessel segmentation with limited annotations"
        },
        "summary": "Publication date: April 2026Source: Displays, Volume 92Author(s): Yan Liu, Yan Yang, Yongquan Jiang, Xiaole Zhao, Liang Fan",
        "summary_detail": {
            "type": "text/html",
            "language": null,
            "base": "",
            "value": "<p>Publication date: April 2026</p><p><b>Source:</b> Displays, Volume 92</p><p>Author(s): Yan Liu, Yan Yang, Yongquan Jiang, Xiaole Zhao, Liang Fan</p>"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.sciencedirect.com/science/article/pii/S0141938226000107?dgcid=rss_sd_all"
            }
        ],
        "link": "https://www.sciencedirect.com/science/article/pii/S0141938226000107",
        "id": "https://www.sciencedirect.com/science/article/pii/S0141938226000107",
        "guidislink": false,
        "journal": "Displays",
        "title_cn": "利用可微几何和方向注意力进行有限注释的半监督血管分割",
        "abstract_cn": "Publication date: April 2026Source: Displays, Volume 92Author(s): Yan Liu, Yan Yang, Yongquan Jiang, Xiaole Zhao, Liang Fan"
    },
    {
        "id": "https://doi.org/10.1364/oe.586748",
        "title": "Dual-plane wavefront sensing using a vision transformer.",
        "link": "https://doi.org/10.1364/oe.586748",
        "published": "2026-01-21",
        "author": "Evan O'Rourke, Kevin OKeeffe",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "使用视觉变换器的双平面波前传感。",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.581013",
        "title": "Chromatic Focus Variation with Projected Pattern Illumination for Wide Range Surface Measurements",
        "link": "https://doi.org/10.1364/oe.581013",
        "published": "2026-01-21",
        "author": "Aalim Mustafa, Hussam Muhamedsalih, Dawei Tang, PRASHANT KUMAR, Xiangqian Jiang",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "具有投影图案照明的色焦变化，适用于宽范围表面测量",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.587567",
        "title": "Study on Multilayered Coatings of Macroscopic Optical Force Transducer",
        "link": "https://doi.org/10.1364/oe.587567",
        "published": "2026-01-21",
        "author": "Siyu Huang, Chunyang Gu, Tan Chen, Fengzhou Fang",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "宏观光学力传感器多层涂层的研究",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.584385",
        "title": "Reconfigurable folded reflectarray based on an active 1-bit metasurface and its application in microwave hyperthermia",
        "link": "https://doi.org/10.1364/oe.584385",
        "published": "2026-01-21",
        "author": "xi Gao, Ke Yan Li, Shu Kun Wang",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "基于有源1位超表面的可重构折叠反射阵列及其在微波热疗中的应用",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.582377",
        "title": "Fast, efficient piston correction of deployable space telescopes using machine learning",
        "link": "https://doi.org/10.1364/oe.582377",
        "published": "2026-01-21",
        "author": "Daniel Martin, cyril Bourgenot, Andrew Reeves, Hubert P H Shum",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "使用机器学习对可部署太空望远镜进行快速、高效的活塞校正",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.581303",
        "title": "Improved Theoretical Model for Differential Wavefront Sensing Based on Complex Gaussian Decomposition with Hard-Edge Apertures",
        "link": "https://doi.org/10.1364/oe.581303",
        "published": "2026-01-21",
        "author": "Yisong Chen, xianyue meng, liu chang liu, Xiong Xinkang, Ziqiao Wang, Dong Yisi, Liang Yu, Ruitao Yang, Haijin Fu, Peng-Cheng Hu",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "基于硬边孔径复杂高斯分解的差分波前传感改进理论模型",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.580362",
        "title": "Absolute testing of weakly aspheric surfaces with pixel-level resolution and subnanometer accuracy",
        "link": "https://doi.org/10.1364/oe.580362",
        "published": "2026-01-21",
        "author": "XuPeng Li, Haitao Zhang, Wentao Gong, Xiangfu Xiao, Mingxiang Zhou, Chun-shui Jin",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "具有像素级分辨率和亚纳米精度的弱非球面绝对测试",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/oe.586647",
        "title": "Concentration and Thickness Dependent Optical Transparency in Biological Tissues via Refractive Index Modulation",
        "link": "https://doi.org/10.1364/oe.586647",
        "published": "2026-01-21",
        "author": "Paz Toledano, Guy Zimmerman, Yaakov Tischler",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Optics Express",
        "title_cn": "通过折射率调制实现生物组织中浓度和厚度相关的光学透明度",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.1364/boe.582188",
        "title": "Two-lens telecentric model eyes for image distortion measurement in adaptive optics ophthalmoscopes",
        "link": "https://doi.org/10.1364/boe.582188",
        "published": "2026-01-22",
        "author": "Yuning Xia, Gastón Ayubi, Julie Bentley, Alfredo Dubra",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Biomedical Optics Express",
        "title_cn": "用于自适应光学检眼镜中图像失真测量的双镜头远心模型眼",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.29026/oea.2026.250269",
        "title": "Timeshare surface-enhanced Raman scattering platform with sensitive and quantitative mode",
        "link": "https://doi.org/10.29026/oea.2026.250269",
        "published": "2026",
        "author": "Qianqian Ding, Xueyan Chen, Yunlu Jia, Hong Liu, Xiaochen Zhang, Ningtao Cheng, Shikuan Yang",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Opto-Electronic Advances",
        "title_cn": "具有敏感和定量模式的分时表面增强拉曼散射平台",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "id": "https://doi.org/10.29026/oea.2026.250177",
        "title": "Soft chiral superstructure enabled dynamic polychromatic holography",
        "link": "https://doi.org/10.29026/oea.2026.250177",
        "published": "2026",
        "author": "Chun-Ting Xu, Lu Li, Quan-Ming Chen, Guang-Yao Wang, Wei Hu",
        "summary": "(Abstract pending update from publisher...)",
        "journal": "Opto-Electronic Advances",
        "title_cn": "软手性上部结构实现动态多色全息术",
        "abstract_cn": "（摘要暂缺，系统已记录，待官方补全后会再次提醒）"
    },
    {
        "title": "Ultrafast VISAR velocity field reconstruction via deep unfolding networks and hardware-optimized deployment",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Ultrafast VISAR velocity field reconstruction via deep unfolding networks and hardware-optimized deployment"
        },
        "summary": "Publication date: June 2026Source: Optics and Lasers in Engineering, Volume 201Author(s): Miao Li, Chaorui Chen, Xi Wang, Xinru Zhang, Youwei Dai, Longwu Luo",
        "summary_detail": {
            "type": "text/html",
            "language": null,
            "base": "",
            "value": "<p>Publication date: June 2026</p><p><b>Source:</b> Optics and Lasers in Engineering, Volume 201</p><p>Author(s): Miao Li, Chaorui Chen, Xi Wang, Xinru Zhang, Youwei Dai, Longwu Luo</p>"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.sciencedirect.com/science/article/pii/S0143816626000230?dgcid=rss_sd_all"
            }
        ],
        "link": "https://www.sciencedirect.com/science/article/pii/S0143816626000230",
        "id": "https://www.sciencedirect.com/science/article/pii/S0143816626000230",
        "guidislink": false,
        "journal": "Optics and Lasers in Engineering",
        "title_cn": "通过深度展开网络和硬件优化部署进行超快 VISAR 速度场重建",
        "abstract_cn": "Publication date: June 2026Source: Optics and Lasers in Engineering, Volume 201Author(s): Miao Li, Chaorui Chen, Xi Wang, Xinru Zhang, Youwei Dai, Longwu Luo"
    },
    {
        "title": "3D light field display with enhanced reconstruction accuracy based on distortion- suppressed compound lens array and pre-correction encoded image",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "3D light field display with enhanced reconstruction accuracy based on distortion- suppressed compound lens array and pre-correction encoded image"
        },
        "summary": "Publication date: June 2026Source: Optics and Lasers in Engineering, Volume 201Author(s): Xudong Wen, Xin Gao, Yaohe Zheng, Ziyun Lu, Jinhong He, Hanyu Li, Ningchi Li, Boyang Liu, Binbin Yan, Xunbo Yu, Xinzhu Sang",
        "summary_detail": {
            "type": "text/html",
            "language": null,
            "base": "",
            "value": "<p>Publication date: June 2026</p><p><b>Source:</b> Optics and Lasers in Engineering, Volume 201</p><p>Author(s): Xudong Wen, Xin Gao, Yaohe Zheng, Ziyun Lu, Jinhong He, Hanyu Li, Ningchi Li, Boyang Liu, Binbin Yan, Xunbo Yu, Xinzhu Sang</p>"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.sciencedirect.com/science/article/pii/S0143816626000308?dgcid=rss_sd_all"
            }
        ],
        "link": "https://www.sciencedirect.com/science/article/pii/S0143816626000308",
        "id": "https://www.sciencedirect.com/science/article/pii/S0143816626000308",
        "guidislink": false,
        "journal": "Optics and Lasers in Engineering",
        "title_cn": "基于畸变抑制复合透镜阵列和预校正编码图像的增强重建精度的3D光场显示",
        "abstract_cn": "Publication date: June 2026Source: Optics and Lasers in Engineering, Volume 201Author(s): Xudong Wen, Xin Gao, Yaohe Zheng, Ziyun Lu, Jinhong He, Hanyu Li, Ningchi Li, Boyang Liu, Binbin Yan, Xunbo Yu, Xinzhu Sang"
    },
    {
        "id": "https://www.nature.com/articles/s41566-026-01852-z",
        "title": "Author Correction: Stabilizing high-efficiency perovskite solar cells via strategic interfacial contact engineering",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Author Correction: Stabilizing high-efficiency perovskite solar cells via strategic interfacial contact engineering"
        },
        "links": [
            {
                "rel": "alternate",
                "type": "text/html",
                "href": "https://www.nature.com/articles/s41566-026-01852-z"
            }
        ],
        "link": "https://www.nature.com/articles/s41566-026-01852-z",
        "content": [
            {
                "type": "text/html",
                "language": null,
                "base": "",
                "value": "<p>Nature Photonics, Published online: 22 January 2026; <a href=\"https://www.nature.com/articles/s41566-026-01852-z\">doi:10.1038/s41566-026-01852-z</a></p>Author Correction: Stabilizing high-efficiency perovskite solar cells via strategic interfacial contact engineering"
            }
        ],
        "summary": "Nature Photonics, Published online: 22 January 2026; doi:10.1038/s41566-026-01852-zAuthor Correction: Stabilizing high-efficiency perovskite solar cells via strategic interfacial contact engineering",
        "authors": [
            {
                "name": "Guixiang Li"
            },
            {
                "name": "Zuhong Zhang"
            },
            {
                "name": "Benjamin Agyei-Tuffour"
            },
            {
                "name": "Luyan Wu"
            },
            {
                "name": "Thomas W. Gries"
            },
            {
                "name": "Karunanantharajah Prashanthan"
            },
            {
                "name": "Lennart Frohloff"
            },
            {
                "name": "Artem Musiienko"
            },
            {
                "name": "Jinzhao Li"
            },
            {
                "name": "Rui Zhu"
            },
            {
                "name": "Lucy J. F. Hart"
            },
            {
                "name": "Luyao Wang"
            },
            {
                "name": "Zhe Li"
            },
            {
                "name": "Bo Hou"
            },
            {
                "name": "Norbert Koch"
            },
            {
                "name": "Michele Saba"
            },
            {
                "name": "Piers R. F. Barnes"
            },
            {
                "name": "Jenny Nelson"
            },
            {
                "name": "Paul J. Dyson"
            },
            {
                "name": "Mohammad Khaja Nazeeruddin"
            },
            {
                "name": "Meng Li"
            },
            {
                "name": "Antonio Abate"
            }
        ],
        "author": "Antonio Abate",
        "author_detail": {
            "name": "Guixiang Li"
        },
        "dc_identifier": "doi:10.1038/s41566-026-01852-z",
        "dc_source": "Nature Photonics, Published online: 2026-01-22; | doi:10.1038/s41566-026-01852-z",
        "updated": "2026-01-22",
        "updated_parsed": [
            2026,
            1,
            22,
            0,
            0,
            0,
            3,
            22,
            0
        ],
        "prism_publicationname": "Nature Photonics",
        "prism_doi": "10.1038/s41566-026-01852-z",
        "prism_url": "https://www.nature.com/articles/s41566-026-01852-z",
        "journal": "Nature Photonics",
        "title_cn": "作者更正：通过战略界面接触工程稳定高效钙钛矿太阳能电池",
        "abstract_cn": "《自然光子学》，在线发布：2026 年 1 月 22 日； doi:10.1038/s41566-026-01852-z作者更正：通过战略界面接触工程稳定高效钙钛矿太阳能电池"
    },
    {
        "id": "http://arxiv.org/abs/2601.15456v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15456v1",
        "title": "Attosecond-timing millimeter waves via Kerr optical frequency division",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Attosecond-timing millimeter waves via Kerr optical frequency division"
        },
        "updated": "2026-01-21T20:52:13Z",
        "updated_parsed": [
            2026,
            1,
            21,
            20,
            52,
            13,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15456v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15456v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Millimeter-wave oscillators underpin key applications in communication, spectroscopy, radar, and astronomy, yet their achievable spectral purity remains limited. Approaches that directly generate millimeter-wave carriers are fundamentally limited by quantum and thermal phase-noise processes. Here we show that these limits can be overcome by combining Kerr-induced optical frequency division in a chip-scale microresonator with a large-spacing dual-wavelength Brillouin laser. This 3.3 THz optical reference injection-locks a Kerr soliton microcomb, with a repetition rate that becomes a coherently divided 300 GHz carrier with phase noise below the quantum limit of a corresponding 300 GHz dual-wavelength Brillouin laser and far below the thermo-refractive noise of a microring resonator. Cross-correlation phase-noise measurements were developed to show that the resulting oscillator reaches a phase-noise floor of -152 dBc/Hz at 1 MHz offset, consistent with photodetection shot noise. Integration of the measured spectrum yields an RMS timing jitter of 135 as from 1 kHz to 1 MHz. These results establish optical frequency division as a generic method for generation of sub-terahertz carriers with coherence no longer constrained by direct-generation limits.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Millimeter-wave oscillators underpin key applications in communication, spectroscopy, radar, and astronomy, yet their achievable spectral purity remains limited. Approaches that directly generate millimeter-wave carriers are fundamentally limited by quantum and thermal phase-noise processes. Here we show that these limits can be overcome by combining Kerr-induced optical frequency division in a chip-scale microresonator with a large-spacing dual-wavelength Brillouin laser. This 3.3 THz optical reference injection-locks a Kerr soliton microcomb, with a repetition rate that becomes a coherently divided 300 GHz carrier with phase noise below the quantum limit of a corresponding 300 GHz dual-wavelength Brillouin laser and far below the thermo-refractive noise of a microring resonator. Cross-correlation phase-noise measurements were developed to show that the resulting oscillator reaches a phase-noise floor of -152 dBc/Hz at 1 MHz offset, consistent with photodetection shot noise. Integration of the measured spectrum yields an RMS timing jitter of 135 as from 1 kHz to 1 MHz. These results establish optical frequency division as a generic method for generation of sub-terahertz carriers with coherence no longer constrained by direct-generation limits."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T20:52:13Z",
        "published_parsed": [
            2026,
            1,
            21,
            20,
            52,
            13,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Scott C. Egbert"
            },
            {
                "name": "Brendan M. Heffernan"
            },
            {
                "name": "James Greenberg"
            },
            {
                "name": "William F. McGrew"
            },
            {
                "name": "Antoine Rolland"
            }
        ],
        "author_detail": {
            "name": "Antoine Rolland"
        },
        "author": "Antoine Rolland",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过克尔光分频实现阿秒定时毫米波",
        "abstract_cn": "毫米波振荡器支撑着通信、光谱学、雷达和天文学中的关键应用，但其可实现的光谱纯度仍然有限​​。直接生成毫米波载波的方法从根本上受到量子和热相位噪声过程的限制。在这里，我们表明，可以通过将芯片级微谐振器中的克尔诱导光学分频与大间距双波长布里渊激光器相结合来克服这些限制。这种 3.3 THz 光学参考注入锁定克尔孤子微梳，其重复率成为相干分割的 300 GHz 载波，相位噪声低于相应 300 GHz 双波长布里渊激光器的量子极限，并且远低于微环谐振器的热折射噪声。开发了互相关相位噪声测量，以表明所得振荡器在 1 MHz 偏移下达到 -152 dBc/Hz 的相位噪声本底，与光电检测散粒噪声一致。测量频谱的积分产生 1kHz 至 1MHz 范围内 135 的 RMS 定时抖动。这些结果将光频分确立为一种生成亚太赫兹载波的通用方法，其相干性不再受到直接生成限制的限制。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15480v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15480v1",
        "title": "Visualization of Gaussian Mode Profile in Gigahertz Surface-Acoustic-Wave Resonators",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Visualization of Gaussian Mode Profile in Gigahertz Surface-Acoustic-Wave Resonators"
        },
        "updated": "2026-01-21T21:30:51Z",
        "updated_parsed": [
            2026,
            1,
            21,
            21,
            30,
            51,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15480v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15480v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Surface-acoustic-wave (SAW) resonators operating at gigahertz (GHz) frequencies are widely used in wireless telecommunication and quantum information processing. Successful implementation of such resonators calls for detailed microscopic understanding of their mode profiles, energy dissipation channels, and imperfections from microfabrication. In this work, we report on the visualization of acoustic waves in LiNbO3 SAW resonators by transmission-mode microwave impedance microscopy (TMIM). The Gaussian mode profile tightly confined by reflecting mirrors is vividly seen in the TMIM images, from which the linewidth of the resonator itself can be extracted. The spatially resolved acoustic profile also allows us to perform failure analysis on faulty devices. Our work establishes a pathway for further optimization of SAW resonators for classical and quantum acoustic applications.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Surface-acoustic-wave (SAW) resonators operating at gigahertz (GHz) frequencies are widely used in wireless telecommunication and quantum information processing. Successful implementation of such resonators calls for detailed microscopic understanding of their mode profiles, energy dissipation channels, and imperfections from microfabrication. In this work, we report on the visualization of acoustic waves in LiNbO3 SAW resonators by transmission-mode microwave impedance microscopy (TMIM). The Gaussian mode profile tightly confined by reflecting mirrors is vividly seen in the TMIM images, from which the linewidth of the resonator itself can be extracted. The spatially resolved acoustic profile also allows us to perform failure analysis on faulty devices. Our work establishes a pathway for further optimization of SAW resonators for classical and quantum acoustic applications."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.other",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T21:30:51Z",
        "published_parsed": [
            2026,
            1,
            21,
            21,
            30,
            51,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Shizai Chu"
            },
            {
                "name": "Suraj Thapa Magar"
            },
            {
                "name": "John Nichol"
            },
            {
                "name": "Keji Lai"
            }
        ],
        "author_detail": {
            "name": "Keji Lai"
        },
        "author": "Keji Lai",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "千兆赫表面声波谐振器中高斯模式分布的可视化",
        "abstract_cn": "在千兆赫 (GHz) 频率下工作的表面声波 (SAW) 谐振器广泛用于无线电信和量子信息处理。成功实现此类谐振器需要对其模式分布、能量耗散通道和微加工缺陷进行详细的微观理解。在这项工作中，我们报告了通过传输模式微波阻抗显微镜 (TMIM) 对 LiNbO3 SAW 谐振器中的声波进行可视化。在 TMIM 图像中可以清楚地看到被反射镜严格限制的高斯模式轮廓，从中可以提取谐振器本身的线宽。空间分辨的声学剖面还使我们能够对故障设备进行故障分析。我们的工作为进一步优化经典和量子声学应用的声表面波谐振器建立了一条途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15502v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15502v1",
        "title": "Optical Manipulation of Erythrocytes via Evanescent Waves: Assessing Glucose-Induced Mobility Variations",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Optical Manipulation of Erythrocytes via Evanescent Waves: Assessing Glucose-Induced Mobility Variations"
        },
        "updated": "2026-01-21T22:18:31Z",
        "updated_parsed": [
            2026,
            1,
            21,
            22,
            18,
            31,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15502v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15502v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This study investigates the dynamics of red blood cells (RBCs) under the influence of evanescent waves generated by total internal reflection (TIR). Using a 1064 nm laser system and a dual-chamber prism setup, we quantified the mobility of erythrocytes in different glucose environments. Our methodology integrates automated tracking via TrackMate\\c{opyright} to analyze over 60 trajectory sets. The results reveal a significant decrease in mean velocity, from 11.8 μm/s in 5 mM glucose to 8.8 μm/s in 50 mM glucose (p = 0.019). These findings suggest that evanescent waves can serve as a non-invasive tool to probe the mechanical properties of cell membranes influenced by biochemical changes.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This study investigates the dynamics of red blood cells (RBCs) under the influence of evanescent waves generated by total internal reflection (TIR). Using a 1064 nm laser system and a dual-chamber prism setup, we quantified the mobility of erythrocytes in different glucose environments. Our methodology integrates automated tracking via TrackMate\\c{opyright} to analyze over 60 trajectory sets. The results reveal a significant decrease in mean velocity, from 11.8 μm/s in 5 mM glucose to 8.8 μm/s in 50 mM glucose (p = 0.019). These findings suggest that evanescent waves can serve as a non-invasive tool to probe the mechanical properties of cell membranes influenced by biochemical changes."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.bio-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "q-bio.CB",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "q-bio.QM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T22:18:31Z",
        "published_parsed": [
            2026,
            1,
            21,
            22,
            18,
            31,
            2,
            21,
            0
        ],
        "arxiv_comment": "5 pages, pre-print",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "T. Troncoso Enríquez"
            },
            {
                "name": "J. Staforelli-Vivanco"
            },
            {
                "name": "I. Bordeu"
            },
            {
                "name": "M. González-Ortiz"
            }
        ],
        "author_detail": {
            "name": "M. González-Ortiz"
        },
        "author": "M. González-Ortiz",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过倏逝波对红细胞进行光学操纵：评估葡萄糖引起的迁移率变化",
        "abstract_cn": "本研究研究了全内反射 (TIR) 产生的倏逝波影响下红细胞 (RBC) 的动态变化。使用 1064 nm 激光系统和双室棱镜装置，我们量化了红细胞在不同葡萄糖环境中的迁移率。我们的方法通过 TrackMate\\c{opyright} 集成自动跟踪来分析 60 多个轨迹集。结果显示平均速度显着下降，从 5 mM 葡萄糖中的 11.8 μm/s 降至 50 mM 葡萄糖中的 8.8 μm/s (p = 0.019)。这些发现表明，倏逝波可以作为一种非侵入性工具来探测受生化变化影响的细胞膜的机械特性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15562v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15562v1",
        "title": "An ultrafast diamond nonlinear photonic sensor",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "An ultrafast diamond nonlinear photonic sensor"
        },
        "updated": "2026-01-22T01:01:43Z",
        "updated_parsed": [
            2026,
            1,
            22,
            1,
            1,
            43,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15562v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15562v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1038/s41467-025-63936-8",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "The integration of light and materials technology is key to the creation of innovative sensing technologies. Sensing of electric and magnetic fields, and temperature with high spatio-temporal resolution is a critical task for the development of the next-generation of nanometer-scale quantum devices. Color centers in diamonds are attractive for potential applications owing to their characteristic quantum states, although they require metallic contacts for the introduction of external microwaves. Here, we build an ultrafast diamond nonlinear photonic sensor to assess the surface electric field; an electro-optic sensor based on nitrogen-vacancy centers in a diamond nanotip breaks the spatial-limit of conventional pump-probe techniques. The 10-fs near-infrared optical pulse modulates the surface electric field of a 2D transition metal dichalcogenide and we monitor the dynamics of the local electric field at nanometer-femtosecond spatio-temporal resolutions. Our nanoscopic technique will provide new horizons to the sensing of advanced nano materials.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The integration of light and materials technology is key to the creation of innovative sensing technologies. Sensing of electric and magnetic fields, and temperature with high spatio-temporal resolution is a critical task for the development of the next-generation of nanometer-scale quantum devices. Color centers in diamonds are attractive for potential applications owing to their characteristic quantum states, although they require metallic contacts for the introduction of external microwaves. Here, we build an ultrafast diamond nonlinear photonic sensor to assess the surface electric field; an electro-optic sensor based on nitrogen-vacancy centers in a diamond nanotip breaks the spatial-limit of conventional pump-probe techniques. The 10-fs near-infrared optical pulse modulates the surface electric field of a 2D transition metal dichalcogenide and we monitor the dynamics of the local electric field at nanometer-femtosecond spatio-temporal resolutions. Our nanoscopic technique will provide new horizons to the sensing of advanced nano materials."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T01:01:43Z",
        "published_parsed": [
            2026,
            1,
            22,
            1,
            1,
            43,
            3,
            22,
            0
        ],
        "arxiv_comment": "20 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "arxiv_journal_ref": "Nature Communications 16, 8300 (2025)",
        "authors": [
            {
                "name": "Daisuke Sato"
            },
            {
                "name": "Junjie Guo"
            },
            {
                "name": "Takuto Ichikawa"
            },
            {
                "name": "Dwi Prananto"
            },
            {
                "name": "Toshu An"
            },
            {
                "name": "Paul Fons"
            },
            {
                "name": "Shoji Yoshida"
            },
            {
                "name": "Hidemi Shigekawa"
            },
            {
                "name": "Muneaki Hase"
            }
        ],
        "author_detail": {
            "name": "Muneaki Hase"
        },
        "author": "Muneaki Hase",
        "arxiv_doi": "10.1038/s41467-025-63936-8",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "超快金刚石非线性光子传感器",
        "abstract_cn": "光和材料技术的集成是创造创新传感技术的关键。以高时空分辨率传感电场、磁场和温度是开发下一代纳米级量子器件的关键任务。钻石中的色心由于其特有的量子态而对潜在应用具有吸引力，尽管它们需要金属接触来引入外部微波。在这里，我们构建了一个超快金刚石非线性光子传感器来评估表面电场；基于金刚石纳米尖端中氮空位中心的电光传感器打破了传统泵浦探针技术的空间限制。 10-fs 近红外光脉冲调制二维过渡金属二硫属化物的表面电场，我们以纳米飞秒时空分辨率监测局部电场的动态。我们的纳米技术将为先进纳米材料的传感提供新的视野。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15565v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15565v1",
        "title": "Bright Pulsed Squeezed Light for Quantum-Enhanced Precision Microscopy",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Bright Pulsed Squeezed Light for Quantum-Enhanced Precision Microscopy"
        },
        "updated": "2026-01-22T01:07:55Z",
        "updated_parsed": [
            2026,
            1,
            22,
            1,
            7,
            55,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15565v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15565v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Squeezed states of light enable enhanced measurement precision by reducing noise below the standard quantum limit. A key application of squeezed light is nonlinear microscopy, where state-of-the-art performance is limited by photodamage and quantum-limited noise. Such microscopes require bright, pulsed light for optimal operation, yet generating and detecting bright pulsed squeezing at high levels remains challenging. In this work, we present an efficient technique to generate high levels of bright picosecond pulsed squeezed light using a $χ^2$ optical parametric amplification process in a waveguide. We measure $-3.2~\\mathrm{dB}$ of bright squeezing with optical power compatible with nonlinear microscopy, as well as $-3.6~\\mathrm{dB}$ of vacuum squeezing. Corrected for losses, these squeezing levels correspond to $-15.4^{+2.7}_{-8.7}~\\mathrm{dB}$ of squeezing generated in the waveguide. The measured level of bright amplitude pulsed squeezing is to our knowledge the highest reported to date, and will contribute to the broader adoption of quantum-enhanced nonlinear microscopy in biological studies.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Squeezed states of light enable enhanced measurement precision by reducing noise below the standard quantum limit. A key application of squeezed light is nonlinear microscopy, where state-of-the-art performance is limited by photodamage and quantum-limited noise. Such microscopes require bright, pulsed light for optimal operation, yet generating and detecting bright pulsed squeezing at high levels remains challenging. In this work, we present an efficient technique to generate high levels of bright picosecond pulsed squeezed light using a $χ^2$ optical parametric amplification process in a waveguide. We measure $-3.2~\\mathrm{dB}$ of bright squeezing with optical power compatible with nonlinear microscopy, as well as $-3.6~\\mathrm{dB}$ of vacuum squeezing. Corrected for losses, these squeezing levels correspond to $-15.4^{+2.7}_{-8.7}~\\mathrm{dB}$ of squeezing generated in the waveguide. The measured level of bright amplitude pulsed squeezing is to our knowledge the highest reported to date, and will contribute to the broader adoption of quantum-enhanced nonlinear microscopy in biological studies."
        },
        "tags": [
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T01:07:55Z",
        "published_parsed": [
            2026,
            1,
            22,
            1,
            7,
            55,
            3,
            22,
            0
        ],
        "arxiv_comment": "5 pages, 3 figures",
        "arxiv_primary_category": {
            "term": "quant-ph"
        },
        "authors": [
            {
                "name": "Alex Terrasson"
            },
            {
                "name": "Lars Madsen"
            },
            {
                "name": "Joel Grim"
            },
            {
                "name": "Warwick Bowen"
            }
        ],
        "author_detail": {
            "name": "Warwick Bowen"
        },
        "author": "Warwick Bowen",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于量子增强精密显微镜的明亮脉冲压缩光",
        "abstract_cn": "压缩光态可将噪声降低到标准量子极限以下，从而提高测量精度。压缩光的一个关键应用是非线性显微镜，其中最先进的性能受到光损伤和量子限制噪声的限制。此类显微镜需要明亮的脉冲光才能实现最佳操作，但生成和检测高水平的明亮脉冲挤压仍然具有挑战性。在这项工作中，我们提出了一种有效的技术，使用波导中的 $χ^2$ 光学参量放大过程来生成高水平的明亮皮秒脉冲压缩光。我们用与非线性显微镜兼容的光功率测量了$-3.2~\\mathrm{dB}$的明亮挤压，以及$-3.6~\\mathrm{dB}$的真空挤压。校正损耗后，这些挤压水平对应于波导中产生的挤压 $-15.4^{+2.7}_{-8.7}~\\mathrm{dB}$。据我们所知，测量到的亮振幅脉冲挤压水平是迄今为止报道的最高水平，并将有助于量子增强非线性显微镜在生物学研究中的更广泛采用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15569v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15569v1",
        "title": "σh-Broken Induced Topological quasi-BIC",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "σh-Broken Induced Topological quasi-BIC"
        },
        "updated": "2026-01-22T01:21:57Z",
        "updated_parsed": [
            2026,
            1,
            22,
            1,
            21,
            57,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15569v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15569v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Transitions from bound states in the continuum (BICs) to quasi-BICs (qBICs) are typically realized by introducing in-plane asymmetry, including permittivity asymmetry (ε-qBICs) and geometry asymmetry (g-qBICs). Here, we demonstrate that when the in-plane symmetry is rigorously kept, the transition can also be occurred, provided the out-of-plane asymmetry is designed, which is called σh -qBICs in this work. When the {σh symmetry is gradually broken, the system undergoes a topological phase transition characterized by a Zak phase inversion, leading to a band inversion between quadrupole and dipole modes. This process not only enables controlled radiation coupling of BICs but also introduces a defect-immune qBIC regime. Our findings establish a general mechanism for engineering high-Q resonances and topologically robust plasmonic cavities.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Transitions from bound states in the continuum (BICs) to quasi-BICs (qBICs) are typically realized by introducing in-plane asymmetry, including permittivity asymmetry (ε-qBICs) and geometry asymmetry (g-qBICs). Here, we demonstrate that when the in-plane symmetry is rigorously kept, the transition can also be occurred, provided the out-of-plane asymmetry is designed, which is called σh -qBICs in this work. When the {σh symmetry is gradually broken, the system undergoes a topological phase transition characterized by a Zak phase inversion, leading to a band inversion between quadrupole and dipole modes. This process not only enables controlled radiation coupling of BICs but also introduces a defect-immune qBIC regime. Our findings establish a general mechanism for engineering high-Q resonances and topologically robust plasmonic cavities."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mes-hall",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T01:21:57Z",
        "published_parsed": [
            2026,
            1,
            22,
            1,
            21,
            57,
            3,
            22,
            0
        ],
        "arxiv_comment": "4 figures in article and 2 figures in SI. 22 pages including SI",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Yongqi Chen"
            },
            {
                "name": "Chaofeng Xie"
            },
            {
                "name": "Tongtong Zhu"
            },
            {
                "name": "Weiqiang Ding"
            },
            {
                "name": "Yurui Fang"
            }
        ],
        "author_detail": {
            "name": "Yurui Fang"
        },
        "author": "Yurui Fang",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "σh 断裂诱导拓扑准 BIC",
        "abstract_cn": "从连续体束缚态 (BIC) 到准 BIC (qBIC) 的转变通常是通过引入面内不对称性来实现的，包括介电常数不对称性 (ε-qBICs) 和几何不对称性 (g-qBICs)。在这里，我们证明，当严格保持面内对称性时，如果设计了面外不对称性，也可以发生转变，这在本工作中称为 σh -qBICs。当{σh对称性逐渐被打破时，系统经历以Zak相反转为特征的拓扑相变，导致四极和偶极模式之间的能带反转。该过程不仅能够实现 BIC 的受控辐射耦合，而且还引入了缺陷免疫 qBIC 机制。我们的研究结果建立了一种设计高 Q 值共振和拓扑鲁棒等离子体腔的通用机制。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15581v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15581v1",
        "title": "Head-wearable Holographic Head-mounted Display with 6 Degrees of Freedom",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Head-wearable Holographic Head-mounted Display with 6 Degrees of Freedom"
        },
        "updated": "2026-01-22T02:02:53Z",
        "updated_parsed": [
            2026,
            1,
            22,
            2,
            2,
            53,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15581v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15581v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "A head-mounted display (HMD) using holography technology (holo-HMD) is expected to be the next generation of HMDs capable of reducing three-dimensional sickness. In HMDs, it is important to generate images that respond to head movement in real time. However, in holo-HMDs, generation of hologram data in real time is difficult due to the large computational resources required. This paper proposes a fast calculation algorithm for generating hologram data for holo-HMDs, which requires low computational power. A holo-HMD supporting six degrees of freedom was also developed using this algorithm and it was confirmed that it obtained reconstructed images with six degrees of freedom in real time (30 fps or more).",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A head-mounted display (HMD) using holography technology (holo-HMD) is expected to be the next generation of HMDs capable of reducing three-dimensional sickness. In HMDs, it is important to generate images that respond to head movement in real time. However, in holo-HMDs, generation of hologram data in real time is difficult due to the large computational resources required. This paper proposes a fast calculation algorithm for generating hologram data for holo-HMDs, which requires low computational power. A holo-HMD supporting six degrees of freedom was also developed using this algorithm and it was confirmed that it obtained reconstructed images with six degrees of freedom in real time (30 fps or more)."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T02:02:53Z",
        "published_parsed": [
            2026,
            1,
            22,
            2,
            2,
            53,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Taichi Sakakihara"
            },
            {
                "name": "Teppei Jodo"
            },
            {
                "name": "Seok Kang"
            },
            {
                "name": "Yuji Sakamoto"
            }
        ],
        "author_detail": {
            "name": "Yuji Sakamoto"
        },
        "author": "Yuji Sakamoto",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "6 自由度头戴式全息头戴显示器",
        "abstract_cn": "采用全息技术的头戴式显示器（HMD）（holo-HMD）有望成为能够减少三维眩晕的下一代头戴式显示器。在 HMD 中，生成实时响应头部运动的图像非常重要。然而，在全息头显中，由于需要大量的计算资源，实时生成全息图数据很困难。本文提出了一种为全息头戴式显示器生成全息图数据的快速计算算法，该算法需要较低的计算能力。利用该算法还开发了支持六自由度的holo-HMD，并证实其实时获得了六自由度的重建图像（30 fps或更高）。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15604v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15604v1",
        "title": "Adaptive information-maximization encoding for ghost imaging--A general Bayesian framework under experimental physical constraints",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Adaptive information-maximization encoding for ghost imaging--A general Bayesian framework under experimental physical constraints"
        },
        "updated": "2026-01-22T03:05:50Z",
        "updated_parsed": [
            2026,
            1,
            22,
            3,
            5,
            50,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15604v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15604v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "[New Paper] Abstract not indexed yet. Please visit the official website.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Ghost imaging (GI) has demonstrated diverse imaging capabilities enabled by its encoding-decoding-based computational imaging mechanism. Accordingly, information-theoretic studies have emerged as a promising avenue for probing the fundamental performance bounds of of GI and related computational imaging paradigms. However, the design of information-theoretically optimal encoding strategies remains largely unexplored, primarily due to the intractability of the prior probability density function (PDF) of an unknown scene. Here, by leveraging the ability of recursively estimating the PDF of the object to be imaged via Bayesian filtering, we propose to establish an adaptive information-maximization encoding (AIME) design framework. Based on the adaptively estimated posterior PDF from previously acquired measurements, the expected information gain of subsequent detections is evaluated and maximized to design the corresponding encoding patterns in a closed-loop manner. Within this framework, the theoretical form of the information-optimal encoding under representative physical constraints is analytically derived. Corresponding experimental results show that, GI systems employing information-optimal encoding achieve markedly improved imaging performance compared with conventional fixed point-to-point imaging without relying on additional heuristic regularization schemes, particularly in low signal-to-noise ratio regimes. Moreover, the proposed strategy consistently enables significantly enhanced information acquisition capability compared with existing encoding strategies, leading to substantially improved imaging quality. These results establish a principled information-theoretic foundation for optimal encoding design in computational imaging paradigms,provided that the forward model can be accurately characterized."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T03:05:50Z",
        "published_parsed": [
            2026,
            1,
            22,
            3,
            5,
            50,
            3,
            22,
            0
        ],
        "arxiv_comment": "37 pages, 15 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jianshuo Sun"
            },
            {
                "name": "Chenyu Hu"
            },
            {
                "name": "Zynwang Bo"
            },
            {
                "name": "Zhentao Liu"
            },
            {
                "name": "Mengyu Chen"
            },
            {
                "name": "Longkun Du"
            },
            {
                "name": "Weitao Liu"
            },
            {
                "name": "Shensheng Han"
            }
        ],
        "author_detail": {
            "name": "Shensheng Han"
        },
        "author": "Shensheng Han",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "鬼影成像的自适应信息最大化编码--实验物理约束下的通用贝叶斯框架",
        "abstract_cn": "【摘要未收录】新文章暂无数据库记录，请点击链接直达官网。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15629v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15629v1",
        "title": "Burst Mode Ultrafast Laser Welding of Sapphire and Fe-36Ni Alloy with Non-optical Contact Condition",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Burst Mode Ultrafast Laser Welding of Sapphire and Fe-36Ni Alloy with Non-optical Contact Condition"
        },
        "updated": "2026-01-22T03:59:28Z",
        "updated_parsed": [
            2026,
            1,
            22,
            3,
            59,
            28,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15629v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15629v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Ultrafast laser welding provides a promising approach for high precision integration of transparent and metallic materials. However, its practical application remains constrained by the precise regulation of the interfacial gap. This study investigates the interfacial response and bonding mechanism of sapphire and Fe-36Ni alloy joints under controlled non-optical contact conditions using burst mode ultrafast laser irradiation. A polymer interlayer was introduced between naturally stacked samples to establish a variable interfacial gap, allowing systematic evaluation of gap-dependent morphology, melting behavior, and elemental transport. By redistributing the pulse energy into sequential sub-pulses, the burst mode reconstructs the temporal energy-deposition process, yielding enhanced plasma-material coupling and stable thermal accumulation. Compared with single pulse irradiation, burst mode sustains continuous bonding across gaps exceeding 10 um--far beyond the failure threshold of the single pulse mode--and forms a fusion zone 82% larger. Fracture surface and cross-sectional analyses of SEM and EDS results confirm that sequential sub-pulses promote extensive sapphire melting, droplet-driven gap bridging, and enhanced Al-Fe interdiffusion at the interface. These results provide a scientific basis for high-gap-tolerance ultrafast laser welding and scalable integration of transparent-metal hybrid components in advanced optoelectronic and precision engineering applications.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Ultrafast laser welding provides a promising approach for high precision integration of transparent and metallic materials. However, its practical application remains constrained by the precise regulation of the interfacial gap. This study investigates the interfacial response and bonding mechanism of sapphire and Fe-36Ni alloy joints under controlled non-optical contact conditions using burst mode ultrafast laser irradiation. A polymer interlayer was introduced between naturally stacked samples to establish a variable interfacial gap, allowing systematic evaluation of gap-dependent morphology, melting behavior, and elemental transport. By redistributing the pulse energy into sequential sub-pulses, the burst mode reconstructs the temporal energy-deposition process, yielding enhanced plasma-material coupling and stable thermal accumulation. Compared with single pulse irradiation, burst mode sustains continuous bonding across gaps exceeding 10 um--far beyond the failure threshold of the single pulse mode--and forms a fusion zone 82% larger. Fracture surface and cross-sectional analyses of SEM and EDS results confirm that sequential sub-pulses promote extensive sapphire melting, droplet-driven gap bridging, and enhanced Al-Fe interdiffusion at the interface. These results provide a scientific basis for high-gap-tolerance ultrafast laser welding and scalable integration of transparent-metal hybrid components in advanced optoelectronic and precision engineering applications."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T03:59:28Z",
        "published_parsed": [
            2026,
            1,
            22,
            3,
            59,
            28,
            3,
            22,
            0
        ],
        "arxiv_comment": "11 pages, 6 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Yu Wang"
            },
            {
                "name": "Nan Li"
            },
            {
                "name": "Yuxuan Li"
            },
            {
                "name": "Yitong Chen"
            },
            {
                "name": "Qingwei Zhang"
            },
            {
                "name": "Jianing Zhao"
            },
            {
                "name": "Zhe Lin"
            },
            {
                "name": "Zihui Dong"
            },
            {
                "name": "Guochang Jiang"
            },
            {
                "name": "Zhengqiang Zhu"
            },
            {
                "name": "Shanglu Yang"
            }
        ],
        "author_detail": {
            "name": "Shanglu Yang"
        },
        "author": "Shanglu Yang",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "非光学接触条件下蓝宝石与 Fe-36Ni 合金的突发模式超快激光焊接",
        "abstract_cn": "超快激光焊接为透明材料和金属材料的高精度集成提供了一种有前途的方法。然而，其实际应用仍然受到界面间隙精确调节的限制。本研究利用突发模式超快激光照射，研究了在受控非光学接触条件下蓝宝石和 Fe-36Ni 合金接头的界面响应和结合机制。在自然堆叠的样品之间引入聚合物夹层，以建立可变的界面间隙，从而可以系统地评估间隙相关的形态、熔化行为和元素传输。通过将脉冲能量重新分配为连续的子脉冲，突发模式重建了时间能量沉积过程，从而产生增强的等离子体-材料耦合和稳定的热积累。与单脉冲照射相比，突发模式可以在超过 10 微米的间隙中维持连续键合（远远超出单脉冲模式的失效阈值），并形成大 82% 的熔合区。 SEM 和 EDS 结果的断裂表面和横截面分析证实，连续子脉冲促进了广泛的蓝宝石熔化、液滴驱动的间隙桥接以及界面处增强的 Al-Fe 相互扩散。这些结果为先进光电和精密工程应用中的高间隙公差超快激光焊接和透明金属混合元件的可扩展集成提供了科学依据。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15631v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15631v1",
        "title": "Anomalous valley Hall dynamics of exciton-polaritons",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Anomalous valley Hall dynamics of exciton-polaritons"
        },
        "updated": "2026-01-22T04:09:25Z",
        "updated_parsed": [
            2026,
            1,
            22,
            4,
            9,
            25,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15631v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15631v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The valley degree of freedom in atomically thin transition-metal dichalcogenides provides a natural binary index for information processing. Exciton-polaritons formed under strong light-matter coupling offer a promising route to overcome the limited lifetime and transport of bare valley excitons. Here we report an anomalous optical valley Hall effect in a monolayer WS2 exciton-polariton system. Using polarization- and time-resolved real-space imaging, we directly visualize a symmetry-breaking spatial separation of polaritons from opposite valleys under linearly polarized excitation, accompanied by an ultrafast Hall drift velocity on the order of 10^5 m/s. This behaviour cannot be accounted for by conventional cavity-induced mechanisms and instead points to a strain-induced synthetic pseudomagnetic field acting on the excitonic component of polaritons. Our results establish exciton-polaritons as a high-speed and optically accessible platform for valley transport, opening pathways towards tunable valleytronic and topological photonic devices.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The valley degree of freedom in atomically thin transition-metal dichalcogenides provides a natural binary index for information processing. Exciton-polaritons formed under strong light-matter coupling offer a promising route to overcome the limited lifetime and transport of bare valley excitons. Here we report an anomalous optical valley Hall effect in a monolayer WS2 exciton-polariton system. Using polarization- and time-resolved real-space imaging, we directly visualize a symmetry-breaking spatial separation of polaritons from opposite valleys under linearly polarized excitation, accompanied by an ultrafast Hall drift velocity on the order of 10^5 m/s. This behaviour cannot be accounted for by conventional cavity-induced mechanisms and instead points to a strain-induced synthetic pseudomagnetic field acting on the excitonic component of polaritons. Our results establish exciton-polaritons as a high-speed and optically accessible platform for valley transport, opening pathways towards tunable valleytronic and topological photonic devices."
        },
        "tags": [
            {
                "term": "cond-mat.mes-hall",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T04:09:25Z",
        "published_parsed": [
            2026,
            1,
            22,
            4,
            9,
            25,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cond-mat.mes-hall"
        },
        "authors": [
            {
                "name": "Xingzhou Chen"
            },
            {
                "name": "Yuanjun Guan"
            },
            {
                "name": "Areg Ghazaryan"
            },
            {
                "name": "Shiran Sun"
            },
            {
                "name": "Lingxiao Yu"
            },
            {
                "name": "Ruitao Lv"
            },
            {
                "name": "Artem Volosniev"
            },
            {
                "name": "Zheng Sun"
            },
            {
                "name": "Jian Wu"
            }
        ],
        "author_detail": {
            "name": "Jian Wu"
        },
        "author": "Jian Wu",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "激子极化子的反常谷霍尔动力学",
        "abstract_cn": "原子薄过渡金属二硫属化物的谷自由度为信息处理提供了天然的二元索引。在强光-物质耦合下形成的激子极化子为克服裸谷激子的有限寿命和传输提供了一条有前途的途径。在这里，我们报告了单层 WS2 激子-极化系统中的反常光谷霍尔效应。使用偏振和时间分辨的实空间成像，我们直接可视化线性偏振激励下来自相反谷的极化激元的对称破缺空间分离，并伴随着 10^5 m/s 量级的超快霍尔漂移速度。这种行为不能用传统的空腔诱发机制来解释，而是表明应变诱发的合成赝磁场作用于极化激元的激子成分。我们的研究结果将激子极化子确立为用于谷传输的高速且光学可访问的平台，为可调谐谷电子和拓扑光子器件开辟了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15638v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15638v1",
        "title": "An optical transistor of the nonlinear resonant structure",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "An optical transistor of the nonlinear resonant structure"
        },
        "updated": "2026-01-22T04:31:34Z",
        "updated_parsed": [
            2026,
            1,
            22,
            4,
            31,
            34,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15638v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15638v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "An optical transistor capable of simultaneous amplification and switching is theoretically proposed via cascaded second-order nonlinear interactions in a resonant structure. Two distinct operational schemes are analyzed. A single frequency scheme employs cascaded second harmonic generation and inverse second harmonic generation (SHG/iSHG) using two Type-I SHG interactions, whereas a dual frequency scheme employs cascaded SHG and optical parametric amplification (SHG/OPA). Exact theoretical solutions and numerical calculations show cascadable amplification and digital on/off switching. A new optical phenomenon of nonlinear transparency is predicted by the theoretical solutions and confirmed by the numerical solutions in each scheme of the cascaded SHG/iSHG and SHG/OPA. The single and dual frequency configurations satisfy the cascadability and fan-out criteria with power transfer ratios of 4.838 and 52.26 and power amplification factors of 48.38 and 522.6, respectively. These results indicate transistor-like performance at input powers in the milliwatt range, readily supplied by laser diodes. The proposed structure establishes a physically feasible and practically scalable route to optical transistors operating at high speed and low power for integrated photonic circuits, with broad applications in all optical communication and computing.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "An optical transistor capable of simultaneous amplification and switching is theoretically proposed via cascaded second-order nonlinear interactions in a resonant structure. Two distinct operational schemes are analyzed. A single frequency scheme employs cascaded second harmonic generation and inverse second harmonic generation (SHG/iSHG) using two Type-I SHG interactions, whereas a dual frequency scheme employs cascaded SHG and optical parametric amplification (SHG/OPA). Exact theoretical solutions and numerical calculations show cascadable amplification and digital on/off switching. A new optical phenomenon of nonlinear transparency is predicted by the theoretical solutions and confirmed by the numerical solutions in each scheme of the cascaded SHG/iSHG and SHG/OPA. The single and dual frequency configurations satisfy the cascadability and fan-out criteria with power transfer ratios of 4.838 and 52.26 and power amplification factors of 48.38 and 522.6, respectively. These results indicate transistor-like performance at input powers in the milliwatt range, readily supplied by laser diodes. The proposed structure establishes a physically feasible and practically scalable route to optical transistors operating at high speed and low power for integrated photonic circuits, with broad applications in all optical communication and computing."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T04:31:34Z",
        "published_parsed": [
            2026,
            1,
            22,
            4,
            31,
            34,
            3,
            22,
            0
        ],
        "arxiv_comment": "Submitted to Japanese Journal of Applied Physics. 14 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jongbae Kim"
            }
        ],
        "author_detail": {
            "name": "Jongbae Kim"
        },
        "author": "Jongbae Kim",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "一种非线性谐振结构的光晶体管",
        "abstract_cn": "理论上通过谐振结构中的级联二阶非线性相互作用提出了一种能够同时放大和开关的光学晶体管。分析了两种不同的操作方案。单频方案采用级联二次谐波产生和逆二次谐波产生（SHG/iSHG），使用两个I型SHG相互作用，而双频方案采用级联SHG和光参量放大（SHG/OPA）。精确的理论解决方案和数值计算显示了级联放大和数字开/关切换。在级联SHG/iSHG和SHG/OPA的每个方案中，通过理论解预测了一种新的非线性透明光学现象，并通过数值解证实了这一现象。单频和双频配置满足级联性和扇出标准，功率传输比分别为 4.838 和 52.26，功率放大系数分别为 48.38 和 522.6。这些结果表明在毫瓦范围内的输入功率下具有类似晶体管的性能，很容易由激光二极管提供。所提出的结构为集成光子电路的高速和低功率运行的光学晶体管建立了一条物理上可行且实际上可扩展的路线，在所有光通信和计算中具有广泛的应用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15654v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15654v1",
        "title": "Enhancing the Size of Phase-Space States Containing Sub-Planck-Scale Structures via Non-Gaussian Operations",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Enhancing the Size of Phase-Space States Containing Sub-Planck-Scale Structures via Non-Gaussian Operations"
        },
        "updated": "2026-01-22T05:02:56Z",
        "updated_parsed": [
            2026,
            1,
            22,
            5,
            2,
            56,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15654v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15654v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We observe a metrological advantage in phase-space sensitivity for photon-added cat and kitten states over their original forms, due to phase-space broadening from increased amplitude via photon addition, albeit with higher energy cost. Using accessible non-classical resources, weak squeezing and displacement, we construct a squeezed state and two superposed states: the squeezed cat state and the symmetrically squeezed state. Their photon-added variants are compared with parity-matched cat and KSs using quantum Fisher information and fidelity. The QFI isocontours reveal regimes where KS exhibit high fidelity and large amplitude, enabling their preparation via Gaussian operations and photon addition. Similar regimes are identified for cat states enhanced by squeezing and photon addition, demonstrating improved metrological performance. Moreover, increased amplitude and thus larger phase-space area reduces the size of interferometric fringes, enhancing the effectiveness of quantum error correction in cat codes.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We observe a metrological advantage in phase-space sensitivity for photon-added cat and kitten states over their original forms, due to phase-space broadening from increased amplitude via photon addition, albeit with higher energy cost. Using accessible non-classical resources, weak squeezing and displacement, we construct a squeezed state and two superposed states: the squeezed cat state and the symmetrically squeezed state. Their photon-added variants are compared with parity-matched cat and KSs using quantum Fisher information and fidelity. The QFI isocontours reveal regimes where KS exhibit high fidelity and large amplitude, enabling their preparation via Gaussian operations and photon addition. Similar regimes are identified for cat states enhanced by squeezing and photon addition, demonstrating improved metrological performance. Moreover, increased amplitude and thus larger phase-space area reduces the size of interferometric fringes, enhancing the effectiveness of quantum error correction in cat codes."
        },
        "tags": [
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T05:02:56Z",
        "published_parsed": [
            2026,
            1,
            22,
            5,
            2,
            56,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "quant-ph"
        },
        "authors": [
            {
                "name": "Arman"
            },
            {
                "name": "Prasanta K. Panigrahi"
            }
        ],
        "author_detail": {
            "name": "Prasanta K. Panigrahi"
        },
        "author": "Prasanta K. Panigrahi",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过非高斯运算增强包含亚普朗克尺度结构的相空间状态的大小",
        "abstract_cn": "我们观察到添加光子的猫和小猫态在相空间灵敏度方面比其原始形式具有计量优势，这是由于通过光子添加增加了振幅而导致相空间变宽，尽管能量成本更高。使用可访问的非经典资源、弱挤压和位移，我们构造了一个挤压状态和两个叠加状态：挤压猫状态和对称挤压状态。使用量子费希尔信息和保真度将它们的光子添加变体与奇偶匹配的猫和 KS 进行比较。 QFI 等值线揭示了 KS 表现出高保真度和大振幅的状态，使其能够通过高斯运算和光子加法进行准备。通过挤压和光子添加增强的猫状态也发现了类似的机制，证明了计量性能的提高。此外，增加的幅度和更大的相空间面积减少了干涉条纹的尺寸，增强了cat码中量子纠错的有效性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15695v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15695v1",
        "title": "Blue to Near-IR Integrated PZT Silicon Nitride Modulators for Quantum and Atomic Applications",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Blue to Near-IR Integrated PZT Silicon Nitride Modulators for Quantum and Atomic Applications"
        },
        "updated": "2026-01-22T06:47:25Z",
        "updated_parsed": [
            2026,
            1,
            22,
            6,
            47,
            25,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15695v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15695v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Modulation and control of lasers and optical signals is necessary for trapped-ion and cold neutral atom quantum systems. Given the diversity of atomic species, experimental modalities, and architectures, integrated optical modulators designed to operate across the visible to near-infrared spectrum are a key step towards portable, robust, and compact quantum computers, clocks, and sensors. Integrated optical modulators that are wavelength-independent, CMOS-compatible, and capable of maintaining low waveguide losses and a high resonator quality factor, DC-coupled broadband frequency response, and low power consumption, are essential for scalable photonic integration. Yet progress towards these goals has remained limited. Here we demonstrate four types of integrated stress-optic lead zirconate titanate (PZT) silicon nitride modulators: a coil Mach-Zehnder modulator, a coil pure phase modulator, and bus-coupled and add-drop ring resonator modulators, with operation from 493 nm to 780 nm. The coil MZM operates at 532 nm with a V$π$ of 2.8V, a 0.4 MHz 3-dB bandwidth, and an extinction ratio of 21.5dB. The coil phase modulator operates at 493 nm with a V$π$ of 2.8V and low residual amplitude modulation of -34 dB at a 1kHz offset. The bus-coupled ring resonator modulator operates at 493 nm and the add-drop ring resonator modulator operates at 780 nm. The ring-based modulators have an intrinsic quality factor of 3.4 million and 1.9 million, a linear tuning strength of 0.9 GHz/V and 1 GHz/V, and a 3-dB bandwidth of 2.6 MHz and 10 MHz, respectively. All four modulator designs maintain the low optical waveguide loss of SiN, are DC coupled with broadband frequency response, operate independent of wavelength, and consume only tens of nW per actuator. Such solutions unlock the potential for further integration with other precision SiN components to realize chip-scale atomic and quantum systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Modulation and control of lasers and optical signals is necessary for trapped-ion and cold neutral atom quantum systems. Given the diversity of atomic species, experimental modalities, and architectures, integrated optical modulators designed to operate across the visible to near-infrared spectrum are a key step towards portable, robust, and compact quantum computers, clocks, and sensors. Integrated optical modulators that are wavelength-independent, CMOS-compatible, and capable of maintaining low waveguide losses and a high resonator quality factor, DC-coupled broadband frequency response, and low power consumption, are essential for scalable photonic integration. Yet progress towards these goals has remained limited. Here we demonstrate four types of integrated stress-optic lead zirconate titanate (PZT) silicon nitride modulators: a coil Mach-Zehnder modulator, a coil pure phase modulator, and bus-coupled and add-drop ring resonator modulators, with operation from 493 nm to 780 nm. The coil MZM operates at 532 nm with a V$π$ of 2.8V, a 0.4 MHz 3-dB bandwidth, and an extinction ratio of 21.5dB. The coil phase modulator operates at 493 nm with a V$π$ of 2.8V and low residual amplitude modulation of -34 dB at a 1kHz offset. The bus-coupled ring resonator modulator operates at 493 nm and the add-drop ring resonator modulator operates at 780 nm. The ring-based modulators have an intrinsic quality factor of 3.4 million and 1.9 million, a linear tuning strength of 0.9 GHz/V and 1 GHz/V, and a 3-dB bandwidth of 2.6 MHz and 10 MHz, respectively. All four modulator designs maintain the low optical waveguide loss of SiN, are DC coupled with broadband frequency response, operate independent of wavelength, and consume only tens of nW per actuator. Such solutions unlock the potential for further integration with other precision SiN components to realize chip-scale atomic and quantum systems."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T06:47:25Z",
        "published_parsed": [
            2026,
            1,
            22,
            6,
            47,
            25,
            3,
            22,
            0
        ],
        "arxiv_comment": "10 pages, 6 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Nick Montifiore"
            },
            {
                "name": "Andrei Isichenko"
            },
            {
                "name": "Nitesh Chauhan"
            },
            {
                "name": "Jiawei Wang"
            },
            {
                "name": "Andrew S. Hunter"
            },
            {
                "name": "Mark W. Harrington"
            },
            {
                "name": "Rahul Chawlani"
            },
            {
                "name": "Ryan Q. Rudy"
            },
            {
                "name": "Iain Kierzewski"
            },
            {
                "name": "Michael Pushkarsky"
            },
            {
                "name": "Daniel J. Blumenthal"
            }
        ],
        "author_detail": {
            "name": "Daniel J. Blumenthal"
        },
        "author": "Daniel J. Blumenthal",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于量子和原子应用的蓝色至近红外集成 PZT 氮化硅调制器",
        "abstract_cn": "激光和光信号的调制和控制对于俘获离子和冷中性原子量子系统是必要的。考虑到原子种类、实验模式和架构的多样性，设计用于在可见光到近红外光谱范围内运行的集成光调制器是迈向便携式、稳健和紧凑的量子计算机、时钟和传感器的关键一步。集成光调制器与波长无关、与 CMOS 兼容，并且能够保持低波导损耗和高谐振器品质因数、直流耦合宽带频率响应和低功耗，对于可扩展的光子集成至关重要。然而，实现这些目标的进展仍然有限。在这里，我们展示了四种类型的集成应力光学锆钛酸铅 (PZT) 氮化硅调制器：线圈马赫曾德调制器、线圈纯相位调制器以及总线耦合和分插环谐振器调制器，工作波长范围为 493 nm 至 780 nm。线圈 MZM 工作波长为 532 nm，V$π$ 为 2.8V、0.4 MHz 3 dB 带宽和 21.5 dB 消光比。线圈相位调制器的工作波长为 493 nm，V$π$ 为 2.8V，1kHz 偏移时的低残余幅度调制为 -34 dB。总线耦合环形谐振器调制器的工作波长为 493 nm，分插环形谐振器调制器的工作波长为 780 nm。基于环的调制器的固有品质因数分别为 340 万和 190 万，线性调谐强度分别为 0.9 GHz/V 和 1 GHz/V，以及 3 dB 带宽分别为 2.6 MHz 和 10 MHz。所有四种调制器设计都保持了 SiN 的低光波导损耗，采用宽带频率响应进行直流耦合，工作时与波长无关，并且每个执行器的功耗仅为数十 nW。此类解决方案释放了与其他精密 SiN 组件进一步集成的潜力，以实现芯片级原子和量子系统。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15753v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15753v1",
        "title": "Monolithic tantalum pentoxide microrings with intrinsic Q factors exceeding 4X10(6)",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Monolithic tantalum pentoxide microrings with intrinsic Q factors exceeding 4X10(6)"
        },
        "updated": "2026-01-22T08:28:04Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            28,
            4,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15753v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15753v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Tantalum pentoxide (Ta2O5), as a silicon-photonic-compatible material platform, has garnered significant attention for high-performance integrated photonics due to its exceptional properties: a broad transparency window spanning from 0.28 um to 8 um, a moderate refractive index of 2.05 at 1550 nm, and an impressive nonlinear refractive index of 7.2X10^(-19) m^2/W. Despite these advantages, achieving low-loss fabrication of monolithic microrings on the Ta2O5 platform remains challenging due to its inherent hardness and brittleness, which often result in rough sidewalls and significant scattering losses. In this work, we successfully demonstrated monolithic Ta2O5 microring resonators with exceptionally high intrinsic and loaded quality (Q) factors. This was accomplished through the innovative application of photolithography-assisted chemo-mechanical etching (PLACE) technology. By optimizing the coupling region between the microring and the bus waveguide, as well as meticulously controlling surface roughness during fabrication, we achieved near-critical coupling in the resulting microrings. The devices exhibited loaded Q factors of 2.74X10(6) in the telecom band without employing expensive electron-beam lithography, showing an intrinsic Q factor as high as 4.47X10(6) and a low propagation loss of only 0.0732 dB/cm - representing the highest results reported for strongly confined Ta2O5-based microring resonators to date. This work paves the way for the development of advanced photonic devices on the Ta2O5 platform with low manufacturing cost, including low-threshold microlasers, highly sensitive sensors, broad bandwidth supercontinuum sources, and optical frequency combs.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Tantalum pentoxide (Ta2O5), as a silicon-photonic-compatible material platform, has garnered significant attention for high-performance integrated photonics due to its exceptional properties: a broad transparency window spanning from 0.28 um to 8 um, a moderate refractive index of 2.05 at 1550 nm, and an impressive nonlinear refractive index of 7.2X10^(-19) m^2/W. Despite these advantages, achieving low-loss fabrication of monolithic microrings on the Ta2O5 platform remains challenging due to its inherent hardness and brittleness, which often result in rough sidewalls and significant scattering losses. In this work, we successfully demonstrated monolithic Ta2O5 microring resonators with exceptionally high intrinsic and loaded quality (Q) factors. This was accomplished through the innovative application of photolithography-assisted chemo-mechanical etching (PLACE) technology. By optimizing the coupling region between the microring and the bus waveguide, as well as meticulously controlling surface roughness during fabrication, we achieved near-critical coupling in the resulting microrings. The devices exhibited loaded Q factors of 2.74X10(6) in the telecom band without employing expensive electron-beam lithography, showing an intrinsic Q factor as high as 4.47X10(6) and a low propagation loss of only 0.0732 dB/cm - representing the highest results reported for strongly confined Ta2O5-based microring resonators to date. This work paves the way for the development of advanced photonic devices on the Ta2O5 platform with low manufacturing cost, including low-threshold microlasers, highly sensitive sensors, broad bandwidth supercontinuum sources, and optical frequency combs."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:28:04Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            28,
            4,
            3,
            22,
            0
        ],
        "arxiv_comment": "11 pages, 4 figures, 1 table",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Xinzhi Zheng"
            },
            {
                "name": "Yixuan Yang"
            },
            {
                "name": "Renhong Gao"
            },
            {
                "name": "Lingling Qiao"
            },
            {
                "name": "Jintian Lin"
            },
            {
                "name": "Ya Cheng"
            }
        ],
        "author_detail": {
            "name": "Ya Cheng"
        },
        "author": "Ya Cheng",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "固有 Q 因子超过 4X10(6) 的单片五氧化二钽微环",
        "abstract_cn": "五氧化二钽（Ta2O5）作为一种硅光子兼容材料平台，由于其卓越的性能而在高性能集成光子学领域获得了广泛关注：跨越0.28 um至8 um的宽透明度窗口、1550 nm处2.05的中等折射率以及令人印象深刻的7.2X10^(-19) m^2/W的非线性折射率。尽管有这些优点，但由于其固有的硬度和脆性，在 Ta2O5 平台上实现整体微环的低损耗制造仍然具有挑战性，这通常会导致侧壁粗糙和显着的散射损耗。在这项工作中，我们成功演示了具有极高本征和负载质量 (Q) 因数的单片 Ta2O5 微环谐振器。这是通过光刻辅助化学机械蚀刻（PLACE）技术的创新应用实现的。通过优化微环和总线波导之间的耦合区域，以及在制造过程中精心控制表面粗糙度，我们在所得微环中实现了近临界耦合。该器件在电信频段中表现出 2.74X10(6) 的负载 Q 因数，无需使用昂贵的电子束光刻，显示出高达 4.47X10(6) 的固有 Q 因数和仅 0.0732 dB/cm 的低传播损耗 - 代表了迄今为止强约束 Ta2O5 基微环谐振器报告的最高结果。这项工作为在Ta2O5平台上开发低制造成本的先进光子器件铺平了道路，包括低阈值微型激光器、高灵敏度传感器、宽带超连续谱源和光学频率梳。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15769v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15769v1",
        "title": "Explainable deep-learning detection of microplastic fibers via polarization-resolved holographic microscopy",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Explainable deep-learning detection of microplastic fibers via polarization-resolved holographic microscopy"
        },
        "updated": "2026-01-22T08:59:55Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            59,
            55,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15769v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15769v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Reliable identification of microplastic fibers is crucial for environmental monitoring but remains analytically challenging. We report an explainable deep-learning framework for classifying microplastic and natural microfibers using polarization-resolved digital holographic microscopy. From multiplexed holograms, the complex Jones matrix of each fiber was reconstructed to extract polarization eigen-parameters describing optical anisotropy. Statistical descriptors of nine polarization characteristics formed a 72-dimensional feature vector for a total of 296 fibers spanning six material classes, including polyamide 6, polyethylene terephthalate, polyamide 6.6, polypropylene, cotton and wool. The designed fully connected deep neural network achieved an accuracy of 96.7 % on the validation data, surpassing that of common machine-learning classifiers. Explainable artificial intelligence analysis with Shapley additive explanations identified eigenvalue-ratio quantities as dominant predictors, revealing the physical basis for classification. An additional reduced-feature model with the preserved architecture exploiting only these most significant eigenvalue-based characteristics retained high accuracy (93.3 %), thereby confirming their dominant role while still outperforming common machine-learning classifiers. These results establish polarization-based features as distinctive optical fingerprints and demonstrate the first explainable deep-learning approach for automated microplastic fiber identification.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Reliable identification of microplastic fibers is crucial for environmental monitoring but remains analytically challenging. We report an explainable deep-learning framework for classifying microplastic and natural microfibers using polarization-resolved digital holographic microscopy. From multiplexed holograms, the complex Jones matrix of each fiber was reconstructed to extract polarization eigen-parameters describing optical anisotropy. Statistical descriptors of nine polarization characteristics formed a 72-dimensional feature vector for a total of 296 fibers spanning six material classes, including polyamide 6, polyethylene terephthalate, polyamide 6.6, polypropylene, cotton and wool. The designed fully connected deep neural network achieved an accuracy of 96.7 % on the validation data, surpassing that of common machine-learning classifiers. Explainable artificial intelligence analysis with Shapley additive explanations identified eigenvalue-ratio quantities as dominant predictors, revealing the physical basis for classification. An additional reduced-feature model with the preserved architecture exploiting only these most significant eigenvalue-based characteristics retained high accuracy (93.3 %), thereby confirming their dominant role while still outperforming common machine-learning classifiers. These results establish polarization-based features as distinctive optical fingerprints and demonstrate the first explainable deep-learning approach for automated microplastic fiber identification."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.data-an",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:59:55Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            59,
            55,
            3,
            22,
            0
        ],
        "arxiv_comment": "14 pages, 5 figures, 1 table",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jan Appel"
            },
            {
                "name": "Marika Valentino"
            },
            {
                "name": "Lisa Miccio"
            },
            {
                "name": "Vittorio Bianco"
            },
            {
                "name": "Raffaella Mossotti"
            },
            {
                "name": "Giulia Dalla Fontana"
            },
            {
                "name": "Miroslav Ježek"
            },
            {
                "name": "Pietro Ferraro"
            },
            {
                "name": "Jaromír Běhal"
            }
        ],
        "author_detail": {
            "name": "Jaromír Běhal"
        },
        "author": "Jaromír Běhal",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "通过偏振分辨全息显微镜对微塑料纤维进行可解释的深度学习检测",
        "abstract_cn": "可靠地识别微塑料纤维对于环境监测至关重要，但在分析上仍然具有挑战性。我们报告了一种可解释的深度学习框架，用于使用偏振分辨数字全息显微镜对微塑料和天然微纤维进行分类。从多重全息图中，重建每根光纤的复杂琼斯矩阵，以提取描述光学各向异性的偏振本征参数。九个偏振特性的统计描述符形成了跨越六种材料类别的总共 296 根纤维的 72 维特征向量，包括聚酰胺 6、聚对苯二甲酸乙二醇酯、聚酰胺 6.6、聚丙烯、棉和羊毛。设计的全连接深度神经网络在验证数据上实现了 96.7% 的准确率，超过了常见的机器学习分类器。具有沙普利附加解释的可解释人工智能分析将特征值比量确定为主要预测因子，揭示了分类的物理基础。具有保留架构的附加简化特征模型仅利用这些最重要的基于特征值的特征，保留了高精度（93.3％），从而证实了它们的主导作用，同时仍然优于常见的机器学习分类器。这些结果将基于偏振的特征建立为独特的光学指纹，并展示了第一个可解释的自动微塑料纤维识别的深度学习方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15776v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15776v1",
        "title": "Coherent Mode Decoupling: A Versatile Framework for High-Throughput Partially Coherent Light Transport",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Coherent Mode Decoupling: A Versatile Framework for High-Throughput Partially Coherent Light Transport"
        },
        "updated": "2026-01-22T09:06:44Z",
        "updated_parsed": [
            2026,
            1,
            22,
            9,
            6,
            44,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15776v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15776v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate and efficient wave-optics simulation of partially coherent light transport systems is critical for the design of advanced optical systems, ranging from computational lithography to diffraction-limited storage rings (DLSR). However, traditional approaches based on Coherent Mode Decomposition suffer from high computational costs due to the propagating massive sets of two-dimensional modes. In this paper, we propose the Coherent Mode Decoupling (CMDC) algorithm, a high-throughput computational framework designed to accelerate these simulations by orders of magnitude without compromising physical fidelity. The method factorizes 2D modes into efficient one-dimensional (1D) components, while crucially incorporating a subspace compression strategy to capture non-separable coupling effects. We demonstrated the generality and robustness of this framework in applications ranging from computational lithography to coherent beamlines of DLSR.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate and efficient wave-optics simulation of partially coherent light transport systems is critical for the design of advanced optical systems, ranging from computational lithography to diffraction-limited storage rings (DLSR). However, traditional approaches based on Coherent Mode Decomposition suffer from high computational costs due to the propagating massive sets of two-dimensional modes. In this paper, we propose the Coherent Mode Decoupling (CMDC) algorithm, a high-throughput computational framework designed to accelerate these simulations by orders of magnitude without compromising physical fidelity. The method factorizes 2D modes into efficient one-dimensional (1D) components, while crucially incorporating a subspace compression strategy to capture non-separable coupling effects. We demonstrated the generality and robustness of this framework in applications ranging from computational lithography to coherent beamlines of DLSR."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T09:06:44Z",
        "published_parsed": [
            2026,
            1,
            22,
            9,
            6,
            44,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Han Xu"
            },
            {
                "name": "Ming Li"
            },
            {
                "name": "Shuo Wang"
            },
            {
                "name": "Zhe Ren"
            },
            {
                "name": "Peng Liu"
            },
            {
                "name": "Yi Zhang"
            },
            {
                "name": "Yuhui Dong"
            },
            {
                "name": "Liang Zhou"
            }
        ],
        "author_detail": {
            "name": "Liang Zhou"
        },
        "author": "Liang Zhou",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "相干模式解耦：高吞吐量部分相干光传输的多功能框架",
        "abstract_cn": "部分相干光传输系统的准确高效的波动光学模拟对于先进光学系统（从计算光刻到衍射极限存储环（DLSR））的设计至关重要。然而，基于相干模态分解的传统方法由于传播大量二维模态而导致计算成本很高。在本文中，我们提出了相干模式解耦（CMDC）算法，这是一种高吞吐量计算框架，旨在在不影响物理保真度的情况下将这些模拟加速几个数量级。该方法将 2D 模式分解为有效的一维 (1D) 分量，同时关键地结合子空间压缩策略来捕获不可分离的耦合效应。我们在从计算光刻到 DLSR 相干光束线的应用中证明了该框架的通用性和鲁棒性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15805v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15805v1",
        "title": "Distance-Independent Atmospheric Refraction Correction for Accurate Retrieval of Fireball Trajectories",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Distance-Independent Atmospheric Refraction Correction for Accurate Retrieval of Fireball Trajectories"
        },
        "updated": "2026-01-22T09:42:39Z",
        "updated_parsed": [
            2026,
            1,
            22,
            9,
            42,
            39,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15805v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15805v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate determination of fireball direction is essential for retrieving trajectories and velocities. Errors in these measurements have significant implications, affecting the calculated pre-impact orbit, influencing mass estimates, and impacting the accuracy of dark flight simulations, where applicable. Here we implement a new atmospheric refraction correction technique that addresses a significant aspect previously overlooked in the field of meteor science. Traditional refraction correction techniques, originally designed for objects positioned at infinite distances, tend to overcompensate when applied to objects within the Earth's atmosphere. To rectify this issue, our study introduces the concept of the atmospheric refraction delta z correction technique, involving the artificial elevation of the observer site height above sea level. We utilize analytically derived formulas for the delta z correction in conjunction with commonly used refraction models, validating these results against a numerical solution that traces light rays through the atmosphere. This ray-tracing model is applied to finely meshed atmospheric layers, yielding precise correction values. We evaluate multiple sources of error in order to quantify the achievable accuracy of the proposed method. Our approach (1) enables the determination of fireball positions with improved astrometric accuracy, (2) removes the explicit dependence on the fireball distance from the observer or its height above Earth's surface within the limits imposed by realistic atmospheric variability, and (3) simplifies meteor data processing by providing a robust framework for analyzing low-elevation fireball observations, for which atmospheric refraction is significant and is automatically corrected by the method. As a result of this work, we provide open, publicly accessible software for calculating the delta z correction.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate determination of fireball direction is essential for retrieving trajectories and velocities. Errors in these measurements have significant implications, affecting the calculated pre-impact orbit, influencing mass estimates, and impacting the accuracy of dark flight simulations, where applicable. Here we implement a new atmospheric refraction correction technique that addresses a significant aspect previously overlooked in the field of meteor science. Traditional refraction correction techniques, originally designed for objects positioned at infinite distances, tend to overcompensate when applied to objects within the Earth's atmosphere. To rectify this issue, our study introduces the concept of the atmospheric refraction delta z correction technique, involving the artificial elevation of the observer site height above sea level. We utilize analytically derived formulas for the delta z correction in conjunction with commonly used refraction models, validating these results against a numerical solution that traces light rays through the atmosphere. This ray-tracing model is applied to finely meshed atmospheric layers, yielding precise correction values. We evaluate multiple sources of error in order to quantify the achievable accuracy of the proposed method. Our approach (1) enables the determination of fireball positions with improved astrometric accuracy, (2) removes the explicit dependence on the fireball distance from the observer or its height above Earth's surface within the limits imposed by realistic atmospheric variability, and (3) simplifies meteor data processing by providing a robust framework for analyzing low-elevation fireball observations, for which atmospheric refraction is significant and is automatically corrected by the method. As a result of this work, we provide open, publicly accessible software for calculating the delta z correction."
        },
        "tags": [
            {
                "term": "astro-ph.IM",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "astro-ph.EP",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.ao-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.geo-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T09:42:39Z",
        "published_parsed": [
            2026,
            1,
            22,
            9,
            42,
            39,
            3,
            22,
            0
        ],
        "arxiv_comment": "Accepted for publication in Monthly Notices of the Royal Astronomical Society",
        "arxiv_primary_category": {
            "term": "astro-ph.IM"
        },
        "authors": [
            {
                "name": "Jaakko Visuri"
            },
            {
                "name": "Maria Gritsevich"
            },
            {
                "name": "Janne Sievinen"
            }
        ],
        "author_detail": {
            "name": "Janne Sievinen"
        },
        "author": "Janne Sievinen",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "用于精确反演火球轨迹的与距离无关的大气折射校正",
        "abstract_cn": "准确确定火球方向对于检索轨迹和速度至关重要。这些测量中的误差具有重大影响，影响计算的撞击前轨道，影响质量估计，并影响暗飞行模拟的准确性（如果适用）。在这里，我们实施了一种新的大气折射校正技术，解决了流星科学领域以前被忽视的一个重要方面。传统的折射校正技术最初是为无限远距离的物体设计的，但当应用于地球大气层内的物体时，往往会过度补偿。为了纠正这个问题，我们的研究引入了大气折射δ z 校正技术的概念，涉及人工抬高观测站海拔高度。我们利用分析得出的公式进行 delta z 校正，并结合常用的折射模型，根据追踪穿过大气的光线的数值解来验证这些结果。该光线追踪模型应用于精细网格化的大气层，产生精确的校正值。我们评估多个误差源，以量化所提出方法可实现的精度。我们的方法（1）能够以更高的天体测量精度确定火球位置，（2）在实际大气变化所施加的限制内消除对火球与观察者的距离或火球在地球表面以上高度的明确依赖，以及（3）通过提供用于分析低海拔火球观测的强大框架来简化流星数据处理，对于低海拔火球观测，大气折射很重要，并且可以通过该方法自动校正。这项工作的结果是，我们提供了开放的、可公开访问的软件来计算 delta z 校正。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15817v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15817v1",
        "title": "Photorefraction Management in Lithium Niobate Waveguides: High-Temperature vs. Cryogenic Solutions",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Photorefraction Management in Lithium Niobate Waveguides: High-Temperature vs. Cryogenic Solutions"
        },
        "updated": "2026-01-22T10:04:46Z",
        "updated_parsed": [
            2026,
            1,
            22,
            10,
            4,
            46,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15817v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15817v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Lithium niobate sees widespread use in nonlinear and quantum optical devices, such as for sum- and difference-frequency generation or spontaneous parametric down-conversion. In lithium niobate waveguides, nonlinear optical processes are often limited by the so-called photorefractive effect, which limits the maximum input or output powers and impacts the nonlinear spectral response. Therefore, strategies for the management of photorefractive damage are a key consideration in device design. Usually, the photorefractive damage threshold, i.e. the maximal permissible operating power, can be increased by high temperature operation of devices. This approach, however, is not applicable in cryogenic environments, which may be required for specialized applications. To better understand the impact of photorefraction in nonlinear optical applications, we study the impact of photorefraction on the phase-matching spectra of two nonlinear-optical sum-frequency generation experiments at 1) high temperatures and 2) cryogenic temperatures. Furthermore, we present an approach to reduce the impact of photorefraction which is compatible with cryogenic operation. This comprises an auxiliary light source, propagating in the same waveguide, which is used to restore phase-matching spectra impacted by photorefraction, as well as reduce pyroelectric effects. Our work provides an alternative route to photorefraction management applicable to cryogenic environments, as well as in situations with tight energy budgets like space applications.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Lithium niobate sees widespread use in nonlinear and quantum optical devices, such as for sum- and difference-frequency generation or spontaneous parametric down-conversion. In lithium niobate waveguides, nonlinear optical processes are often limited by the so-called photorefractive effect, which limits the maximum input or output powers and impacts the nonlinear spectral response. Therefore, strategies for the management of photorefractive damage are a key consideration in device design. Usually, the photorefractive damage threshold, i.e. the maximal permissible operating power, can be increased by high temperature operation of devices. This approach, however, is not applicable in cryogenic environments, which may be required for specialized applications. To better understand the impact of photorefraction in nonlinear optical applications, we study the impact of photorefraction on the phase-matching spectra of two nonlinear-optical sum-frequency generation experiments at 1) high temperatures and 2) cryogenic temperatures. Furthermore, we present an approach to reduce the impact of photorefraction which is compatible with cryogenic operation. This comprises an auxiliary light source, propagating in the same waveguide, which is used to restore phase-matching spectra impacted by photorefraction, as well as reduce pyroelectric effects. Our work provides an alternative route to photorefraction management applicable to cryogenic environments, as well as in situations with tight energy budgets like space applications."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T10:04:46Z",
        "published_parsed": [
            2026,
            1,
            22,
            10,
            4,
            46,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Nina A. Lange"
            },
            {
                "name": "René Pollmann"
            },
            {
                "name": "Michael Rüsing"
            },
            {
                "name": "Michael Stefszky"
            },
            {
                "name": "Maximilian Protte"
            },
            {
                "name": "Raimund Ricken"
            },
            {
                "name": "Laura Padberg"
            },
            {
                "name": "Christof Eigner"
            },
            {
                "name": "Tim J. Bartley"
            },
            {
                "name": "Christine Silberhorn"
            }
        ],
        "author_detail": {
            "name": "Christine Silberhorn"
        },
        "author": "Christine Silberhorn",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "铌酸锂波导中的光折射管理：高温与低温解决方案",
        "abstract_cn": "铌酸锂广泛应用于非线性和量子光学器件，例如和频和差频生成或自发参量下变频。在铌酸锂波导中，非线性光学过程通常受到所谓的光折变效应的限制，这限制了最大输入或输出功率并影响非线性光谱响应。因此，光折变损伤的管理策略是设备设计中的关键考虑因素。通常，光折变损伤阈值，即最大允许工作功率，可以通过器件的高温工作来提高。然而，这种方法不适用于低温环境，而特殊应用可能需要低温环境。为了更好地了解光折射在非线性光学应用中的影响，我们研究了光折射对 1) 高温和 2) 低温下两个非线性光学和频生成实验的相位匹配光谱的影响。此外，我们提出了一种减少光折射影响的方法，该方法与低温操作兼容。这包括在同一波导中传播的辅助光源，用于恢复受光折射影响的相位匹配光谱，并减少热释电效应。我们的工作提供了一种适用于低温环境以及能源预算紧张的情况（如太空应用）的光折射管理的替代途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15898v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15898v1",
        "title": "Size-dependent Dielectric Permittivity of Perovskite Nanocrystals",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Size-dependent Dielectric Permittivity of Perovskite Nanocrystals"
        },
        "updated": "2026-01-22T12:25:07Z",
        "updated_parsed": [
            2026,
            1,
            22,
            12,
            25,
            7,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15898v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15898v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Perovskite nanocrystals (PNCs) are promising building blocks for quantum photonic devices. Optical properties of PNCs can be enhanced by integration with optical cavities or nanoantennas. Designing such structures requires accurate size dependent dielectric permittivity of PNCs. However, current reports provide primarily ensemble averaged values with limited access to the intrinsic response of individual PNCs. Here we suggest a methodology to reconstruct the size dependent complex dielectric permittivity of CsPbBr3 PNCs from the measured absorbance spectrum of colloidal solution. The permittivity of PNCs is modeled as a sum of Voigt profile oscillators, with the size dependent transition energies governed by the exciton effective mass. Using a transmission electron microscopy derived size distribution of the PNCs, the solution permittivity is obtained via Maxwell Garnett effective medium approximation. This permittivity is used in a transfer matrix method to simulate and fit the absorbance spectrum, from which the permittivity of PNCs is reconstructed. The extracted spectral linewidth from the imaginary part of the permittivity (78.4 meV) is consistent with single nanocrystal emission linewidths at room temperature. Finite element simulations show enhanced absorption cross section of a single PNC coupled to a nanoantenna, demonstrating applicability of the extracted permittivity. More generally, these findings provide a route to extract intrinsic permittivity of individual nanocrystals from absorbance measurements of their ensembles.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Perovskite nanocrystals (PNCs) are promising building blocks for quantum photonic devices. Optical properties of PNCs can be enhanced by integration with optical cavities or nanoantennas. Designing such structures requires accurate size dependent dielectric permittivity of PNCs. However, current reports provide primarily ensemble averaged values with limited access to the intrinsic response of individual PNCs. Here we suggest a methodology to reconstruct the size dependent complex dielectric permittivity of CsPbBr3 PNCs from the measured absorbance spectrum of colloidal solution. The permittivity of PNCs is modeled as a sum of Voigt profile oscillators, with the size dependent transition energies governed by the exciton effective mass. Using a transmission electron microscopy derived size distribution of the PNCs, the solution permittivity is obtained via Maxwell Garnett effective medium approximation. This permittivity is used in a transfer matrix method to simulate and fit the absorbance spectrum, from which the permittivity of PNCs is reconstructed. The extracted spectral linewidth from the imaginary part of the permittivity (78.4 meV) is consistent with single nanocrystal emission linewidths at room temperature. Finite element simulations show enhanced absorption cross section of a single PNC coupled to a nanoantenna, demonstrating applicability of the extracted permittivity. More generally, these findings provide a route to extract intrinsic permittivity of individual nanocrystals from absorbance measurements of their ensembles."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cond-mat.mtrl-sci",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T12:25:07Z",
        "published_parsed": [
            2026,
            1,
            22,
            12,
            25,
            7,
            3,
            22,
            0
        ],
        "arxiv_comment": "29 pages, 4 figures",
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jehyeok Ryu"
            },
            {
                "name": "Victor Krivenkov"
            },
            {
                "name": "Vitaly Goryashko"
            },
            {
                "name": "Yury Rakovich"
            },
            {
                "name": "Alexey Y. Nikitin"
            }
        ],
        "author_detail": {
            "name": "Alexey Y. Nikitin"
        },
        "author": "Alexey Y. Nikitin",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "钙钛矿纳米晶体的尺寸依赖性介电常数",
        "abstract_cn": "钙钛矿纳米晶体（PNC）是有前途的量子光子器件的构建模块。 PNC 的光学特性可以通过与光腔或纳米天线集成来增强。设计此类结构需要精确的 PNC 尺寸相关介电常数。然而，当前的报告主要提供整体平均值，对单个 PNC 的内在响应的访问有限。在这里，我们提出了一种根据测量的胶体溶液吸收光谱重建 CsPbBr3 PNC 的尺寸依赖性复介电常数的方法。 PNC 的介电常数被建模为 Voigt 剖面振荡器的总和，其尺寸相关的跃迁能量由激子有效质量控制。使用透射电子显微镜导出的 PNC 尺寸分布，通过 Maxwell Garnett 有效介质近似获得溶液介电常数。该介电常数用于传输矩阵方法来模拟和拟合吸收光谱，从中重建 PNC 的介电常数。从介电常数虚部（78.4 meV）提取的光谱线宽与室温下单纳米晶发射线宽一致。有限元模拟显示耦合到纳米天线的单个 PNC 的吸收截面增强，证明了提取的介电常数的适用性。更一般地说，这些发现提供了一种从单个纳米晶体整体的吸光度测量中提取其固有介电常数的途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15925v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15925v1",
        "title": "Quantitative absorption tomography",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Quantitative absorption tomography"
        },
        "updated": "2026-01-22T12:59:28Z",
        "updated_parsed": [
            2026,
            1,
            22,
            12,
            59,
            28,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15925v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15925v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Brightfield microscopy is central to wide range of biology, engineering, and histopathology; but is inherently limited to two-dimensional qualitative imaging, systematically investigating three-dimensional (3D) volumetric architecture. Here we introduce quantitative absorption tomography (QAT), a computational approach that quantitatively reconstructs high-resolution volumetric absorption coefficient distributions from brightfield focal stacks. By modeling absorption image formation in logarithmic intensity space and applying deconvolution with an absorption optical transfer function, QAT enables quantitative, spectrally resolved 3D absorption imaging without interferometry, sample rotation, or specialized hardware. We validate QAT using spectrally selective phantoms and demonstrate absorption-specific contrast complementary to refractive index tomography in living melanocytes and intact plant tissue. QAT further scales to millimeter-scale volumes of H&E-stained human tissue, revealing 3D histological microarchitecture without serial sectioning. This approach extends brightfield microscopy toward practical 3D histopathology.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Brightfield microscopy is central to wide range of biology, engineering, and histopathology; but is inherently limited to two-dimensional qualitative imaging, systematically investigating three-dimensional (3D) volumetric architecture. Here we introduce quantitative absorption tomography (QAT), a computational approach that quantitatively reconstructs high-resolution volumetric absorption coefficient distributions from brightfield focal stacks. By modeling absorption image formation in logarithmic intensity space and applying deconvolution with an absorption optical transfer function, QAT enables quantitative, spectrally resolved 3D absorption imaging without interferometry, sample rotation, or specialized hardware. We validate QAT using spectrally selective phantoms and demonstrate absorption-specific contrast complementary to refractive index tomography in living melanocytes and intact plant tissue. QAT further scales to millimeter-scale volumes of H&E-stained human tissue, revealing 3D histological microarchitecture without serial sectioning. This approach extends brightfield microscopy toward practical 3D histopathology."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T12:59:28Z",
        "published_parsed": [
            2026,
            1,
            22,
            12,
            59,
            28,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Yoonjae Chung"
            },
            {
                "name": "Sehyun Lee"
            },
            {
                "name": "Herve Hugonnet"
            },
            {
                "name": "Chulmin Oh"
            },
            {
                "name": "Weisun Park"
            },
            {
                "name": "Yeon Wook Kim"
            },
            {
                "name": "Seung-Mo Hong"
            },
            {
                "name": "YongKeun Park"
            }
        ],
        "author_detail": {
            "name": "YongKeun Park"
        },
        "author": "YongKeun Park",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "定量吸收断层扫描",
        "abstract_cn": "明场显微镜是广泛的生物学、工程学和组织病理学的核心。但本质上仅限于二维定性成像，系统地研究三维 (3D) 体积结构。在这里，我们介绍定量吸收断层扫描（QAT），这是一种从明场焦点堆栈定量重建高分辨率体积吸收系数分布的计算方法。通过对对数强度空间中的吸收图像形成进行建模，并应用吸收光学传递函数的反卷积，QAT 无需干涉测量、样品旋转或专用硬件即可实现定量、光谱解析的 3D 吸收成像。我们使用光谱选择性模型验证 QAT，并在活体黑素细胞和完整植物组织中证明与折射率断层扫描互补的吸收特异性对比度。 QAT 进一步将 H&E 染色的人体组织缩小到毫米级体积，无需连续切片即可揭示 3D 组织学微结构。这种方法将明场显微镜扩展到实用的 3D 组织病理学。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15947v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15947v1",
        "title": "Multimodal Imaging System Combining Hyperspectral and Laser Speckle Imaging for In Vivo Hemodynamic and Metabolic Monitoring",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multimodal Imaging System Combining Hyperspectral and Laser Speckle Imaging for In Vivo Hemodynamic and Metabolic Monitoring"
        },
        "updated": "2026-01-22T13:32:34Z",
        "updated_parsed": [
            2026,
            1,
            22,
            13,
            32,
            34,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15947v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15947v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We present the development and validation of a novel multimodal optical imaging platform that integrates hyperspectral imaging (HSI) and laser speckle contrast imaging (LSCI) to enable real-time, non-invasive mapping of tissue oxygenation, perfusion and metabolism, via blood flowmetry and targeting of oxy- (HbO2), deoxyhemoglobin (HHb), as well as oxidized cytochrome-c-oxidase (oxCCO). The system architecture features a single high-speed camera and dual optical path, with synchronized alternating illumination: a filtered, supercontinuum laser for HSI and a He-Ne laser for LSCI. The system performances were evaluated through in vivo experiments on rat spinal cord under normoxic and hypoxic conditions, revealing coherent physiological changes in hemodynamics, metabolism and relative blood flow index (rBFI). These results demonstrate the potential of the platform for functional tissue imaging and quantitative dynamic monitoring of both oxygen delivery and consumption.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We present the development and validation of a novel multimodal optical imaging platform that integrates hyperspectral imaging (HSI) and laser speckle contrast imaging (LSCI) to enable real-time, non-invasive mapping of tissue oxygenation, perfusion and metabolism, via blood flowmetry and targeting of oxy- (HbO2), deoxyhemoglobin (HHb), as well as oxidized cytochrome-c-oxidase (oxCCO). The system architecture features a single high-speed camera and dual optical path, with synchronized alternating illumination: a filtered, supercontinuum laser for HSI and a He-Ne laser for LSCI. The system performances were evaluated through in vivo experiments on rat spinal cord under normoxic and hypoxic conditions, revealing coherent physiological changes in hemodynamics, metabolism and relative blood flow index (rBFI). These results demonstrate the potential of the platform for functional tissue imaging and quantitative dynamic monitoring of both oxygen delivery and consumption."
        },
        "tags": [
            {
                "term": "physics.med-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.bio-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.ins-det",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T13:32:34Z",
        "published_parsed": [
            2026,
            1,
            22,
            13,
            32,
            34,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.med-ph"
        },
        "authors": [
            {
                "name": "Junda Wang"
            },
            {
                "name": "Luca Giannoni"
            },
            {
                "name": "Ayse Gertrude Yenicelik"
            },
            {
                "name": "Eleni Giama"
            },
            {
                "name": "Frederic Lange"
            },
            {
                "name": "Kenneth J. Smith"
            },
            {
                "name": "Ilias Tachtsidis"
            }
        ],
        "author_detail": {
            "name": "Ilias Tachtsidis"
        },
        "author": "Ilias Tachtsidis",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "结合高光谱和激光散斑成像的多模态成像系统，用于体内血流动力学和代谢监测",
        "abstract_cn": "我们展示了一种新型多模态光学成像平台的开发和验证，该平台集成了高光谱成像（HSI）和激光散斑对比成像（LSCI），通过血流量测定和靶向氧合（HbO2）、脱氧血红蛋白（HHb）以及氧化细胞色素-c-氧化酶（oxCCO），实现组织氧合、灌注和代谢的实时、非侵入性绘图。该系统架构具有单个高速相机和双光路，具有同步交替照明：用于 HSI 的过滤超连续谱激光器和用于 LSCI 的 He-Ne 激光器。通过在常氧和低氧条件下对大鼠脊髓进行体内实验来评估系统性能，揭示血流动力学、代谢和相对血流指数（rBFI）的连贯生理变化。这些结果证明了该平台在功能组织成像和氧气输送和消耗的定量动态监测方面的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15971v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15971v1",
        "title": "Reaching the intrinsic performance limits of superconducting strip photon detectors up to 0.1 mm wide",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Reaching the intrinsic performance limits of superconducting strip photon detectors up to 0.1 mm wide"
        },
        "updated": "2026-01-22T13:51:49Z",
        "updated_parsed": [
            2026,
            1,
            22,
            13,
            51,
            49,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15971v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15971v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Superconducting nanowire single-photon detectors (SNSPDs) have emerged as the highest performing photon-counting detectors, making them a critical technology in quantum photonics and photon-starved optical sensing. However, the performance of SNSPDs is limited not by the intrinsic properties of the superconducting film, but by edge-induced current crowding. Despite extensive materials optimization and increasingly demanding fabrication strategies aimed at mitigating this edge-limited behavior, the device edges continue to limit the maximum device operating current, thereby degrading key performance metrics. Here, we demonstrate for the first time in situ tuning of a detector from an edge-limited to a bulk-limited regime, allowing the device to reach its intrinsic performance limit. Our approach is based on current-biased superconducting \"rails\" placed on either side of the detector to suppress current crowding at the edges. We show that activation of the rails reduces the dark count rate by nine orders of magnitude and extends the photon detection plateau at 1550 nm by more than 40%. These results are demonstrated on detectors up to 0.1 mm wide, establishing an entirely new class of ultra-wide strip detectors that we call superconducting strip photon detectors (SSPD). Moreover, the ability to suppress edge current crowding using the rails provides a pathway toward SSPDs with strip widths extending into the mm-scale. Such devices will enable large-area, high efficiency SSPD arrays with infrared sensitivity and open new opportunities in applications ranging from biomedical imaging to deep space optical communication.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Superconducting nanowire single-photon detectors (SNSPDs) have emerged as the highest performing photon-counting detectors, making them a critical technology in quantum photonics and photon-starved optical sensing. However, the performance of SNSPDs is limited not by the intrinsic properties of the superconducting film, but by edge-induced current crowding. Despite extensive materials optimization and increasingly demanding fabrication strategies aimed at mitigating this edge-limited behavior, the device edges continue to limit the maximum device operating current, thereby degrading key performance metrics. Here, we demonstrate for the first time in situ tuning of a detector from an edge-limited to a bulk-limited regime, allowing the device to reach its intrinsic performance limit. Our approach is based on current-biased superconducting \"rails\" placed on either side of the detector to suppress current crowding at the edges. We show that activation of the rails reduces the dark count rate by nine orders of magnitude and extends the photon detection plateau at 1550 nm by more than 40%. These results are demonstrated on detectors up to 0.1 mm wide, establishing an entirely new class of ultra-wide strip detectors that we call superconducting strip photon detectors (SSPD). Moreover, the ability to suppress edge current crowding using the rails provides a pathway toward SSPDs with strip widths extending into the mm-scale. Such devices will enable large-area, high efficiency SSPD arrays with infrared sensitivity and open new opportunities in applications ranging from biomedical imaging to deep space optical communication."
        },
        "tags": [
            {
                "term": "cond-mat.supr-con",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.app-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.ins-det",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "quant-ph",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T13:51:49Z",
        "published_parsed": [
            2026,
            1,
            22,
            13,
            51,
            49,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cond-mat.supr-con"
        },
        "authors": [
            {
                "name": "Kristen M. Parzuchowski"
            },
            {
                "name": "Eli Mueller"
            },
            {
                "name": "Bakhrom G. Oripov"
            },
            {
                "name": "Benedikt Hampel"
            },
            {
                "name": "Ravin A. Chowdhury"
            },
            {
                "name": "Sahil R. Patel"
            },
            {
                "name": "Daniel Kuznesof"
            },
            {
                "name": "Emma K. Batson"
            },
            {
                "name": "Ryan Morgenstern"
            },
            {
                "name": "Robert H. Hadfield"
            },
            {
                "name": "Varun B. Verma"
            },
            {
                "name": "Matthew D. Shaw"
            },
            {
                "name": "Jason P. Allmaras"
            },
            {
                "name": "Martin J. Stevens"
            },
            {
                "name": "Alex Gurevich"
            },
            {
                "name": "Adam N. McCaughan"
            }
        ],
        "author_detail": {
            "name": "Adam N. McCaughan"
        },
        "author": "Adam N. McCaughan",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "达到宽度达 0.1 毫米的超导带状光子探测器的固有性能极限",
        "abstract_cn": "超导纳米线单光子探测器（SNSPD）已成为性能最高的光子计数探测器，使其成为量子光子学和光子匮乏光学传感领域的关键技术。然而，SNSPD 的性能不仅受到超导薄膜固有特性的限制，还受到边缘感应电流拥挤的限制。尽管为了减轻这种边缘限制行为而进行了广泛的材料优化和日益严格的制造策略，但器件边缘仍然限制了最大器件工作电流，从而降低了关键性能指标。在这里，我们首次演示了探测器从边缘限制到体限制状态的原位调谐，使设备能够达到其固有的性能极限。我们的方法基于放置在探测器两侧的电流偏置超导“轨道”，以抑制边缘的电流拥挤。我们表明，轨道的激活将暗计数率降低了 9 个数量级，并将 1550 nm 处的光子检测平台延长了 40% 以上。这些结果在宽度达 0.1 毫米的探测器上得到了证明，建立了一种全新的超宽带状探测器，我们称之为超导带状光子探测器 (SSPD)。此外，使用导轨抑制边缘电流拥挤的能力为带宽度延伸至毫米级的 SSPD 提供了一条途径。此类器件将实现具有红外灵敏度的大面积、高效 SSPD 阵列，并为从生物医学成像到深空光通信等应用领域开辟新的机遇。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15981v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15981v1",
        "title": "Mid-infrared high-sensitive cavity-free in-situ CO gas sensing based on up-conversion detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Mid-infrared high-sensitive cavity-free in-situ CO gas sensing based on up-conversion detection"
        },
        "updated": "2026-01-22T14:03:56Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            3,
            56,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15981v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15981v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Carbon monoxide (CO) is a significant indicator gas with considerable application value in atmospheric monitoring, industrial production and medical diagnosis. Its fundamental vibrational band locates around 4.6 $\\upmu$m and has larger absorption line strength than that of overtone band, which is more suitable for the precise identification and concentration detection of CO. In this paper, the up-conversion detection is employed to convert the mid-infrared absorption signal obtained by TDLAS to the visible light band, then a silicon-based detector is utilized for detection. By which, we can achieve the highest sensitivity of 79.6 ppb under the condition of cavity-free in-situ with an absorption range length of only 0.14 m. Furthermore, the single-photon level real-time detection of CO concentration after the diffuse reflection is realized by using SPAD. This work demonstrates the merits of the up-conversion detection in terms of its functionality at room temperature and capacity for sensitivity detection. Furthermore, it presents a design and optimization methodology that has the potential to underpin the advancement of the method towards more practical applications, like industrial process monitoring, medical diagnosis and so on.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Carbon monoxide (CO) is a significant indicator gas with considerable application value in atmospheric monitoring, industrial production and medical diagnosis. Its fundamental vibrational band locates around 4.6 $\\upmu$m and has larger absorption line strength than that of overtone band, which is more suitable for the precise identification and concentration detection of CO. In this paper, the up-conversion detection is employed to convert the mid-infrared absorption signal obtained by TDLAS to the visible light band, then a silicon-based detector is utilized for detection. By which, we can achieve the highest sensitivity of 79.6 ppb under the condition of cavity-free in-situ with an absorption range length of only 0.14 m. Furthermore, the single-photon level real-time detection of CO concentration after the diffuse reflection is realized by using SPAD. This work demonstrates the merits of the up-conversion detection in terms of its functionality at room temperature and capacity for sensitivity detection. Furthermore, it presents a design and optimization methodology that has the potential to underpin the advancement of the method towards more practical applications, like industrial process monitoring, medical diagnosis and so on."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.ins-det",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:03:56Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            3,
            56,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Zhao-Qi-Zhi Han"
            },
            {
                "name": "He Zhang"
            },
            {
                "name": "Fan Yang"
            },
            {
                "name": "Xiao-Hua Wang"
            },
            {
                "name": "Bo-Wen Liu"
            },
            {
                "name": "Jin-Peng Li"
            },
            {
                "name": "Zheng-He Zhou"
            },
            {
                "name": "Yin-Hai Li"
            },
            {
                "name": "Yan Li"
            },
            {
                "name": "Zhi-Yuan Zhou"
            },
            {
                "name": "Bao-Sen Shi"
            }
        ],
        "author_detail": {
            "name": "Bao-Sen Shi"
        },
        "author": "Bao-Sen Shi",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "基于上转换检测的中红外高灵敏无腔原位CO气体传感",
        "abstract_cn": "一氧化碳（CO）是一种重要的指示气体，在大气监测、工业生产和医疗诊断等方面具有相当大的应用价值。其基振谱带位于4.6 $\\upmu$m附近，比泛谱带具有更大的吸收线强度，更适合CO的精确识别和浓度检测。本文采用上转换检测将TDLAS获得的中红外吸收信号转换到可见光波段，然后利用硅基探测器进行检测。由此，我们可以在无空腔原位条件下实现最高79.6 ppb的灵敏度，吸收范围长度仅为0.14 m。此外，利用SPAD实现了漫反射后CO浓度的单光子级实时检测。这项工作证明了上转换检测在室温下的功能和灵敏度检测能力方面的优点。此外，它还提出了一种设计和优化方法，有可能支持该方法向更实际的应用发展，例如工业过程监控、医疗诊断等。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15989v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15989v1",
        "title": "On-chip Multimode Opto-electronic Neural Network",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "On-chip Multimode Opto-electronic Neural Network"
        },
        "updated": "2026-01-22T14:12:39Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            12,
            39,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15989v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15989v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Opto-electronic computing combines the complementary strengths of photonics and electronics to deliver ultrahigh computational throughput with high energy efficiency. However, its practical deployment for real-world applications has been limited by architectures that rely on delicate wavelength management or phase-sensitive coherent detection. Here, we demonstrate the first multimode opto-electronic neural network (MOENN) on a silicon-on-insulator platform. By utilizing orthogonal waveguide eigenmodes as independent information carriers, our architecture achieves robust single-wavelength computation that is inherently immune to spectral crosstalk and phase noise. The fabricated MOENN chip monolithically integrates all functional components, including input encoders, programmable mode-division fan-in/-out units, and most importantly, the nonlinear multimode activation functions. We report the system's versatility through in-situ training via a genetic algorithm, successfully resolving the nonlinear decision boundaries of a two-class dataset and achieving 92.1% accuracy on the Iris classification benchmark. Furthermore, we reconfigure the MOENN into a one-dimensional convolutional neural network, attaining an accuracy of 90.7% on the electrocardiogram-based emotion recognition task. This work establishes a new opto-electronic computing paradigm of simple control and excellent robustness, providing a compelling path toward scalable, deployable photonic intelligence.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Opto-electronic computing combines the complementary strengths of photonics and electronics to deliver ultrahigh computational throughput with high energy efficiency. However, its practical deployment for real-world applications has been limited by architectures that rely on delicate wavelength management or phase-sensitive coherent detection. Here, we demonstrate the first multimode opto-electronic neural network (MOENN) on a silicon-on-insulator platform. By utilizing orthogonal waveguide eigenmodes as independent information carriers, our architecture achieves robust single-wavelength computation that is inherently immune to spectral crosstalk and phase noise. The fabricated MOENN chip monolithically integrates all functional components, including input encoders, programmable mode-division fan-in/-out units, and most importantly, the nonlinear multimode activation functions. We report the system's versatility through in-situ training via a genetic algorithm, successfully resolving the nonlinear decision boundaries of a two-class dataset and achieving 92.1% accuracy on the Iris classification benchmark. Furthermore, we reconfigure the MOENN into a one-dimensional convolutional neural network, attaining an accuracy of 90.7% on the electrocardiogram-based emotion recognition task. This work establishes a new opto-electronic computing paradigm of simple control and excellent robustness, providing a compelling path toward scalable, deployable photonic intelligence."
        },
        "tags": [
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:12:39Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            12,
            39,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "physics.optics"
        },
        "authors": [
            {
                "name": "Jinlong Xiang"
            },
            {
                "name": "Youlve Chen"
            },
            {
                "name": "Chaojun Xu"
            },
            {
                "name": "Yuchen Yin"
            },
            {
                "name": "Yufeng Zhang"
            },
            {
                "name": "Yikai Su"
            },
            {
                "name": "Zhipei Sun"
            },
            {
                "name": "Xuhan Guo"
            }
        ],
        "author_detail": {
            "name": "Xuhan Guo"
        },
        "author": "Xuhan Guo",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "片上多模光电神经网络",
        "abstract_cn": "光电计算结合了光子学和电子学的互补优势，可提供超高计算吞吐量和高能效。然而，其在实际应用中的实际部署受到依赖于精密波长管理或相敏相干检测的架构的限制。在这里，我们展示了绝缘体上硅平台上的第一个多模光电神经网络（MOENN）。通过利用正交波导本征模作为独立的信息载体，我们的架构实现了鲁棒的单波长计算，该计算本质上不受光谱串扰和相位噪声的影响。所制造的 MOENN 芯片单片集成了所有功能组件，包括输入编码器、可编程模分扇入/出单元，以及最重要的非线性多模激活功能。我们通过遗传算法进行原位训练来报告系统的多功能性，成功解决了两类数据集的非线性决策边界，并在鸢尾花分类基准上实现了 92.1% 的准确率。此外，我们将 MOENN 重新配置为一维卷积神经网络，在基于心电图的情绪识别任务上达到 90.7% 的准确率。这项工作建立了一种控制简单、鲁棒性优异的新型光电计算范式，为实现可扩展、可部署的光子智能提供了一条引人注目的道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16005v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16005v1",
        "title": "Critical speed of a binary superfuid of light",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Critical speed of a binary superfuid of light"
        },
        "updated": "2026-01-22T14:30:17Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            30,
            17,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16005v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16005v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We theoretically study the critical speed for superfluid flow of a two-dimensional (2D) binary superfluid of light past a polarization-sensitive optical obstacle. This speed corresponds to the maximum mean flow velocity below which dissipation is absent. In the weak-obstacle regime, linear-response theory shows that the critical speed is set by Landau's criterion applied to the density and spin Bogoliubov modes, whose relative ordering can be inverted due to saturation of the optical nonlinearity. For obstacles of arbitrary strength and large spatial extent, we determine the critical speed from the conditions for strong ellipticity of the stationary hydrodynamic equations within the hydraulic and incompressible approximations. Numerical simulations in this regime reveal that the breakdown of superfluidity is initiated by the nucleation of vortex-antivortex pairs for an impenetrable obstacle, and of Jones-Roberts soliton-type structures for a penetrable obstacle. Beyond superfluids of light, our results provide a general framework for the critical speed of 2D binary nonlinear Schrödinger superflows, including Bose-Bose quantum mixtures.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We theoretically study the critical speed for superfluid flow of a two-dimensional (2D) binary superfluid of light past a polarization-sensitive optical obstacle. This speed corresponds to the maximum mean flow velocity below which dissipation is absent. In the weak-obstacle regime, linear-response theory shows that the critical speed is set by Landau's criterion applied to the density and spin Bogoliubov modes, whose relative ordering can be inverted due to saturation of the optical nonlinearity. For obstacles of arbitrary strength and large spatial extent, we determine the critical speed from the conditions for strong ellipticity of the stationary hydrodynamic equations within the hydraulic and incompressible approximations. Numerical simulations in this regime reveal that the breakdown of superfluidity is initiated by the nucleation of vortex-antivortex pairs for an impenetrable obstacle, and of Jones-Roberts soliton-type structures for a penetrable obstacle. Beyond superfluids of light, our results provide a general framework for the critical speed of 2D binary nonlinear Schrödinger superflows, including Bose-Bose quantum mixtures."
        },
        "tags": [
            {
                "term": "cond-mat.quant-gas",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "nlin.PS",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "physics.optics",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:30:17Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            30,
            17,
            3,
            22,
            0
        ],
        "arxiv_comment": "Submitted to the Topical Collection \"Paraxial Fluids of Light\" in Eur. Phys. J. D",
        "arxiv_primary_category": {
            "term": "cond-mat.quant-gas"
        },
        "authors": [
            {
                "name": "Pierre-Élie Larré"
            },
            {
                "name": "Claire Michel"
            },
            {
                "name": "Nicolas Cherroret"
            }
        ],
        "author_detail": {
            "name": "Nicolas Cherroret"
        },
        "author": "Nicolas Cherroret",
        "journal": "arXiv: Physics - Optics",
        "title_cn": "二元光超流体的临界速度",
        "abstract_cn": "我们从理论上研究了二维（2D）二元超流体光通过偏振敏感光学障碍物的超流体流动的临界速度。该速度对应于最大平均流速，低于该速度则不存在耗散。在弱障碍物状态下，线性响应理论表明，临界速度是由应用于密度和自旋 Bogoliubov 模式的朗道准则设定的，由于光学非线性的饱和，其相对顺序可以反转。对于任意强度和大空间范围的障碍物，我们根据水力和不可压缩近似内的稳态流体动力学方程的强椭圆性条件来确定临界速度。该状态下的数值模拟表明，对于不可穿透的障碍物，超流性的破坏是由涡旋-反涡对的成核引发的，对于可穿透的障碍物，是由琼斯-罗伯茨孤子型结构的成核引发的。除了光的超流体之外，我们的结果还为二维二元非线性薛定谔超流（包括玻色-玻色量子混合物）的临界速度提供了通用框架。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15368v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15368v1",
        "title": "Aligned Stable Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Aligned Stable Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency"
        },
        "updated": "2026-01-21T17:57:18Z",
        "updated_parsed": [
            2026,
            1,
            21,
            17,
            57,
            18,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15368v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15368v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Generative image inpainting can produce realistic, high-fidelity results even with large, irregular masks. However, existing methods still face key issues that make inpainted images look unnatural. In this paper, we identify two main problems: (1) Unwanted object insertion: generative models may hallucinate arbitrary objects in the masked region that do not match the surrounding context. (2) Color inconsistency: inpainted regions often exhibit noticeable color shifts, leading to smeared textures and degraded image quality. We analyze the underlying causes of these issues and propose efficient post-hoc solutions for pre-trained inpainting models. Specifically, we introduce the principled framework of Aligned Stable inpainting with UnKnown Areas prior (ASUKA). To reduce unwanted object insertion, we use reconstruction-based priors to guide the generative model, suppressing hallucinated objects while preserving generative flexibility. To address color inconsistency, we design a specialized VAE decoder that formulates latent-to-image decoding as a local harmonization task. This design significantly reduces color shifts and produces more color-consistent results. We implement ASUKA on two representative inpainting architectures: a U-Net-based model and a DiT-based model. We analyze and propose lightweight injection strategies that minimize interference with the model's original generation capacity while ensuring the mitigation of the two issues. We evaluate ASUKA using the Places2 dataset and MISATO, our proposed diverse benchmark. Experiments show that ASUKA effectively suppresses object hallucination and improves color consistency, outperforming standard diffusion, rectified flow models, and other inpainting methods. Dataset, models and codes will be released in github.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Generative image inpainting can produce realistic, high-fidelity results even with large, irregular masks. However, existing methods still face key issues that make inpainted images look unnatural. In this paper, we identify two main problems: (1) Unwanted object insertion: generative models may hallucinate arbitrary objects in the masked region that do not match the surrounding context. (2) Color inconsistency: inpainted regions often exhibit noticeable color shifts, leading to smeared textures and degraded image quality. We analyze the underlying causes of these issues and propose efficient post-hoc solutions for pre-trained inpainting models. Specifically, we introduce the principled framework of Aligned Stable inpainting with UnKnown Areas prior (ASUKA). To reduce unwanted object insertion, we use reconstruction-based priors to guide the generative model, suppressing hallucinated objects while preserving generative flexibility. To address color inconsistency, we design a specialized VAE decoder that formulates latent-to-image decoding as a local harmonization task. This design significantly reduces color shifts and produces more color-consistent results. We implement ASUKA on two representative inpainting architectures: a U-Net-based model and a DiT-based model. We analyze and propose lightweight injection strategies that minimize interference with the model's original generation capacity while ensuring the mitigation of the two issues. We evaluate ASUKA using the Places2 dataset and MISATO, our proposed diverse benchmark. Experiments show that ASUKA effectively suppresses object hallucination and improves color consistency, outperforming standard diffusion, rectified flow models, and other inpainting methods. Dataset, models and codes will be released in github."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T17:57:18Z",
        "published_parsed": [
            2026,
            1,
            21,
            17,
            57,
            18,
            2,
            21,
            0
        ],
        "arxiv_comment": "Extension of our CVPR 2025 highlight paper: arXiv:2312.04831",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Yikai Wang"
            },
            {
                "name": "Junqiu Yu"
            },
            {
                "name": "Chenjie Cao"
            },
            {
                "name": "Xiangyang Xue"
            },
            {
                "name": "Yanwei Fu"
            }
        ],
        "author_detail": {
            "name": "Yanwei Fu"
        },
        "author": "Yanwei Fu",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "对齐稳定修复：减少不需要的对象插入并保持颜色一致性",
        "abstract_cn": "即使使用大的、不规则的蒙版，生成图像修复也可以产生逼真的高保真结果。然而，现有方法仍然面临一些关键问题，导致修复后的图像看起来不自然。在本文中，我们确定了两个主要问题：（1）不需要的对象插入：生成模型可能会产生与周围上下文不匹配的屏蔽区域中的任意对象的幻觉。 (2) 颜色不一致：修复区域通常表现出明显的颜色变化，导致纹理模糊和图像质量下降。我们分析这些问题的根本原因，并为预训练的修复模型提出有效的事后解决方案。具体来说，我们介绍了带有未知区域先验的对齐稳定修复（ASUKA）的原则框架。为了减少不需要的对象插入，我们使用基于重建的先验来指导生成模型，抑制幻觉对象，同时保留生成灵活性。为了解决颜色不一致问题，我们设计了一个专门的 VAE 解码器，将潜在图像解码制定为本地协调任务。这种设计显着减少了颜色偏移并产生颜色更加一致的结果。我们在两种代表性的修复架构上实现了 ASUKA：基于 U-Net 的模型和基于 DiT 的模型。我们分析并提出了轻量级注入策略，最大限度地减少对模型原始发电能力的干扰，同时确保缓解这两个问题。我们使用 Places2 数据集和 MISATO（我们提出的多样化基准）来评估 ASUKA。实验表明，ASUKA 有效抑制物体幻觉并提高颜色一致性，优于标准扩散、整流流模型和其他修复方法。数据集、模型和代码将在github上发布。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15369v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15369v1",
        "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"
        },
        "updated": "2026-01-21T18:47:12Z",
        "updated_parsed": [
            2026,
            1,
            21,
            18,
            47,
            12,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15369v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15369v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T18:47:12Z",
        "published_parsed": [
            2026,
            1,
            21,
            18,
            47,
            12,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Letian Zhang"
            },
            {
                "name": "Sucheng Ren"
            },
            {
                "name": "Yanqing Liu"
            },
            {
                "name": "Xianhang Li"
            },
            {
                "name": "Zeyu Wang"
            },
            {
                "name": "Yuyin Zhou"
            },
            {
                "name": "Huaxiu Yao"
            },
            {
                "name": "Zeyu Zheng"
            },
            {
                "name": "Weili Nie"
            },
            {
                "name": "Guilin Liu"
            },
            {
                "name": "Zhiding Yu"
            },
            {
                "name": "Cihang Xie"
            }
        ],
        "author_detail": {
            "name": "Cihang Xie"
        },
        "author": "Cihang Xie",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "OpenVision 3：用于理解和生成的统一视觉编码器系列",
        "abstract_cn": "本文提出了一系列先进的视觉编码器，名为 OpenVision 3，它学习单一、统一的视觉表示，可以服务于图像理解和图像生成。我们的核心架构很简单：我们将 VAE 压缩的潜在图像提供给 ViT 编码器，并训练其输出以支持两个互补的角色。首先，编码器输出被传递到 ViT-VAE 解码器以重建原始图像，从而鼓励表示捕获生成结构。其次，通过对比学习和图像描述目标优化相同的表示，从而加强语义特征。通过在共享潜在空间中联合优化重建和语义驱动的信号，编码器学习在两种机制之间协同和泛化的表示。我们通过冻结编码器的广泛下游评估来验证这种统一设计。为了实现多模态理解，我们将编码器插入 LLaVA-1.5 框架中：它的性能与标准 CLIP 视觉编码器相当（例如，SeedBench 上的 62.4 与 62.2，以及 POPE 上的 83.7 与 82.9）。对于生成，我们在 RAE 框架下对其进行测试：我们的编码器大大超过了标准的基于 CLIP 的编码器（例如，gFID：ImageNet 上的 1.89 与 2.54）。我们希望这项工作能够促进未来统一建模的研究。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15539v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15539v1",
        "title": "A Machine Vision Approach to Preliminary Skin Lesion Assessments",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Machine Vision Approach to Preliminary Skin Lesion Assessments"
        },
        "updated": "2026-01-21T23:48:59Z",
        "updated_parsed": [
            2026,
            1,
            21,
            23,
            48,
            59,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15539v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15539v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T23:48:59Z",
        "published_parsed": [
            2026,
            1,
            21,
            23,
            48,
            59,
            2,
            21,
            0
        ],
        "arxiv_comment": "6 pages, 2 figures, 2 tables",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Ali Khreis"
            },
            {
                "name": "Ro'Yah Radaideh"
            },
            {
                "name": "Quinn McGill"
            }
        ],
        "author_detail": {
            "name": "Quinn McGill"
        },
        "author": "Quinn McGill",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "初步皮肤病变评估的机器视觉方法",
        "abstract_cn": "早期发现恶性皮肤病变对于改善侵袭性、转移性皮肤癌的患者预后至关重要。本研究评估了一个用于初步皮肤病变评估的综合系统，该系统将临床建立的皮肤镜检查 ABCD 规则（分析不对称性、边界、颜色和皮肤镜结构）与机器学习分类相结合。该系统使用 HAM10000 数据集的 1,000 个图像子集，实现一个基于规则的自动化管道来计算每个病变的总皮肤镜评分 (TDS)。将这种手工方法与各种机器学习解决方案进行比较，包括传统分类器（逻辑回归、随机森林和 SVM）和深度学习模型。虽然基于规则的系统提供了较高的临床可解释性，但结果表明，将复杂的形态学简化为五个数字特征时存在性能瓶颈。实验结果表明，由于自然图像和医学图像之间的域转移，EfficientNet-B0 的迁移学习明显失败。相比之下，从头开始训练的定制三层卷积神经网络 (CNN) 在中值滤波图像上实现了 78.5% 的准确率和 86.5% 的召回率，比传统方法提高了 19 个点的准确度。结果表明，直接像素级学习可以捕获超出手工特征的诊断模式，并且针对小型、特定领域的医疗数据集，专门构建的轻量级架构可以优于大型预训练模型。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15572v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15572v1",
        "title": "FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation"
        },
        "updated": "2026-01-22T01:34:39Z",
        "updated_parsed": [
            2026,
            1,
            22,
            1,
            34,
            39,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15572v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15572v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CE",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T01:34:39Z",
        "published_parsed": [
            2026,
            1,
            22,
            1,
            34,
            39,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Jieyun Bai"
            },
            {
                "name": "Yitong Tang"
            },
            {
                "name": "Zihao Zhou"
            },
            {
                "name": "Mahdi Islam"
            },
            {
                "name": "Musarrat Tabassum"
            },
            {
                "name": "Enrique Almar-Munoz"
            },
            {
                "name": "Hongyu Liu"
            },
            {
                "name": "Hui Meng"
            },
            {
                "name": "Nianjiang Lv"
            },
            {
                "name": "Bo Deng"
            },
            {
                "name": "Yu Chen"
            },
            {
                "name": "Zilun Peng"
            },
            {
                "name": "Yusong Xiao"
            },
            {
                "name": "Li Xiao"
            },
            {
                "name": "Nam-Khanh Tran"
            },
            {
                "name": "Dac-Phu Phan-Le"
            },
            {
                "name": "Hai-Dang Nguyen"
            },
            {
                "name": "Xiao Liu"
            },
            {
                "name": "Jiale Hu"
            },
            {
                "name": "Mingxu Huang"
            },
            {
                "name": "Jitao Liang"
            },
            {
                "name": "Chaolu Feng"
            },
            {
                "name": "Xuezhi Zhang"
            },
            {
                "name": "Lyuyang Tong"
            },
            {
                "name": "Bo Du"
            },
            {
                "name": "Ha-Hieu Pham"
            },
            {
                "name": "Thanh-Huy Nguyen"
            },
            {
                "name": "Min Xu"
            },
            {
                "name": "Juntao Jiang"
            },
            {
                "name": "Jiangning Zhang"
            },
            {
                "name": "Yong Liu"
            },
            {
                "name": "Md. Kamrul Hasan"
            },
            {
                "name": "Jie Gan"
            },
            {
                "name": "Zhuonan Liang"
            },
            {
                "name": "Weidong Cai"
            },
            {
                "name": "Yuxin Huang"
            },
            {
                "name": "Gongning Luo"
            },
            {
                "name": "Mohammad Yaqub"
            },
            {
                "name": "Karim Lekadir"
            }
        ],
        "author_detail": {
            "name": "Karim Lekadir"
        },
        "author": "Karim Lekadir",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "FUGC：宫颈分割半监督学习方法的基准测试",
        "abstract_cn": "经阴道超声 (TVS) 中宫颈结构的准确分割对于评估自发性早产 (PTB) 的风险至关重要，但标记数据的缺乏限制了监督学习方法的性能。本文介绍了胎儿超声大挑战赛 (FUGC)，这是在 ISBI 2025 上主办的第一个宫颈分割半监督学习基准。FUGC 提供了 890 张 TVS 图像的数据集，其中包括 500 张训练图像、90 张验证图像和 300 张测试图像。使用 Dice 相似系数 (DSC)、豪斯多夫距离 (HD) 和运行时间 (RT) 评估方法，加权组合为 0.4/0.4/0.2。本次挑战赛吸引了 10 个团队、82 名参赛者提交创新解决方案。每个指标的最佳性能方法分别达到 90.26% mDSC、38.88 mHD 和 32.85 ms RT。 FUGC 建立了宫颈分割的标准化基准，证明了有限标记数据的半监督方法的有效性，并为人工智能辅助临床 PTB 风险评估提供了基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16011v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16011v1",
        "title": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications"
        },
        "updated": "2026-01-22T14:38:00Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            38,
            0,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16011v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16011v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:38:00Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            38,
            0,
            3,
            22,
            0
        ],
        "arxiv_comment": "25 pages",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Theodor Forgaard"
            },
            {
                "name": "Jarle H. Reksten"
            },
            {
                "name": "Anders U. Waldeland"
            },
            {
                "name": "Valerio Marsocci"
            },
            {
                "name": "Nicolas Longépé"
            },
            {
                "name": "Michael Kampffmeyer"
            },
            {
                "name": "Arnt-Børre Salberg"
            }
        ],
        "author_detail": {
            "name": "Arnt-Børre Salberg"
        },
        "author": "Arnt-Børre Salberg",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "THOR：地球观测气候和社会应用的多功能基础模型",
        "abstract_cn": "当前的地球观测基础模型在架构上是僵化的，难以应对异构传感器，并且受限于固定的斑块尺寸。这限制了它们在需要灵活计算精度权衡的现实场景中的部署。我们提出了 THOR，一种“计算自适应”基础模型，可以解决输入异构性和部署刚性问题。 THOR 是第一个统一来自 Copernicus Sentinel-1、-2 和 -3 (OLCI & SLSTR) 卫星数据的架构，在单个模型中处理其原始 10 m 至 1000 m 分辨率。我们使用新颖的随机补丁和输入图像大小策略对 THOR 进行预训练。这允许在任何补丁大小的推理中部署一组预训练的权重，从而无需重新训练即可在计算成本和特征分辨率之间进行动态权衡。我们在 THOR Pretrain（一种新的大规模多传感器数据集）上对 THOR 进行预训练，并在下游基准测试中展示了最先进的性能，特别是在 PANGEA 10% split 等数据有限的情况下，验证了 THOR 灵活的特征生成在不同的气候和社会应用中表现出色。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16064v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16064v1",
        "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation"
        },
        "updated": "2026-01-22T16:00:41Z",
        "updated_parsed": [
            2026,
            1,
            22,
            16,
            0,
            41,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16064v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16064v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization."
        },
        "tags": [
            {
                "term": "eess.IV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T16:00:41Z",
        "published_parsed": [
            2026,
            1,
            22,
            16,
            0,
            41,
            3,
            22,
            0
        ],
        "arxiv_comment": "10 pages, 7 figures",
        "arxiv_primary_category": {
            "term": "eess.IV"
        },
        "authors": [
            {
                "name": "Shams Nafisa Ali"
            },
            {
                "name": "Taufiq Hasan"
            }
        ],
        "author_detail": {
            "name": "Taufiq Hasan"
        },
        "author": "Taufiq Hasan",
        "journal": "arXiv: Computational Imaging",
        "title_cn": "Phi-SegNet：医学图像分割的相位集成监督",
        "abstract_cn": "深度学习极大地推进了医学图像分割，但在不同的成像模式和解剖结构之间实现稳健的泛化仍然是一个重大挑战。造成这一限制的一个关键因素在于现有的架构（从 CNN 到 Transformer 及其混合体）主要编码空间信息，而忽略了捕获丰富结构和纹理线索的频域表示。尽管最近很少有研究开始探索特征级别的光谱信息，但频率线索的监督级别整合（对于细粒度对象定位至关重要）仍然很大程度上尚未开发。为此，我们提出了 Phi-SegNet，这是一种基于 CNN 的架构，在架构和优化级别上融合了阶段感知信息。该网络集成了双特征掩模形成器 (BFMF) 模块和反向傅里叶注意 (RFA) 模块，前者混合相邻编码器特征以减少语义间隙，后者使用相位正则化特征细化解码器输出。专用的相位感知损失将这些特征与结构先验保持一致，形成强调边界精度的闭合反馈环。在涵盖 X 射线、US、组织病理学、MRI 和结肠镜检查的五个公共数据集上进行评估后，Phi-SegNet 始终实现了最先进的性能，与下一个表现最佳的模型相比，IoU 平均相对提高了 1.54+/-1.26%，F1 分数平均相对提高了 0.98+/-0.71%。在涉及已知领域未见过的数据集的跨数据集泛化场景中，Phi-SegNet 还表现出稳健且卓越的性能，突出了其适应性和模态不可知的设计。这些发现证明了在特征表示和监督中利用光谱先验的潜力，为擅长细粒度对象定位的广义分割框架铺平了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15392v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15392v1",
        "title": "GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation"
        },
        "updated": "2026-01-21T19:03:54Z",
        "updated_parsed": [
            2026,
            1,
            21,
            19,
            3,
            54,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15392v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15392v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            },
            {
                "rel": "related",
                "href": "https://doi.org/10.1007/978-3-032-11317-7_33",
                "title": "doi",
                "type": "text/html"
            }
        ],
        "summary": "Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN"
        },
        "tags": [
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T19:03:54Z",
        "published_parsed": [
            2026,
            1,
            21,
            19,
            3,
            54,
            2,
            21,
            0
        ],
        "arxiv_comment": "12 pages, 2 figures. Published at Image Analysis and Processing - ICIAP 2025 Workshops",
        "arxiv_primary_category": {
            "term": "cs.AI"
        },
        "authors": [
            {
                "name": "Francesca Pia Panaccione"
            },
            {
                "name": "Carlo Sgaravatti"
            },
            {
                "name": "Pietro Pinoli"
            }
        ],
        "author_detail": {
            "name": "Pietro Pinoli"
        },
        "author": "Pietro Pinoli",
        "arxiv_doi": "10.1007/978-3-032-11317-7_33",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "GeMM-GAN：以组织病理学图像和临床描述为条件的多模态生成模型，用于生成基因表达谱",
        "abstract_cn": "生物医学研究越来越依赖于整合不同的数据模式，包括基因表达谱、医学图像和临床元数据。虽然医学图像和临床元数据是在临床实践中常规收集的，但基因表达数据对广泛的研究使用提出了独特的挑战，这主要是由于严格的隐私法规和昂贵的实验室实验。为了解决这些局限性，我们提出了 GeMM-GAN，这是一种以组织病理学组织切片和临床元数据为条件的新型生成对抗网络，旨在合成真实的基因表达谱。 GeMM-GAN 将图像补丁的 Transformer 编码器与补丁和文本标记之间的最终交叉注意力机制相结合，生成条件向量来指导生成模型生成生物相干的基因表达谱。我们在 TCGA 数据集上评估了我们的方法，并证明我们的框架优于标准生成模型，并生成更真实且具有功能意义的基因表达谱，与当前最先进的生成模型相比，下游疾病类型预测的准确性提高了 11% 以上。代码位于：https://github.com/francescapia/GeMM-GAN"
    },
    {
        "id": "http://arxiv.org/abs/2601.15406v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15406v1",
        "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition"
        },
        "updated": "2026-01-21T19:17:21Z",
        "updated_parsed": [
            2026,
            1,
            21,
            19,
            17,
            21,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15406v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15406v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T19:17:21Z",
        "published_parsed": [
            2026,
            1,
            21,
            19,
            17,
            21,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Hatef Otroshi Shahreza"
            },
            {
                "name": "Anjith George"
            },
            {
                "name": "Sébastien Marcel"
            }
        ],
        "author_detail": {
            "name": "Sébastien Marcel"
        },
        "author": "Sébastien Marcel",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "评估异构人脸识别的多模态大语言模型",
        "abstract_cn": "多模态大型语言模型 (MLLM) 最近在各种视觉语言任务中表现出了强大的性能，引起了人们对其在生物识别应用中的潜在用途的兴趣。在本文中，我们对用于异构人脸识别 (HFR) 的最先进的 MLLM 进行了系统评估，其中登记和探测图像来自不同的传感模式，包括视觉 (VIS)、近红外 (NIR)、短波红外 (SWIR) 和热像仪。我们在多个跨模态场景中对多个开源 MLLM 进行基准测试，包括 VIS-NIR、VIS-SWIR 和 VIS-THERMAL 人脸识别。 MLLM 的识别性能使用生物识别协议并基于不同的指标进行评估，包括获取率、等错误率 (EER) 和真实接受率 (TAR)。我们的结果揭示了 MLLM 和经典人脸识别系统之间存在巨大的性能差距，特别是在具有挑战性的跨光谱条件下，尽管 MLLM 最近取得了进展。我们的研究结果强调了当前 MLLM 在 HFR 方面的局限性，以及在考虑将其部署在人脸识别系统中时严格的生物识别评估的重要性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15416v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15416v1",
        "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction"
        },
        "updated": "2026-01-21T19:27:47Z",
        "updated_parsed": [
            2026,
            1,
            21,
            19,
            27,
            47,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15416v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15416v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T19:27:47Z",
        "published_parsed": [
            2026,
            1,
            21,
            19,
            27,
            47,
            2,
            21,
            0
        ],
        "arxiv_comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Cuong Tran Van"
            },
            {
                "name": "Trong-Thang Pham"
            },
            {
                "name": "Ngoc-Son Nguyen"
            },
            {
                "name": "Duy Minh Ho Nguyen"
            },
            {
                "name": "Ngan Le"
            }
        ],
        "author_detail": {
            "name": "Ngan Le"
        },
        "author": "Ngan Le",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DuFal：用于高保真极稀疏视图 CBCT 重建的双频感知学习",
        "abstract_cn": "由于与高频分量相对应的细粒度解剖细节固有的欠采样，基于有限 X 射线投影的稀疏视图锥形束计算机断层扫描重建仍然是医学成像中的一个具有挑战性的问题。传统的基于 CNN 的方法通常很难恢复这些精细结构，因为它们通常偏向于学习低频信息。为了应对这一挑战，本文提出了 DuFal（双频感知学习），这是一种通过双路径架构集成频域和空间域处理的新颖框架。核心创新在于我们的高局部分解傅里叶神经算子，它由两个互补的分支组成：捕获全局频率模式的全局高频增强傅里叶神经算子和处理空间分区补丁以保留全局频率分析中可能丢失的空间局部性的局部高频增强傅里叶神经算子。为了提高效率，我们设计了一种频谱通道分解方案，以减少傅立叶神经算子参数数量。我们还设计了一个交叉注意力频率融合模块来有效地整合空间和频率特征。然后，融合的特征通过特征解码器进行解码，以产生投影表示，随后通过强度场解码管道进行处理，以重建最终的计算机断层扫描体积。 LUNA16 和 ToothFairy 数据集上的实验结果表明，DuFal 在保留高频解剖特征方面显着优于现有的最先进方法，特别是在极其稀疏的视图设置下。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15441v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15441v1",
        "title": "CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models"
        },
        "updated": "2026-01-21T20:14:17Z",
        "updated_parsed": [
            2026,
            1,
            21,
            20,
            14,
            17,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15441v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15441v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models."
        },
        "tags": [
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T20:14:17Z",
        "published_parsed": [
            2026,
            1,
            21,
            20,
            14,
            17,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.LG"
        },
        "authors": [
            {
                "name": "Zhenghao He"
            },
            {
                "name": "Guangzhi Xiong"
            },
            {
                "name": "Boyang Wang"
            },
            {
                "name": "Sanchit Sinha"
            },
            {
                "name": "Aidong Zhang"
            }
        ],
        "author_detail": {
            "name": "Aidong Zhang"
        },
        "author": "Aidong Zhang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "CASL：用于解释扩散模型的概念对齐稀疏潜伏",
        "abstract_cn": "扩散模型的内部激活编码丰富的语义信息，但解释此类表示仍然具有挑战性。虽然稀疏自动编码器 (SAE) 在解开潜在表示方面表现出了良好的前景，但现有的基于 SAE 的扩散模型理解方法依赖于无监督方法，而这些方法无法将稀疏特征与人类可理解的概念结合起来。这限制了他们对生成的图像提供可靠的语义控制的能力。我们引入了 CASL（概念对齐稀疏潜在），这是一种监督框架，它将扩散模型的稀疏潜在维度与语义概念对齐。 CASL 首先在冻结的 U-Net 激活上训练 SAE，以获得解开的潜在表示，然后学习轻量级线性映射，将每个概念与一小组相关潜在维度相关联。为了验证这些对齐方向的语义意义，我们提出了 CASL-Steer，这是一种受控的潜在干预，可以沿着学习的概念轴改变激活。与编辑方法不同，CASL-Steer 仅用作因果探测，以揭示概念对齐的潜在因素如何影响生成的内容。我们进一步介绍了编辑精确率（EPR），这是一种联合衡量概念特异性和不相关属性保留的指标。实验表明，与现有方法相比，我们的方法实现了卓越的编辑精度和可解释性。据我们所知，这是第一个在扩散模型中实现潜在表示和语义概念之间的监督对齐的工作。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15453v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15453v1",
        "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection"
        },
        "updated": "2026-01-21T20:35:51Z",
        "updated_parsed": [
            2026,
            1,
            21,
            20,
            35,
            51,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15453v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15453v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T20:35:51Z",
        "published_parsed": [
            2026,
            1,
            21,
            20,
            35,
            51,
            2,
            21,
            0
        ],
        "arxiv_comment": "8 pages",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Morteza Poudineh"
            },
            {
                "name": "Marc Lalonde"
            }
        ],
        "author_detail": {
            "name": "Marc Lalonde"
        },
        "author": "Marc Lalonde",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DevPrompt：用于单法线镜头图像异常检测的基于偏差的提示学习",
        "abstract_cn": "少正常镜头异常检测（FNSAD）旨在仅使用少量正常训练样本来检测图像中的异常区域，由于有限的监督和潜在缺陷的多样性，使得该任务极具挑战性。最近的方法利用视觉语言模型（例如 CLIP）和基于提示的学习来对齐图像和文本特征。然而，现有方法通常对正常和异常提示的区分能力较弱，并且缺乏针对补丁级异常的原则性评分机制。我们提出了一种偏差引导的即时学习框架，它将视觉语言模型的语义能力与基于偏差的评分的统计可靠性相结合。具体来说，我们用在正常和异常提示之间共享的可学习上下文向量替换固定提示前缀，而特定于异常的后缀标记启用类感知对齐。为了增强可分离性，我们引入了 Top-K 多实例学习 (MIL) 的偏差损失，将块级特征建模为与正态分布的高斯偏差。这使得网络能够为具有统计显着偏差的补丁分配更高的异常分数，从而提高定位和可解释性。 MVTecAD 和 VISA 基准测试的实验表明，与 PromptAD 和其他基准相比，具有卓越的像素级检测性能。消融研究进一步验证了可学习提示、基于偏差的评分和 Top-K MIL 策略的有效性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15475v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15475v1",
        "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events"
        },
        "updated": "2026-01-21T21:25:58Z",
        "updated_parsed": [
            2026,
            1,
            21,
            21,
            25,
            58,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15475v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15475v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T21:25:58Z",
        "published_parsed": [
            2026,
            1,
            21,
            21,
            25,
            58,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yunshan Qi"
            },
            {
                "name": "Lin Zhu"
            },
            {
                "name": "Nan Bao"
            },
            {
                "name": "Yifan Zhao"
            },
            {
                "name": "Jia Li"
            }
        ],
        "author_detail": {
            "name": "Jia Li"
        },
        "author": "Jia Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "透视光明与黑暗：基于传感器物理的单次曝光图像和事件去模糊 HDR NeRF",
        "abstract_cn": "在野外常见的低动态范围 (LDR) 模糊图像的新颖视图合成很难在极端照明条件下恢复高动态范围 (HDR) 和清晰的 3D 表示。尽管现有方法采用事件数据来解决此问题，但它们忽略了相机输出和物理世界辐射亮度之间的传感器物理不匹配，从而导致 HDR 和去模糊结果不理想。为了解决这个问题，我们提出了一个基于传感器物理的统一 NeRF 框架，用于从单曝光模糊 LDR 图像和相应事件合成清晰的 HDR 新颖视图。我们使用 NeRF 直接表示 HDR 域中 3D 场景的实际辐射亮度，并对照射到传感器像素的原始 HDR 场景光线进行建模，就像在物理世界中一样。引入逐像素 RGB 映射字段，将上述渲染的像素值与传感器记录的输入图像的 LDR 像素值对齐。还设计了一个新颖的事件映射字段来桥接物理场景动态和实际事件传感器输出。这两个映射领域与 NeRF 网络联合优化，利用事件中的空间和时间动态信息来增强锐利的 HDR 3D 表示学习。对收集的公共数据集进行的实验表明，我们的方法可以通过单曝光模糊 LDR 图像和相应事件实现最先进的去模糊 HDR 新颖视图合成结果。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15490v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15490v1",
        "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis"
        },
        "updated": "2026-01-21T21:43:19Z",
        "updated_parsed": [
            2026,
            1,
            21,
            21,
            43,
            19,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15490v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15490v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T21:43:19Z",
        "published_parsed": [
            2026,
            1,
            21,
            21,
            43,
            19,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jobeal Solomon"
            },
            {
                "name": "Ali Mohammed Mansoor Alsahag"
            },
            {
                "name": "Seyed Sahand Mohammadi Ziabari"
            }
        ],
        "author_detail": {
            "name": "Seyed Sahand Mohammadi Ziabari"
        },
        "author": "Seyed Sahand Mohammadi Ziabari",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "混合视觉 Transformer_GAN 属性中和器，用于减轻胸部 X 射线诊断中的偏差",
        "abstract_cn": "胸部 X 光分类器的偏差常常源于与性别和年龄相关的捷径，导致少数亚组的系统性诊断不足。以前的像素空间属性中和器依赖于卷积编码器，在临床可用的编辑强度下减少但没有完全消除这种属性泄漏。本研究评估在属性中性框架中用 Vision Transformer 主干替代 U-Net 卷积编码器是否可以减少人口统计属性泄漏，同时保持诊断准确性。在 ChestX-ray14 数据集上训练了数据高效的 Image Transformer Small (DeiT-S) 中和器。其编辑后的图像在 11 个编辑强度级别上生成，并由独立的人工智能法官评估属性泄漏，并使用卷积神经网络 (ConvNet) 进行疾病预测。在中等编辑水平（alpha = 0.5）下，Vision Transformer (ViT) 中和器将患者性别识别曲线下面积 (AUC) 降低至约 0.80，比原始框架的卷积 U-Net 编码器低约 10 个百分点，尽管训练次数仅为原来框架的一半。与此同时，15 个研究结果的宏观受试者工作特征曲线下面积 (ROC AUC) 与未经编辑的基线相比保持在 5 个百分点以内，最坏情况亚组 AUC 仍接近 0.70。这些结果表明，全局自注意力视觉模型可以在不牺牲临床实用性的情况下进一步抑制属性泄漏，这为实现更公平的胸部 X 射线人工智能提供了一条实用途径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15507v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15507v1",
        "title": "Controllable Layered Image Generation for Real-World Editing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Controllable Layered Image Generation for Real-World Editing"
        },
        "updated": "2026-01-21T22:29:33Z",
        "updated_parsed": [
            2026,
            1,
            21,
            22,
            29,
            33,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15507v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15507v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T22:29:33Z",
        "published_parsed": [
            2026,
            1,
            21,
            22,
            29,
            33,
            2,
            21,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jinrui Yang"
            },
            {
                "name": "Qing Liu"
            },
            {
                "name": "Yijun Li"
            },
            {
                "name": "Mengwei Ren"
            },
            {
                "name": "Letian Zhang"
            },
            {
                "name": "Zhe Lin"
            },
            {
                "name": "Cihang Xie"
            },
            {
                "name": "Yuyin Zhou"
            }
        ],
        "author_detail": {
            "name": "Yuyin Zhou"
        },
        "author": "Yuyin Zhou",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于现实世界编辑的可控分层图像生成",
        "abstract_cn": "最近的图像生成模型已经显示出令人印象深刻的进步，但当用户尝试编辑现有图像中的特定元素时，它们通常难以产生可控且一致的结果。分层表示可以实现灵活的、用户驱动的内容创建，但现有方法通常无法生成具有连贯合成关系的图层，并且它们的对象图层通常缺乏真实的视觉效果，例如阴影和反射。为了克服这些限制，我们提出了 LASAGNA，这是一种新颖的、统一的框架，可以与其合成层联合生成图像——逼真的背景和具有引人注目的视觉效果的高质量透明前景。与之前的工作不同，LASAGNA 可以从各种条件输入（文本提示、前景、背景和位置蒙版）中有效地学习正确的图像构成，为现实世界的应用提供更大的可控性。为了实现这一点，我们引入了 LASAGNA-48K，这是一个由干净背景和 RGBA 前景组成的新数据集，具有物理基础的视觉效果。我们还提出了 LASAGNABENCH，这是图层编辑的第一个基准。我们证明，LASAGNA 擅长同时跨多个图像层生成高度一致和连贯的结果，从而支持准确保留身份和视觉效果的各种后期编辑应用程序。 LASAGNA-48K 和 LASAGNABENCH 将公开发布，以促进社区的开放研究。项目页面为 https://rayjryang.github.io/LASAGNA-Page/。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15516v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15516v1",
        "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views"
        },
        "updated": "2026-01-21T23:00:43Z",
        "updated_parsed": [
            2026,
            1,
            21,
            23,
            0,
            43,
            2,
            21,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15516v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15516v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.HC",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-21T23:00:43Z",
        "published_parsed": [
            2026,
            1,
            21,
            23,
            0,
            43,
            2,
            21,
            0
        ],
        "arxiv_comment": "16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "William Huang"
            },
            {
                "name": "Siyou Pei"
            },
            {
                "name": "Leyi Zou"
            },
            {
                "name": "Eric J. Gonzalez"
            },
            {
                "name": "Ishan Chatterjee"
            },
            {
                "name": "Yang Zhang"
            }
        ],
        "author_detail": {
            "name": "Yang Zhang"
        },
        "author": "Yang Zhang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DeltaDorsal：利用自我中心视图中的背部特征增强手部姿势估计",
        "abstract_cn": "XR 设备的激增使得以自我为中心的手势估计成为一项重要任务，但这种观点本质上受到频繁的手指遮挡的挑战。为了解决这个问题，我们提出了一种新颖的方法，利用密集视觉特征器的最新进展解锁的手背皮肤变形的丰富信息。我们引入了一种双流增量编码器，它通过对比动态手与基线放松位置的特征来学习姿势。我们的评估表明，与依赖于整个手的几何形状和大型模型主干的最先进技术相比，仅使用裁剪的背部图像，我们的方法在自遮挡场景（手指 >= 50% 遮挡）中将平均每关节角度误差 (MPJAE) 降低了 18%。因此，我们的方法不仅增强了下游任务的可靠性，例如在遮挡场景中食指捏合和敲击估计，而且还解锁了新的交互范例，例如在没有可见移动的情况下检测表面“点击”的等距力，同时最小化模型尺寸。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15560v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15560v1",
        "title": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation"
        },
        "updated": "2026-01-22T00:58:59Z",
        "updated_parsed": [
            2026,
            1,
            22,
            0,
            58,
            59,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15560v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15560v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T00:58:59Z",
        "published_parsed": [
            2026,
            1,
            22,
            0,
            58,
            59,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sylvey Lin"
            },
            {
                "name": "Eranki Vasistha"
            }
        ],
        "author_detail": {
            "name": "Eranki Vasistha"
        },
        "author": "Eranki Vasistha",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "相对分类准确性：细粒度 K-pop 人脸生成中身份一致性的校准指标",
        "abstract_cn": "去噪扩散概率模型（DDPM）在高保真图像生成方面取得了显着的成功。然而，评估它们的语义可控性（特别是针对细粒度、单域任务）仍然具有挑战性。 FID 和初始分数 (IS) 等标准指标通常无法检测此类专门环境中的身份错位。在这项工作中，我们研究了用于 K-pop 偶像脸部生成 (32x32) 的类条件 DDPM，这是一个以类间高度相似性为特征的领域。我们提出了一个校准指标，即相对分类精度（RCA），它根据预言机分类器的基线标准化生成性能。我们的评估揭示了一个关键的权衡：虽然该模型实现了高视觉质量（FID 8.93），但它遭受了严重的语义模式崩溃（RCA 0.27），特别是对于视觉上模糊的身份。我们通过混淆矩阵分析这些失败模式，并将其归因于分辨率限制和性别内部模糊性。我们的框架为验证条件生成模型中的身份一致性提供了严格的标准。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15624v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15624v1",
        "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images"
        },
        "updated": "2026-01-22T03:55:46Z",
        "updated_parsed": [
            2026,
            1,
            22,
            3,
            55,
            46,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15624v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15624v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T03:55:46Z",
        "published_parsed": [
            2026,
            1,
            22,
            3,
            55,
            46,
            3,
            22,
            0
        ],
        "arxiv_comment": "Accepted at ICASSP 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ning Jiang"
            },
            {
                "name": "Dingheng Zeng"
            },
            {
                "name": "Yanhong Liu"
            },
            {
                "name": "Haiyang Yi"
            },
            {
                "name": "Shijie Yu"
            },
            {
                "name": "Minghe Weng"
            },
            {
                "name": "Haifeng Shen"
            },
            {
                "name": "Ying Li"
            }
        ],
        "author_detail": {
            "name": "Ying Li"
        },
        "author": "Ying Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用 RL 增强型自混合图像进行可解释的 Deepfake 检测",
        "abstract_cn": "大多数现有的深度伪造检测方法缺乏可解释的输出。随着人们对多模态大语言模型（MLLM）的兴趣日益浓厚，研究人员开始探索它们在可解释的深度伪造检测中的应用。然而，将 MLLM 应用于此任务的一个主要障碍是缺乏具有详细伪造属性注释的高质量数据集，因为文本注释既昂贵又具有挑战性 - 特别是对于高保真伪造图像或视频。此外，多项研究表明强化学习（RL）可以显着提高视觉任务的表现，特别是在提高跨领域泛化能力方面。为了促进在 Deepfake 检测中采用主流 MLLM 框架并降低注释成本，并研究 RL 在这方面的潜力，我们提出了一种基于自混合图像的自动化思想链（CoT）数据生成框架，以及 RL 增强的 Deepfake 检测框架。大量的实验验证了我们的 CoT 数据构建管道、定制奖励机制和反馈驱动的合成数据生成方法的有效性。我们的方法在多个跨数据集基准测试中实现了与最先进 (SOTA) 方法相媲美的性能。实施细节可在 https://github.com/deon1219/rlsbi 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15643v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15643v1",
        "title": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception"
        },
        "updated": "2026-01-22T04:45:28Z",
        "updated_parsed": [
            2026,
            1,
            22,
            4,
            45,
            28,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15643v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15643v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T04:45:28Z",
        "published_parsed": [
            2026,
            1,
            22,
            4,
            45,
            28,
            3,
            22,
            0
        ],
        "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2407.14242",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Bo Yuan"
            },
            {
                "name": "Danpei Zhao"
            },
            {
                "name": "Wentao Li"
            },
            {
                "name": "Tian Li"
            },
            {
                "name": "Zhiguo Jiang"
            }
        ],
        "author_detail": {
            "name": "Zhiguo Jiang"
        },
        "author": "Zhiguo Jiang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "不断进化：统一多模态增量学习以实现持续的全景感知",
        "abstract_cn": "持续学习（CL）是开发智能感知人工智能系统的伟大努力。然而，先驱研究主要集中在单任务 CL 上，这限制了多任务和多模式场景中的潜力。除了众所周知的灾难性遗忘问题之外，多任务 CL 还带来了多模态对齐的语义混淆，导致增量训练步骤中模型严重退化。在本文中，我们将 CL 扩展到连续全景感知（CPP），集成多模态和多任务 CL，通过像素级、实例级和图像级联合解释来增强综合图像感知。我们在多模态场景中形式化了 CL 任务，并提出了一种端到端的连续全景感知模型。具体来说，CPP 模型具有用于多模态嵌入的协作跨模态编码器（CCE）。我们还通过对比特征蒸馏和实例蒸馏提出了可延展的知识继承模块，解决任务交互提升方式中的灾难性遗忘问题。此外，我们提出了跨模态一致性约束并开发了CPP+，确保多任务增量场景下模型更新的多模态语义对齐。此外，我们提出的模型采用了非对称伪标记方式，使模型能够在没有样本重放的情况下进化。对多模态数据集和各种 CL 任务的大量实验证明了所提出模型的优越性，特别是在细粒度 CL 任务中。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15664v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15664v1",
        "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling"
        },
        "updated": "2026-01-22T05:23:20Z",
        "updated_parsed": [
            2026,
            1,
            22,
            5,
            23,
            20,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15664v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15664v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T05:23:20Z",
        "published_parsed": [
            2026,
            1,
            22,
            5,
            23,
            20,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Hongyang Wei"
            },
            {
                "name": "Hongbo Liu"
            },
            {
                "name": "Zidong Wang"
            },
            {
                "name": "Yi Peng"
            },
            {
                "name": "Baixin Xu"
            },
            {
                "name": "Size Wu"
            },
            {
                "name": "Xuying Zhang"
            },
            {
                "name": "Xianglong He"
            },
            {
                "name": "Zexiang Liu"
            },
            {
                "name": "Peiyu Wang"
            },
            {
                "name": "Xuchen Song"
            },
            {
                "name": "Yangguang Li"
            },
            {
                "name": "Yang Liu"
            },
            {
                "name": "Yahui Zhou"
            }
        ],
        "author_detail": {
            "name": "Yahui Zhou"
        },
        "author": "Yahui Zhou",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Skywork UniPic 3.0：通过序列建模统一多图像合成",
        "abstract_cn": "Nano-Banana 和 Seedream 4.0 最近的流行度突显了社区对多图像合成任务的浓厚兴趣。与单图像编辑相比，多图像合成在一致性和质量方面提出了更大的挑战，但现有模型尚未公开实现高质量融合的具体方法细节。通过统计分析，我们将人机交互（HOI）确定为社区最抢手的类别。因此，我们系统地分析和实施了最先进的多图像合成解决方案，主要关注以 HOI 为中心的任务。我们推出了 Skywork UniPic 3.0，这是一个集成了单图像编辑和多图像合成的统一多模式框架。我们的模型支持任意 (1~6) 数量和分辨率的输入图像，以及任意输出分辨率（在 1024x1024 的总像素预算内）。为了解决多图像合成的挑战，我们设计了全面的数据收集、过滤和合成管道，仅用 70 万个高质量训练样本即可实现强大的性能。此外，我们引入了一种新颖的训练范例，将多图像合成表述为序列建模问题，将条件生成转化为统一的序列合成。为了加速推理，我们将轨迹映射和分布匹配集成到训练后阶段，使模型只需 8 个步骤即可生成高保真样本，并比标准合成采样实现 12.5 倍的加速。 Skywork UniPic 3.0 在单图像编辑基准上实现了最先进的性能，并在多图像合成基准上超越了 Nano-Banana 和 Seedream 4.0，从而验证了我们的数据管道和训练范例的有效性。代码、模型和数据集是公开的。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15698v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15698v1",
        "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs"
        },
        "updated": "2026-01-22T06:56:27Z",
        "updated_parsed": [
            2026,
            1,
            22,
            6,
            56,
            27,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15698v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15698v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T06:56:27Z",
        "published_parsed": [
            2026,
            1,
            22,
            6,
            56,
            27,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Mingyu Yu"
            },
            {
                "name": "Lana Liu"
            },
            {
                "name": "Zhehao Zhao"
            },
            {
                "name": "Wei Wang"
            },
            {
                "name": "Sujuan Qin"
            }
        ],
        "author_detail": {
            "name": "Sujuan Qin"
        },
        "author": "Sujuan Qin",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "超越视觉安全：通过与语义无关的输入生成有害图像的越狱多模态大型语言模型",
        "abstract_cn": "多模态大型语言模型 (MLLM) 的快速发展带来了复杂的安全挑战，特别是在文本和视觉安全的交叉点。虽然现有方案已经探索了 MLLM 的安全漏洞，但对其视觉安全边界的调查仍然不足。在本文中，我们提出了超越视觉安全（BVS），这是一种新颖的图像文本对越狱框架，专门用于探测 MLLM 的视觉安全边界。 BVS 采用“重建然后生成”策略，利用中和视觉拼接和归纳重组将恶意意图与原始输入分离，从而导致 MLLM 被诱导生成有害图像。实验结果表明，BVS 对 GPT-5（2026 年 1 月 12 日发布）的越狱成功率达到了 98.21%。我们的研究结果暴露了当前 MLLM 视觉安全调整中的关键漏洞。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15711v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15711v1",
        "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework"
        },
        "updated": "2026-01-22T07:33:41Z",
        "updated_parsed": [
            2026,
            1,
            22,
            7,
            33,
            41,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15711v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15711v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T07:33:41Z",
        "published_parsed": [
            2026,
            1,
            22,
            7,
            33,
            41,
            3,
            22,
            0
        ],
        "arxiv_comment": "Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Shubham Shukla"
            },
            {
                "name": "Kunal Sonalkar"
            }
        ],
        "author_detail": {
            "name": "Kunal Sonalkar"
        },
        "author": "Kunal Sonalkar",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用视觉语言模型进行零样本产品属性标记：三层评估框架",
        "abstract_cn": "细粒度的属性预测对于时尚零售应用至关重要，包括目录丰富、视觉搜索和推荐系统。视觉语言模型（VLM）无需特定任务训练即可提供零样本预测，但其对多属性时尚任务的系统评估仍未得到充分探索。一个关键的挑战是时尚属性通常是有条件的。例如，当没有可见的外衣时，“外层织物”是未定义的。这需要模型在尝试分类之前检测属性的适用性。我们引入了一个三层评估框架来分解这一挑战：（1）所有属性的所有类（包括 NA 类：建议属性不适用）的总体任务性能，（2）属性适用性检测，以及（3）当属性可确定时的细粒度分类。使用 DeepFashion-MultiModal 在属性标签空间中明确定义 NA（表示属性不存在或不可见），我们对涵盖旗舰（GPT-5、Gemini 2.5 Pro）、高效（GPT-5 Mini、Gemini 2.5 Flash）和超高效层（GPT-5 Nano、Gemini 2.5 Flash-Lite）的 9 个 VLM 与在预训练的 Fashion-CLIP 嵌入上训练的分类器进行了基准测试涵盖 18 个属性的 5,000 张图像。我们的研究结果表明：(1) 零样本 VLM 实现了 64.0% 的宏 F1，比预训练 Fashion-CLIP 嵌入的逻辑回归提高了三倍； (2) VLM 擅长细粒度分类（第 3 层：70.8% F1），但在适用性检测方面遇到困难（第 2 层：34.1% NA-F1），识别关键瓶颈； (3)高效机型以更低的成本实现旗舰机90%以上的性能，提供实用的部署路径。该诊断框架使从业人员能够查明错误是否源于可见性检测或分类，从而指导生产系统的有针对性的改进。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15731v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15731v1",
        "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging"
        },
        "updated": "2026-01-22T07:57:27Z",
        "updated_parsed": [
            2026,
            1,
            22,
            7,
            57,
            27,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15731v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15731v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T07:57:27Z",
        "published_parsed": [
            2026,
            1,
            22,
            7,
            57,
            27,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Linyong Zou"
            },
            {
                "name": "Liang Zhang"
            },
            {
                "name": "Xiongfei Wang"
            },
            {
                "name": "Jia-Hong Gao"
            },
            {
                "name": "Yi Sun"
            },
            {
                "name": "Shurong Sheng"
            },
            {
                "name": "Kuntao Xiao"
            },
            {
                "name": "Wanli Yang"
            },
            {
                "name": "Pengfei Teng"
            },
            {
                "name": "Guoming Luan"
            },
            {
                "name": "Zhao Lv"
            },
            {
                "name": "Zikang Xu"
            }
        ],
        "author_detail": {
            "name": "Zikang Xu"
        },
        "author": "Zikang Xu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "FAIR-ESI：电生理源成像的特征自适应重要性细化",
        "abstract_cn": "诊断脑部疾病的一项重要技术是电生理源成像（ESI）。虽然基于模型的优化和深度学习方法在该领域取得了可喜的成果，但特征的准确选择和细化仍然是精确 ESI 的核心挑战。本文提出了 FAIR-ESI，这是一种新颖的框架，可以在不同视图中自适应地细化特征重要性，包括基于 FFT 的频谱特征细化、加权时间特征细化和基于自注意力的 patch-wise 特征细化。对两个具有不同配置的模拟数据集和两个真实世界的临床数据集进行的广泛实验验证了我们的框架的功效，突出了其推进脑部疾病诊断并提供对大脑功能的新见解的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15734v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15734v1",
        "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation"
        },
        "updated": "2026-01-22T08:03:17Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            3,
            17,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15734v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15734v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:03:17Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            3,
            17,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Shadi Alijani"
            },
            {
                "name": "Fereshteh Aghaee Meibodi"
            },
            {
                "name": "Homayoun Najjaran"
            }
        ],
        "author_detail": {
            "name": "Homayoun Najjaran"
        },
        "author": "Homayoun Najjaran",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "多模态脑肿瘤分割的子区域感知模态融合和自适应提示",
        "abstract_cn": "基础模型成功适应多模态医学成像是一个关键但尚未解决的挑战。现有模型通常难以有效地融合多个来源的信息并适应病理组织的异质性。为了解决这个问题，我们引入了一种使基础模型适应多模态医学成像的新颖框架，具有两项关键技术创新：子区域感知模态注意和自适应提示工程。注意力机制使模型能够学习每个肿瘤子区域的最佳模式组合，而自适应提示策略则利用基础模型的固有功能来提高分割精度。我们在 BraTS 2020 脑肿瘤分割数据集上验证了我们的框架，证明我们的方法显着优于基线方法，特别是在具有挑战性的坏死核心子区域。我们的工作为多模态融合和提示提供了一种有原则且有效的方法，为医学成像中更准确、更强大的基于基础模型的解决方案铺平了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15739v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15739v1",
        "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework"
        },
        "updated": "2026-01-22T08:07:10Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            7,
            10,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15739v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15739v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:07:10Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            7,
            10,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xinjue Hu"
            },
            {
                "name": "Chi Wang"
            },
            {
                "name": "Boyu Wang"
            },
            {
                "name": "Xiang Zhang"
            },
            {
                "name": "Zhenshan Tan"
            },
            {
                "name": "Zhangjie Fu"
            }
        ],
        "author_detail": {
            "name": "Zhangjie Fu"
        },
        "author": "Zhangjie Fu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "打破分辨率障碍：任意分辨率深度图像隐写框架",
        "abstract_cn": "深度图像隐写术（DIS）在容量和隐秘性方面取得了显着的成果。然而，当前的范例强制秘密图像在隐藏和显示期间保持与封面图像相同的分辨率。这带来了两个挑战：分辨率不一致的秘密图像必须事先进行重采样，这会导致恢复过程中细节丢失；并且当分辨率值未知时，秘密图像无法恢复到原始分辨率。为了解决这些问题，我们提出了 ARDIS，这是第一个任意分辨率 DIS 框架，它将范式从离散映射转变为参考引导的连续信号重建。具体来说，为了最大限度地减少分辨率不匹配造成的细节损失，我们首先在隐藏阶段设计了频率解耦架构。它将秘密分解为与分辨率一致的全局基础和隐藏在固定分辨率覆盖物中的与分辨率无关的高频潜力。其次，对于恢复，我们提出了一个潜在引导隐式重建器来执行确定性恢复。恢复的细节潜在代码调制连续隐式函数，以准确查询高频残差并将其渲染到恢复的全局基础上，确保忠实恢复原始细节。此外，为了实现盲恢复，我们引入了隐式解析编码策略。通过将离散分辨率值转换为密集特征图并将其隐藏在特征域的冗余空间中，重建器可以直接从隐写表示中正确解码秘密的分辨率。实验结果表明，ARDIS 在隐形性和跨分辨率恢复保真度方面均显着优于最先进的方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15757v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15757v1",
        "title": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification"
        },
        "updated": "2026-01-22T08:48:01Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            48,
            1,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15757v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15757v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:48:01Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            48,
            1,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yimin Zhu"
            },
            {
                "name": "Lincoln Linlin Xu"
            },
            {
                "name": "Zhengsen Xu"
            },
            {
                "name": "Zack Dewis"
            },
            {
                "name": "Mabel Heffring"
            },
            {
                "name": "Saeid Taleghanidoozdoozan"
            },
            {
                "name": "Motasem Alkayid"
            },
            {
                "name": "Quinn Ledingham"
            },
            {
                "name": "Megan Greenwood"
            }
        ],
        "author_detail": {
            "name": "Megan Greenwood"
        },
        "author": "Megan Greenwood",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "白盒 mHC：用于高光谱图像分类的电磁频谱感知和可解释流交互",
        "abstract_cn": "在高光谱图像分类（HSIC）中，大多数深度学习模型依赖于不透明的光谱空间特征混合，限制了它们的可解释性并阻碍了对内部决策机制的理解。我们提出了物理频谱感知白盒 mHC，名为 ES-mHC，这是一个超连接框架，它使用结构化的方向矩阵显式地模拟不同电磁频谱分组（mHC 中的剩余流）之间的交互。通过将特征表示与交互结构分离，ES-mHC促进电磁频谱分组专业化，减少冗余，并暴露可直接可视化和空间分析的内部信息流。使用高光谱图像分类作为代表性测试平台，我们证明了学习的超连接矩阵表现出连贯的空间模式和不对称的交互行为，为模型内部动态提供了机制洞察。此外，我们发现增加扩张率加速了结构化交互模式的出现。这些结果表明，ES-mHC 将 HSIC 从纯粹的黑盒预测任务转变为结构透明、部分白盒的学习过程。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15759v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15759v1",
        "title": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)"
        },
        "updated": "2026-01-22T08:49:33Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            49,
            33,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15759v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15759v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:49:33Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            49,
            33,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Qi Zeng"
            },
            {
                "name": "Weide Liu"
            },
            {
                "name": "Bo Li"
            },
            {
                "name": "Ryne Didier"
            },
            {
                "name": "P. Ellen Grant"
            },
            {
                "name": "Davood Karimi"
            }
        ],
        "author_detail": {
            "name": "Davood Karimi"
        },
        "author": "Davood Karimi",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于胎儿脑 MRI 的 Atlas 辅助分段任意模型 (FeTal-SAM)",
        "abstract_cn": "本文介绍了 FeTal-SAM，这是一种针对胎儿大脑 MRI 分割而定制的分段任意模型 (SAM) 的新型改编版本。传统的深度学习方法通​​常需要大量带注释的数据集来表示一组固定的标签，这使得它们在临床或研究需求发生变化时变得不灵活。通过整合基于图谱的提示和基础模型原则，FeTal-SAM 解决了胎儿大脑 MRI 分割的两个关键限制：(1) 需要针对不同的标签定义重新训练模型，(2) 缺乏对分割是由真实图像对比度驱动还是由学习的空间先验驱动的洞察力。我们利用多图集配准来生成空间对齐的标签模板，这些模板作为密集提示以及边界框提示，用于 SAM 的分段解码器。该策略能够在每个结构的基础上进行二元分割，随后将其融合以重建完整的 3D 分割体积。对两个数据集（dHCP 数据集和内部数据集）的评估证明了 FeTal-SAM 在各个孕龄期间的稳健性能。值得注意的是，它获得的 Dice 分数与最先进的基线相当，这些基线针对每个数据集进行了训练，并针对皮质板和小脑等对比度良好的结构进行了标签定义，同时保持了分割任何用户指定的解剖结构的灵活性。尽管对于细微的、低对比度的结构（例如海马体、杏仁核）观察到的准确度略低，但我们的结果凸显了 FeTal-SAM 无需彻底再训练即可用作通用分割模型的潜力。因此，该方法为临床适用的胎儿脑 MRI 分析工具迈出了有希望的一步。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15766v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15766v1",
        "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps"
        },
        "updated": "2026-01-22T08:57:36Z",
        "updated_parsed": [
            2026,
            1,
            22,
            8,
            57,
            36,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15766v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15766v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T08:57:36Z",
        "published_parsed": [
            2026,
            1,
            22,
            8,
            57,
            36,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yuhan Chen"
            },
            {
                "name": "Ying Fang"
            },
            {
                "name": "Guofa Li"
            },
            {
                "name": "Wenxuan Yu"
            },
            {
                "name": "Yicui Shi"
            },
            {
                "name": "Jingrui Zhang"
            },
            {
                "name": "Kefei Qian"
            },
            {
                "name": "Wenbo Chu"
            },
            {
                "name": "Keqiang Li"
            }
        ],
        "author_detail": {
            "name": "Keqiang Li"
        },
        "author": "Keqiang Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "LL-GaussianMap：通过 2D 高斯泼溅引导增益图进行零样本低光图像增强",
        "abstract_cn": "低光图像增强在视觉质量方面取得了重大进展。然而，大多数现有方法主要在像素域中操作或依赖于隐式特征表示。因此，图像的内在几何结构先验常常被忽视。二维高斯分布（2DGS）已成为一种突出的显式场景表示技术，其特点是卓越的结构拟合能力和高渲染效率。尽管有这些优点，2DGS 在低级视觉任务中的应用仍未得到探索。为了弥补这一差距，LL-GaussianMap 被提出作为第一个将 2DGS 纳入低光图像增强的无监督框架。与传统方法不同，增强任务被制定为由 2DGS 原语引导的增益图生成过程。所提出的方法包括两个主要阶段。首先，利用 2DGS 执行高保真结构重建。然后，通过创新的统一增强模块，通过高斯泼溅的光栅化机制渲染数据驱动的增强字典系数。该设计有效地将 2DGS 的结构感知能力融入增益图生成中，从而在增强过程中保留边缘并抑制伪影。此外，通过无监督学习避免了对配对数据的依赖。实验结果表明，LL-GaussianMap 以极低的存储占用实现了卓越的增强性能，凸显了显式高斯表示对于图像增强的有效性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15772v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15772v1",
        "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting"
        },
        "updated": "2026-01-22T09:01:08Z",
        "updated_parsed": [
            2026,
            1,
            22,
            9,
            1,
            8,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15772v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15772v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T09:01:08Z",
        "published_parsed": [
            2026,
            1,
            22,
            9,
            1,
            8,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yuhan Chen"
            },
            {
                "name": "Wenxuan Yu"
            },
            {
                "name": "Guofa Li"
            },
            {
                "name": "Yijun Xu"
            },
            {
                "name": "Ying Fang"
            },
            {
                "name": "Yicui Shi"
            },
            {
                "name": "Long Cao"
            },
            {
                "name": "Wenbo Chu"
            },
            {
                "name": "Keqiang Li"
            }
        ],
        "author_detail": {
            "name": "Keqiang Li"
        },
        "author": "Keqiang Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "LL-GaussianImage：通过 2D 高斯分布实现零样本低光增强的高效图像表示",
        "abstract_cn": "2D 高斯分布 (2DGS) 是一种新兴的显式场景表示方法，由于高保真度和高压缩比，在图像压缩方面具有巨大的潜力。然而，现有的低光增强算法主要在像素域内运行。处理 2DGS 压缩图像需要繁琐的解压-增强-再压缩流程，这会降低效率并引入二次降级。为了解决这些限制，我们提出了 LL-GaussianImage，这是第一个零样本无监督框架，专为直接在 2DGS 压缩表示域内进行低光增强而设计。该框架提供了三个主要优点。首先，设计了一个语义引导的 Mixture-of-Experts 增强框架。使用渲染图像作为指导，将动态自适应变换应用于 2DGS 的稀疏属性空间，以实现压缩增强，而无需完全解压缩到像素网格。其次，建立多目标协同损失函数系统，严格约束增强过程中的平滑度和保真度，在提高视觉质量的同时抑制伪影。第三，利用两阶段优化过程来实现重建增强。通过单尺度重建保证了基础表示的准确性，增强了网络的鲁棒性。在保持高压缩比的同时实现低光图像的高质量增强。通过实验结果验证了压缩表示域内直接处理范例的可行性和优越性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15779v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15779v1",
        "title": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation"
        },
        "updated": "2026-01-22T09:12:05Z",
        "updated_parsed": [
            2026,
            1,
            22,
            9,
            12,
            5,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15779v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15779v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T09:12:05Z",
        "published_parsed": [
            2026,
            1,
            22,
            9,
            12,
            5,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Liuyun Jiang"
            },
            {
                "name": "Yanchao Zhang"
            },
            {
                "name": "Jinyue Guo"
            },
            {
                "name": "Yizhuo Lu"
            },
            {
                "name": "Ruining Zhou"
            },
            {
                "name": "Hua Han"
            }
        ],
        "author_detail": {
            "name": "Hua Han"
        },
        "author": "Hua Han",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "基于扩散模型的数据增强增强神经元分割",
        "abstract_cn": "电子显微镜（EM）中的神经元分割旨在重建完整的神经元连接组；然而，当前基于深度学习的方法由于依赖大规模训练数据和大量耗时的手动注释而受到限制。传统方法通过几何和光度变换来扩充训练集；然而，生成的样本仍然与原始图像高度相关，并且缺乏结构多样性。为了解决这个限制，我们提出了一种基于扩散的数据增强框架，能够为神经元分割生成多样化且结构合理的图像标签对。具体来说，该框架采用具有多尺度调节和 EM 分辨率先验的分辨率感知条件扩散模型，以实现从 3D 掩模合成体素级图像。它还包含一个生物学引导的面具重塑模块，可生成具有增强结构真实性的增强面具。这些组件共同有效地丰富了训练集并提高了分割性能。在低注释机制下的 AC3 和 AC4 数据集上，当与两种不同的后处理方法相结合时，我们的方法将 ARAND 指标分别提高了 32.1% 和 30.7%。我们的代码可在 https://github.com/HeadLiuYun/NeuroDiff 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15813v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15813v1",
        "title": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data"
        },
        "updated": "2026-01-22T10:01:01Z",
        "updated_parsed": [
            2026,
            1,
            22,
            10,
            1,
            1,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15813v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15813v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T10:01:01Z",
        "published_parsed": [
            2026,
            1,
            22,
            10,
            1,
            1,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Clare Chemery"
            },
            {
                "name": "Hendrik Edelhoff"
            },
            {
                "name": "Ludwig Bothmann"
            }
        ],
        "author_detail": {
            "name": "Ludwig Bothmann"
        },
        "author": "Ludwig Bothmann",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "超越现成模型：为生态学家处理图像数据提供轻量级且可访问的机器学习管道",
        "abstract_cn": "我们引入了一种轻量级实验流程，旨在降低在生态研究中应用机器学习（ML）方法对图像进行分类的障碍。我们使生态学家能够独立试验机器学习模型，因此他们可以超越现成的模型，并生成针对本地数据集和特定分类任务和目标变量的见解。我们的工具将用于预处理、训练和评估的简单命令行界面与用于注释、错误分析和模型比较的图形界面相结合。这种设计使生态学家能够构建和迭代紧凑的、特定于任务的分类器，而无需高级的机器学习专业知识。作为概念验证，我们应用该管道根据在德国 Veldenstein 森林收集的 3392 张相机陷阱图像中的年龄和性别对马鹿 (Cervus elaphus) 进行分类。使用 4352 张包含由专家标记的个体鹿的裁剪图像，我们使用各种参数和数据增强策略来训练和评估多个骨干架构。我们表现​​最好的模型的年龄分类准确率达到 90.77%，性别分类准确率达到 96.15%。这些结果表明，即使数据有限，也可以进行可靠的人口统计分类来回答狭窄的、明确的生态问题。更广泛地说，该框架为生态学家提供了一个易于使用的工具，用于开发针对特定研究问题的机器学习模型，为在野生动物监测和人口分析中更广泛地采用机器学习铺平了道路。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15829v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15829v1",
        "title": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion"
        },
        "updated": "2026-01-22T10:30:32Z",
        "updated_parsed": [
            2026,
            1,
            22,
            10,
            30,
            32,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15829v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15829v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD)."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T10:30:32Z",
        "published_parsed": [
            2026,
            1,
            22,
            10,
            30,
            32,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yonghao Xu"
            },
            {
                "name": "Pedram Ghamisi"
            },
            {
                "name": "Qihao Weng"
            }
        ],
        "author_detail": {
            "name": "Qihao Weng"
        },
        "author": "Qihao Weng",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "通过判别原型引导扩散实现现实遥感数据集蒸馏",
        "abstract_cn": "近年来，在大规模基准数据集的推动下，深度学习在遥感图像解释方面取得了巨大成功。然而，这种对海量训练数据的依赖也带来了两大挑战：（1）高昂的存储和计算成本，以及（2）数据泄露的风险，尤其是涉及敏感类别时。为了应对这些挑战，本研究首次将数据集蒸馏的概念引入遥感图像解译领域。具体来说，我们训练文本到图像的扩散模型，将大规模遥感数据集压缩为紧凑且具有代表性的蒸馏数据集。为了提高合成样本的判别质量，我们提出了一种分类器驱动的指导，通过将预训练模型的分类一致性损失注入到扩散训练过程中。此外，考虑到遥感图像丰富的语义复杂性，我们进一步对训练样本进行潜在空间聚类，以选择具有代表性和多样性的原型作为视觉风格指导，同时使用视觉语言模型提供聚合的文本描述。在三个高分辨率遥感场景分类基准上的实验表明，该方法可以提取真实且多样化的样本用于下游模型训练。代码和预训练模型可在线获取（https://github.com/YonghaoXu/DPD）。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15859v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15859v1",
        "title": "Uncertainty-guided Generation of Dark-field Radiographs",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Uncertainty-guided Generation of Dark-field Radiographs"
        },
        "updated": "2026-01-22T11:07:19Z",
        "updated_parsed": [
            2026,
            1,
            22,
            11,
            7,
            19,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15859v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15859v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications."
        },
        "tags": [
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T11:07:19Z",
        "published_parsed": [
            2026,
            1,
            22,
            11,
            7,
            19,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.LG"
        },
        "authors": [
            {
                "name": "Lina Felsner"
            },
            {
                "name": "Henriette Bast"
            },
            {
                "name": "Tina Dorosti"
            },
            {
                "name": "Florian Schaff"
            },
            {
                "name": "Franz Pfeiffer"
            },
            {
                "name": "Daniela Pfeiffer"
            },
            {
                "name": "Julia Schnabel"
            }
        ],
        "author_detail": {
            "name": "Julia Schnabel"
        },
        "author": "Julia Schnabel",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "不确定性引导的暗场射线照片生成",
        "abstract_cn": "X 射线暗场放射成像通过小角度散射可视化微观结构组织变化，为传统衰减成像提供补充诊断信息。然而，此类数据的可用性有限，给开发强大的深度学习模型带来了挑战。在这项工作中，我们提出了第一个使用不确定性引导渐进生成对抗网络直接从标准衰减胸部 X 射线生成暗场图像的框架。该模型结合了任意和认知不确定性，以提高可解释性和可靠性。实验证明生成的图像具有高结构保真度，并且各个阶段的定量指标得到了一致的改进。此外，分布外评估证实所提出的模型具有良好的泛化性。我们的结果表明，不确定性引导的生成模型可以实现逼真的暗场图像合成，并为未来的临床应用提供可靠的基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15865v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15865v1",
        "title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies"
        },
        "updated": "2026-01-22T11:14:37Z",
        "updated_parsed": [
            2026,
            1,
            22,
            11,
            14,
            37,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15865v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15865v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T11:14:37Z",
        "published_parsed": [
            2026,
            1,
            22,
            11,
            14,
            37,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Jingsong Xia"
            },
            {
                "name": "Siqi Wang"
            }
        ],
        "author_detail": {
            "name": "Siqi Wang"
        },
        "author": "Siqi Wang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于冠状动脉造影的轻量级脑启发机器学习框架：混合神经表示和鲁棒学习策略",
        "abstract_cn": "背景：冠状动脉造影（CAG）是评估冠状动脉疾病和指导介入治疗决策的基础成像方式。然而，在现实临床环境中，血管造影图像通常具有复杂的病变形态、严重的类别不平衡、标签不确定性和有限的计算资源等特点，这对传统深度学习方法在鲁棒性和泛化性方面提出了巨大的挑战。 方法：所提出的框架建立在预训练的卷积神经网络的基础上，以构建轻量级的混合神经表示。引入选择性神经可塑性训练策略以实现有效的参数适应。此外，采用受大脑启发的注意力调制损失函数，将焦点损失与标签平滑相结合，以增强对硬样本和不确定注释的敏感性。采用类不平衡感知采样和热重启余弦退火来模拟生物神经系统中观察到的节律调节和注意力分配机制。结果：实验结果表明，所提出的轻量级类脑启发模型在二元冠状动脉造影分类中实现了强大而稳定的性能，在保持高计算效率的同时产生了有竞争力的准确性、召回率、F1分数和AUC指标。结论：本研究验证了类脑启发学习机制在轻量级医学图像分析中的有效性，并提供了一种在有限的计算资源下为智能临床决策支持提供生物学上合理且可部署的解决方案。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15867v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15867v1",
        "title": "Out-of-Distribution Detection Based on Total Variation Estimation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Out-of-Distribution Detection Based on Total Variation Estimation"
        },
        "updated": "2026-01-22T11:15:16Z",
        "updated_parsed": [
            2026,
            1,
            22,
            11,
            15,
            16,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15867v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15867v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T11:15:16Z",
        "published_parsed": [
            2026,
            1,
            22,
            11,
            15,
            16,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Dabiao Ma"
            },
            {
                "name": "Zhiba Su"
            },
            {
                "name": "Jian Yang"
            },
            {
                "name": "Haojun Fei"
            }
        ],
        "author_detail": {
            "name": "Haojun Fei"
        },
        "author": "Haojun Fei",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "基于总变异估计的分布外检测",
        "abstract_cn": "本文介绍了一种确保机器学习模型部署免受实际应用中潜在分布变化影响的新方法，即总变异分布外 (TV-OOD) 检测方法。现有方法已经产生了令人满意的结果，但 TV-OOD 通过利用总变异网络估计器来计算每个输入对总体总变异的贡献，从而对这些方法进行了改进。通过将其定义为总变异分数，TV-OOD 可以区分分布内和分布外的数据。该方法的功效在一系列模型和数据集上进行了测试，在图像分类任务中始终产生与所有评估指标中领先的分布外检测技术所获得的结果相当或优于的结果。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15884v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15884v1",
        "title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis"
        },
        "updated": "2026-01-22T11:58:37Z",
        "updated_parsed": [
            2026,
            1,
            22,
            11,
            58,
            37,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15884v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15884v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T11:58:37Z",
        "published_parsed": [
            2026,
            1,
            22,
            11,
            58,
            37,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yifan Chen"
            },
            {
                "name": "Fei Yin"
            },
            {
                "name": "Hao Chen"
            },
            {
                "name": "Jia Wu"
            },
            {
                "name": "Chao Li"
            }
        ],
        "author_detail": {
            "name": "Chao Li"
        },
        "author": "Chao Li",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "PMPBench：用于医学图像合成的配对多模态泛癌症基准",
        "abstract_cn": "造影剂在放射成像中发挥着关键作用，因为它可以增强病变的清晰度并改善肿瘤相关疾病诊断的检测。然而，根据患者的健康状况或可用的医疗资源，使用造影剂并不总是可行的。最近的工作探索了基于人工智能的图像翻译，直接从非对比扫描合成对比增强图像，旨在减少副作用并简化临床工作流程。这一方向的进展受到数据限制的限制：（1）现有的公共数据集几乎完全关注与大脑相关的配对 MR 模式； (2) 其他集合包括部分配对的数据，但存在模式/时间戳缺失和空间对齐不完善的问题； (3) 通常缺乏 CT 与 CTC 或 DCE 相位的明确标记； (4)大量资源仍然是私人的。为了弥补这一差距，我们引入了第一个公开的、完全配对的、涵盖 11 个人体器官的泛癌症医学成像数据集。 MR 数据包括覆盖所有三个阶段 (DCE1-DCE3) 的完整动态对比增强 (DCE) 序列，而 CT 数据提供配对的非对比和对比增强采集 (CTC)。该数据集针对解剖对应进行整理，能够对 1 对 1、N 对 1 和 N 对 N 转换设置进行严格评估（例如，根据非对比输入预测 DCE 相位）。在此资源的基础上，我们建立了一个全面的基准。我们报告了当代图像到图像翻译的代表性基线的结果。我们发布数据集和基准，以促进安全、有效的对比合成研究，与多器官肿瘤成像工作流程直接相关。我们的代码和数据集可在 https://github.com/YifanChen02/PMPBench 上公开获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15888v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15888v1",
        "title": "Understanding the Transfer Limits of Vision Foundation Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Understanding the Transfer Limits of Vision Foundation Models"
        },
        "updated": "2026-01-22T12:07:56Z",
        "updated_parsed": [
            2026,
            1,
            22,
            12,
            7,
            56,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15888v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15888v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T12:07:56Z",
        "published_parsed": [
            2026,
            1,
            22,
            12,
            7,
            56,
            3,
            22,
            0
        ],
        "arxiv_comment": "accepted in ISBI 2026",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Shiqi Huang"
            },
            {
                "name": "Yipei Wang"
            },
            {
                "name": "Natasha Thorley"
            },
            {
                "name": "Alexander Ng"
            },
            {
                "name": "Shaheer Saeed"
            },
            {
                "name": "Mark Emberton"
            },
            {
                "name": "Shonit Punwani"
            },
            {
                "name": "Veeru Kasivisvanathan"
            },
            {
                "name": "Dean Barratt"
            },
            {
                "name": "Daniel Alexander"
            },
            {
                "name": "Yipeng Hu"
            }
        ],
        "author_detail": {
            "name": "Yipeng Hu"
        },
        "author": "Yipeng Hu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "了解视觉基础模型的传输限制",
        "abstract_cn": "基础模型利用大规模预训练来捕获广泛的知识，展示各种语言任务的泛化能力。相比之下，尽管计算投入巨大，但视觉基础模型 (VFM) 在下游任务中通常表现出不均匀的改进。我们假设这种限制是由于预训练目标与下游视觉和成像任务的需求之间的不匹配造成的。用于恢复通用视觉模式或全局语义结构等任务的预训练策略（例如掩模图像重建或对比学习形状表示）可能与下游应用程序（包括分割、分类或图像合成）的特定任务要求不一致。为了在具体的现实世界临床领域中研究这一点，我们在五个前列腺多参数 MR 成像任务上评估了两个 VFM，一个专注于重建的基于 MAE 的模型 (ProFound) 和一个基于对比学习的模型 (ProViCNet)，检查此类任务对齐如何影响传输性能，即从预训练到微调。我们的研究结果表明，通过简单的分歧指标（例如微调前后相同特征之间的最大均值差异（MMD））来衡量预训练和下游任务之间的更好一致性，与更大的性能改进和更快的收敛相关，强调了设计和分析预训练目标时考虑下游适用性的重要性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15891v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15891v1",
        "title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture"
        },
        "updated": "2026-01-22T12:11:53Z",
        "updated_parsed": [
            2026,
            1,
            22,
            12,
            11,
            53,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15891v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15891v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T12:11:53Z",
        "published_parsed": [
            2026,
            1,
            22,
            12,
            11,
            53,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Anas Anwarul Haq Khan"
            },
            {
                "name": "Mariam Husain"
            },
            {
                "name": "Kshitij Jadhav"
            }
        ],
        "author_detail": {
            "name": "Kshitij Jadhav"
        },
        "author": "Kshitij Jadhav",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "RadJEPA：通过联合嵌入预测架构进行胸部 X 光放射学编码器",
        "abstract_cn": "医学视觉语言模型的最新进展指导了视觉表征的学习；然而，这种形式的监督受到配对图像文本数据的可用性的限制，这就提出了一个问题：是否可以在不依赖语言监督的情况下学习鲁棒的放射学编码器。在这项工作中，我们介绍了 RadJEPA，这是一个基于联合嵌入预测架构构建的自监督框架，无需语言监督即可学习。该模型仅针对未标记的胸部 X 射线图像进行预训练，学习预测屏蔽图像区域的潜在表示。这种预测目标与图像文本预训练和 DINO 式自蒸馏都有根本的不同：​​RadJEPA 不是跨视图或模态对齐全局表示，而是显式地对潜在空间预测进行建模。我们在疾病分类、语义分割和报告生成任务上评估学习的编码器。在各个基准测试中，RadJEPA 的性能超过了最先进的方法，包括 Rad-DINO。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15909v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15909v1",
        "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech"
        },
        "updated": "2026-01-22T12:38:20Z",
        "updated_parsed": [
            2026,
            1,
            22,
            12,
            38,
            20,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15909v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15909v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals."
        },
        "tags": [
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T12:38:20Z",
        "published_parsed": [
            2026,
            1,
            22,
            12,
            38,
            20,
            3,
            22,
            0
        ],
        "arxiv_comment": "Accepted at IEEE ISBI 2026",
        "arxiv_primary_category": {
            "term": "cs.CL"
        },
        "authors": [
            {
                "name": "Soufiane Jhilal"
            },
            {
                "name": "Stéphanie Martin"
            },
            {
                "name": "Anne-Lise Giraud"
            }
        ],
        "author_detail": {
            "name": "Anne-Lise Giraud"
        },
        "author": "Anne-Lise Giraud",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于基于 MEG 的想象语音解码的 ImageNet 迁移学习",
        "abstract_cn": "由于微弱的分布式信号和有限的标记数据，对想象语音的非侵入式解码仍然具有挑战性。我们的论文介绍了一种基于图像的方法，可将脑磁图（MEG）信号转换为与预训练视觉模型兼容的时频表示。来自执行想象语音任务的 21 名参与者的 MEG 数据通过可学习的传感器空间卷积投影到三个空间尺度图混合物中，为 ImageNet 预训练的视觉架构生成紧凑的类似图像的输入。这些模型的性能优于经典模型和非预训练模型，图像与静音的平衡准确率高达 90.4%，默读的平衡准确率为 81.0%，元音解码的平衡准确率为 60.6%。跨受试者评估证实，预训练模型捕获共享神经表征，并针对图像锁定间隔进行局部判别信息的时间分析。这些发现表明，应用于基于图像的 MEG 表示的预训练视觉模型可以有效地捕获非侵入性神经信号中想象的语音结构。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15951v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15951v1",
        "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis"
        },
        "updated": "2026-01-22T13:39:29Z",
        "updated_parsed": [
            2026,
            1,
            22,
            13,
            39,
            29,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15951v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15951v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T13:39:29Z",
        "published_parsed": [
            2026,
            1,
            22,
            13,
            39,
            29,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sheng Miao"
            },
            {
                "name": "Sijin Li"
            },
            {
                "name": "Pan Wang"
            },
            {
                "name": "Dongfeng Bai"
            },
            {
                "name": "Bingbing Liu"
            },
            {
                "name": "Yue Wang"
            },
            {
                "name": "Andreas Geiger"
            },
            {
                "name": "Yiyi Liao"
            }
        ],
        "author_detail": {
            "name": "Yiyi Liao"
        },
        "author": "Yiyi Liao",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "EVolSplat4D：用于 4D 城市场景合成的高效基于体积的高斯分布",
        "abstract_cn": "静态和动态城市场景的新颖视图合成（NVS）对于自动驾驶模拟至关重要，但现有方法往往难以平衡重建时间和质量。虽然最先进的神经辐射场和 3D 高斯喷射方法可以实现照片级真实感，但它们通常依赖于耗时的每个场景优化。相反，新兴的前馈方法经常采用每像素高斯表示，这会导致在复杂、动态环境中聚合多视图预测时出现 3D 不一致。我们提出了 EvolSplat4D，这是一种前馈框架，通过统一三个专门分支中基于体积和基于像素的高斯预测，超越了现有的每像素范式。对于近距离静态区域，我们直接从 3D 特征体积预测多个帧上 3D 高斯的一致几何形状，并辅以语义增强的基于图像的渲染模块来预测其外观。对于动态演员，我们利用以对象为中心的规范空间和运动调整渲染模块来聚合时间特征，确保稳定的 4D 重建，尽管运动先验存在噪声。远场场景由高效的每像素高斯分支处理，以确保全场景覆盖。 KITTI-360、KITTI、Waymo 和 PandaSet 数据集上的实验结果表明，EvolSplat4D 能够以卓越的准确性和一致性重建静态和动态环境，优于按场景优化和最先进的前馈基线。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15968v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15968v1",
        "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models"
        },
        "updated": "2026-01-22T13:49:47Z",
        "updated_parsed": [
            2026,
            1,
            22,
            13,
            49,
            47,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15968v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15968v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T13:49:47Z",
        "published_parsed": [
            2026,
            1,
            22,
            13,
            49,
            47,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Xin Xie"
            },
            {
                "name": "Jiaxian Guo"
            },
            {
                "name": "Dong Gong"
            }
        ],
        "author_detail": {
            "name": "Dong Gong"
        },
        "author": "Dong Gong",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "HyperAlign：用于扩散模型的高效测试时对齐的超网络",
        "abstract_cn": "扩散模型实现了最先进的性能，但通常无法生成符合人类偏好和意图的输出，导致图像的审美质量较差和语义不一致。现有的对齐方法存在一个困难的权衡：微调方法会因奖励过度优化而损失多样性，而测试时间缩放方法会引入大量的计算开销，并且往往会优化不足。为了解决这些限制，我们提出了 HyperAlign，这是一种新颖的框架，可以训练超网络以实现高效且有效的测试时间对齐。 HyperAlign 不是修改潜在状态，而是动态生成低秩适应权重来调节扩散模型的生成算子。这使得去噪轨迹能够根据输入潜伏、时间步长和奖励条件对齐的提示进行自适应调整。我们引入了 HyperAlign 的多个变体，这些变体的不同之处在于超网络的应用频率，以平衡性能和效率。此外，我们使用偏好数据规范化的奖励分数目标来优化超网络，以减少奖励黑客行为。我们在多种扩展生成范式上评估 HyperAlign，包括稳定扩散和通量。它在增强语义一致性和视觉吸引力方面显着优于现有的微调和测试时间缩放基线。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16007v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16007v1",
        "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models"
        },
        "updated": "2026-01-22T14:33:01Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            33,
            1,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16007v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16007v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:33:01Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            33,
            1,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chak-Wing Mak"
            },
            {
                "name": "Guanyu Zhu"
            },
            {
                "name": "Boyi Zhang"
            },
            {
                "name": "Hongji Li"
            },
            {
                "name": "Xiaowei Chi"
            },
            {
                "name": "Kevin Zhang"
            },
            {
                "name": "Yichen Wu"
            },
            {
                "name": "Yangfan He"
            },
            {
                "name": "Chun-Kai Fan"
            },
            {
                "name": "Wentao Lu"
            },
            {
                "name": "Kuangzhi Ge"
            },
            {
                "name": "Xinyu Fang"
            },
            {
                "name": "Hongyang He"
            },
            {
                "name": "Kuan Lu"
            },
            {
                "name": "Tianxiang Xu"
            },
            {
                "name": "Li Zhang"
            },
            {
                "name": "Yongxin Ni"
            },
            {
                "name": "Youhua Li"
            },
            {
                "name": "Shanghang Zhang"
            }
        ],
        "author_detail": {
            "name": "Shanghang Zhang"
        },
        "author": "Shanghang Zhang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "PhysicsMind：基础 VLM 和世界模型中物理推理和预测的模拟和真实力学基准测试",
        "abstract_cn": "现代基础多模态大语言模型 (MLLM) 和视频世界模型在数学、常识和视觉推理方面取得了显着进步，但它们对底层物理的掌握仍然有待探索。试图衡量这个问题的现有基准依赖于合成的视觉问答模板或关注感知视频质量，这与衡量视频遵守物理定律的程度无关。为了解决这种碎片化问题，我们引入了PhysicsMind，这是一个具有真实和模拟环境的统一基准，可根据三个规范原理（质心、杠杆平衡和牛顿第一定律）评估规律一致的推理和生成。 PhysicsMind 包括两个主要任务：i）VQA 任务，测试模型是否可以推理并确定图像或短视频中的物理量和值；ii）视频生成（VG）任务，评估预测的运动轨迹是否遵循与地面真实情况相同的质心、扭矩和惯性约束。在PhysicsMind 上评估了一系列最新的模型和视频生成模型，发现它们依赖于外观启发法，同时经常违反基本力学。这些差距表明，当前的扩展和训练仍然不足以实现强大的物理理解，这凸显了PhysicsMind作为物理感知多模态模型的重点测试平台。我们的数据将在接受后发布。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16020v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16020v1",
        "title": "Keyframe-Based Feed-Forward Visual Odometry",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Keyframe-Based Feed-Forward Visual Odometry"
        },
        "updated": "2026-01-22T14:45:42Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            45,
            42,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16020v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16020v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.RO",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:45:42Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            45,
            42,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Weichen Dai"
            },
            {
                "name": "Wenhan Su"
            },
            {
                "name": "Da Kong"
            },
            {
                "name": "Yuhang Ming"
            },
            {
                "name": "Wanzeng Kong"
            }
        ],
        "author_detail": {
            "name": "Wanzeng Kong"
        },
        "author": "Wanzeng Kong",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "基于关键帧的前馈视觉里程计",
        "abstract_cn": "视觉基础模型的出现彻底改变了视觉里程计（VO）和 SLAM，在单个前馈网络中实现姿态估计和密集重建。然而，与利用关键帧方法来提高效率和准确性的传统流程不同，当前基于基础模型的方法（例如 VGGT-Long）通常不加区别地处理原始图像序列。这会导致计算冗余和低帧间视差导致的性能下降，从而提供有限的上下文立体信息。将传统的几何启发式集成到这些方法中并非易事，因为它们的性能取决于高维潜在表示而不是显式几何度量。为了弥补这一差距，我们提出了一种新颖的基于关键帧的前馈 VO。我们的方法不依赖手工制定的规则，而是采用强化学习以数据驱动的方式导出自适应关键帧策略，使选择与底层基础模型的内在特征保持一致。我们在 TartanAir 数据集上训练我们的代理，并在几个现实世界的数据集上进行广泛的评估。实验结果表明，所提出的方法比最先进的前馈 VO 方法取得了一致且实质性的改进。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16024v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16024v1",
        "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry"
        },
        "updated": "2026-01-22T14:49:30Z",
        "updated_parsed": [
            2026,
            1,
            22,
            14,
            49,
            30,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16024v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16024v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T14:49:30Z",
        "published_parsed": [
            2026,
            1,
            22,
            14,
            49,
            30,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Rongze Ma"
            },
            {
                "name": "Mengkang Lu"
            },
            {
                "name": "Zhenyu Xiang"
            },
            {
                "name": "Yongsheng Pan"
            },
            {
                "name": "Yicheng Wu"
            },
            {
                "name": "Qingjie Zeng"
            },
            {
                "name": "Yong Xia"
            }
        ],
        "author_detail": {
            "name": "Yong Xia"
        },
        "author": "Yong Xia",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "PAINT：虚拟免疫组织化学的病理学感知集成下一代转化",
        "abstract_cn": "虚拟免疫组织化学 (IHC) 旨在通过常规苏木精和曙红 (H\\&E) 图像通过计算合成分子染色模式，为传统物理染色提供一种经济有效且组织高效的替代方案。然而，这项任务特别具有挑战性：H\\&E 形态提供了有关蛋白质表达的模糊线索，而相似的组织结构可能对应于不同的分子状态。大多数现有方法侧重于直接外观合成以隐式实现跨模态生成，通常由于结构先验不足而导致语义不一致。在本文中，我们提出了病理学感知集成下一尺度转换（PAINT），这是一种视觉自回归框架，它将合成过程重新表述为结构优先的条件生成任务。与直接图像翻译不同，PAINT 通过解析以全局结构布局为条件的分子细节来强制执行因果顺序。该方法的核心是引入空间结构起始图（3S-Map），它将自回归初始化基于观察到的形态，确保确定性、空间对齐的合成。 IHC4BC 和 MIST 数据集上的实验表明，PAINT 在结构保真度和临床下游任务方面优于最先进的方法，验证了结构引导自回归建模的潜力。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16060v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16060v1",
        "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation"
        },
        "updated": "2026-01-22T15:56:21Z",
        "updated_parsed": [
            2026,
            1,
            22,
            15,
            56,
            21,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16060v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16060v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T15:56:21Z",
        "published_parsed": [
            2026,
            1,
            22,
            15,
            56,
            21,
            3,
            22,
            0
        ],
        "arxiv_comment": "5 pages, 4 figures. It has been accepted by IEEE ISBI",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Yuan Lin"
            },
            {
                "name": "Murong Xu"
            },
            {
                "name": "Marc Hölle"
            },
            {
                "name": "Chinmay Prabhakar"
            },
            {
                "name": "Andreas Maier"
            },
            {
                "name": "Vasileios Belagiannis"
            },
            {
                "name": "Bjoern Menze"
            },
            {
                "name": "Suprosanna Shit"
            }
        ],
        "author_detail": {
            "name": "Suprosanna Shit"
        },
        "author": "Suprosanna Shit",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "ProGiDiff：基于即时引导扩散的医学图像分割",
        "abstract_cn": "广泛采用的医学图像分割方法虽然有效，但主要是确定性的，并且仍然很难适应自然语言提示。因此，它们缺乏估计多个提议、人类交互和跨模态适应的能力。最近，文本到图像的扩散模型已显示出弥补这一差距的潜力。然而，从头开始训练它们需要一个大的数据集——这是医学图像分割的限制。此外，它们通常仅限于二进制分割，并且不能以自然语言提示为条件。为此，我们提出了一种名为 ProGiDiff 的新颖框架，该框架利用现有的图像生成模型进行医学图像分割。具体来说，我们提出了一种带有自定义编码器的 ControlNet 式调节机制，适用于图像调节，以引导预训练的扩散模型输出分割掩模。只需通过提示目标器官，它就可以自然地扩展到多类别设置。与以前的方法相比，我们对 CT 图像器官分割的实验展示了强大的性能，并且可以极大地受益于专家在环设置中利用多个建议。重要的是，我们证明了学习到的调节机制可以通过低秩、少样本适应轻松转移以分割 MR 图像。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16065v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16065v1",
        "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models"
        },
        "updated": "2026-01-22T16:02:56Z",
        "updated_parsed": [
            2026,
            1,
            22,
            16,
            2,
            56,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16065v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16065v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.RO",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T16:02:56Z",
        "published_parsed": [
            2026,
            1,
            22,
            16,
            2,
            56,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Chenyang Li"
            },
            {
                "name": "Jieyuan Liu"
            },
            {
                "name": "Bin Li"
            },
            {
                "name": "Bo Gao"
            },
            {
                "name": "Yilin Yuan"
            },
            {
                "name": "Yangfan He"
            },
            {
                "name": "Yuchen Li"
            },
            {
                "name": "Jingqun Tang"
            }
        ],
        "author_detail": {
            "name": "Jingqun Tang"
        },
        "author": "Jingqun Tang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DTP：一种简单而有效的视觉语言动作模型分散标记修剪框架",
        "abstract_cn": "视觉语言动作（VLA）模型利用视觉语言模型（VLM）强大的感知能力来理解环境并直接输出动作，在机器人操作方面取得了显着的进展。然而，默认情况下，VLA 模型可能会过度关注与任务无关区域中的图像标记，我们将其描述为“分散注意力的标记”。这种行为可能会干扰模型在每个步骤中生成所需操作标记，从而影响任务的成功率。在本文中，我们介绍了一种简单而有效的即插即用分散注意力标记修剪（DTP）框架，该框架可以动态检测和修剪这些分散注意力的图像标记。通过纠正模型的视觉注意模式，我们的目标是提高任务成功率，并在不改变其原始架构或添加额外输入的情况下探索模型的性能上限。 SIMPLER Benchmark（Li 等人，2024）上的实验表明，我们的方法在不同类型的新型 VLA 模型中始终实现了任务成功率的相对提高，证明了基于 Transformer 的 VLA 的通用性。进一步的分析揭示了所有测试模型的任务成功率与任务无关区域的关注量之间存在负相关，这凸显了 VLA 模型的普遍现象，可以指导未来的研究。我们还将我们的代码发布在：https://anonymous.4open.science/r/CBD3。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16073v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16073v1",
        "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models"
        },
        "updated": "2026-01-22T16:18:02Z",
        "updated_parsed": [
            2026,
            1,
            22,
            16,
            18,
            2,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16073v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16073v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.DC",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T16:18:02Z",
        "published_parsed": [
            2026,
            1,
            22,
            16,
            18,
            2,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Hanwen Zhang"
            },
            {
                "name": "Qiaojin Shen"
            },
            {
                "name": "Yuxi Liu"
            },
            {
                "name": "Yuesheng Zhu"
            },
            {
                "name": "Guibo Luo"
            }
        ],
        "author_detail": {
            "name": "Guibo Luo"
        },
        "author": "Guibo Luo",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "DSFedMed：通过基础模型和轻量级模型之间的相互蒸馏进行双尺度联合医学图像分割",
        "abstract_cn": "基础模型（FM）在不同的视觉任务中表现出了很强的泛化能力。然而，它们在联合设置中的部署受到高计算需求、大量通信开销和巨大推理成本的阻碍。我们提出了 DSFedMed，这是一个双尺度联合框架，可以实现集中式基础模型和轻量级客户端模型之间的相互知识蒸馏，以进行医学图像分割。为了支持知识蒸馏，生成了一组高质量的医学图像来替换真实的公共数据集，并提出了一种可学习性引导的样本选择策略来提高双尺度蒸馏的效率和有效性。这种相互蒸馏使基础模型能够将一般知识传递给轻量级客户端，同时还结合特定于客户端的见解来完善基础模型。对五个医学成像分割数据集的评估表明，与现有的联合基础模型基线相比，DSFedMed 的 Dice 分数平均提高了 2%，同时将通信成本和推理时间减少了近 90%。这些结果证明了资源有限的联合部署的显着效率提升和可扩展性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16079v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16079v1",
        "title": "Masked Modeling for Human Motion Recovery Under Occlusions",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Masked Modeling for Human Motion Recovery Under Occlusions"
        },
        "updated": "2026-01-22T16:22:20Z",
        "updated_parsed": [
            2026,
            1,
            22,
            16,
            22,
            20,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16079v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16079v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T16:22:20Z",
        "published_parsed": [
            2026,
            1,
            22,
            16,
            22,
            20,
            3,
            22,
            0
        ],
        "arxiv_comment": "Project page: https://mikeqzy.github.io/MoRo",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zhiyin Qian"
            },
            {
                "name": "Siwei Zhang"
            },
            {
                "name": "Bharat Lal Bhatnagar"
            },
            {
                "name": "Federica Bogo"
            },
            {
                "name": "Siyu Tang"
            }
        ],
        "author_detail": {
            "name": "Siyu Tang"
        },
        "author": "Siyu Tang",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "遮挡下人体运动恢复的掩蔽建模",
        "abstract_cn": "单目视频的人体运动重建是计算机视觉领域的一项基本挑战，在 AR/VR、机器人技术和数字内容创建领域有着广泛的应用，但在现实环境中频繁遮挡的情况下仍然具有挑战性。现有的基于回归的方法虽然高效，但容易丢失观察结果，而基于优化和扩散的方法则以缓慢的推理速度和繁重的预处理步骤为代价提高了鲁棒性。为了解决这些限制，我们利用生成掩模建模的最新进展，并提出了 MoRo：遮挡下人体运动恢复的掩模建模。 MoRo 是一种遮挡稳健的端到端生成框架，它将运动重建制定为视频条件任务，并在一致的全局坐标系中从 RGB 视频中有效地恢复人体运动。通过屏蔽建模，MoRo 自然地处理遮挡，同时实现高效的端到端推理。为了克服成对视频运动数据的稀缺性，我们设计了一种跨模态学习方案，从一组异构数据集中学习多模态先验：（i）在MoCap数据集上训练的轨迹感知运动，（ii）在图像姿势数据集上训练的图像条件姿势，捕获不同的每帧姿势，以及（iii）融合运动和姿势先验的视频条件掩模变换器，在视频运动数据集上进行微调，以将视觉线索与运动动力学相结合为了稳健的推理。在 EgoBody 和 RICH 上进行的大量实验表明，MoRo 在遮挡情况下的准确性和运动真实感方面远远优于最先进的方法，而在非遮挡场景中的表现则相当。 MoRo 在单个 H200 GPU 上实现了 70 FPS 的实时推理。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16098v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16098v1",
        "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification"
        },
        "updated": "2026-01-22T16:47:07Z",
        "updated_parsed": [
            2026,
            1,
            22,
            16,
            47,
            7,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16098v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16098v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T16:47:07Z",
        "published_parsed": [
            2026,
            1,
            22,
            16,
            47,
            7,
            3,
            22,
            0
        ],
        "arxiv_comment": "5 pages, 3 figures",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zack Dewis"
            },
            {
                "name": "Yimin Zhu"
            },
            {
                "name": "Zhengsen Xu"
            },
            {
                "name": "Mabel Heffring"
            },
            {
                "name": "Saeid Taleghanidoozdoozan"
            },
            {
                "name": "Quinn Ledingham"
            },
            {
                "name": "Lincoln Linlin Xu"
            }
        ],
        "author_detail": {
            "name": "Lincoln Linlin Xu"
        },
        "author": "Lincoln Linlin Xu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "用于高光谱图像分类的聚类引导空间光谱 Mamba",
        "abstract_cn": "尽管 Mamba 模型极大地改进了高光谱图像 (HSI) 分类，但它们在定义高效和自适应标记序列以提高性能方面面临着严峻的挑战。因此，本文提出了CSSMamba（聚类引导的空间频谱Mamba）框架，以更好地应对这些挑战，并做出以下贡献。首先，为了实现高效和自适应的令牌序列以提高 Mamba 性能，我们将聚类机制集成到空间 Mamba 架构中，形成了聚类引导的空间 Mamba 模块（CSpaMamba），可以减少 Mamba 序列长度并提高 Mamba 特征学习能力。其次，为了提高空间和光谱信息的学习，我们将 CSpaMamba 模块与光谱曼巴模块 (SpeMamba) 集成，形成一个完整的聚类引导的空间光谱 Mamba 框架。第三，为了进一步提高特征学习能力，我们引入了注意力驱动的令牌选择机制来优化 Mamba 令牌排序。最后，为了以一致的方式将聚类无缝集成到 Mamba 模型中，我们设计了一个可学习聚类模块，以自适应方式学习聚类成员资格。在 Pavia University、Indian Pines 和 Liao-Ning 01 数据集上的实验表明，与最先进的 CNN、Transformer 和基于 Mamba 的方法相比，CSSMamba 实现了更高的精度和更好的边界保留。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16113v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16113v1",
        "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier"
        },
        "updated": "2026-01-22T17:01:33Z",
        "updated_parsed": [
            2026,
            1,
            22,
            17,
            1,
            33,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16113v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16113v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\n  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\n  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\n  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\n  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide."
        },
        "tags": [
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T17:01:33Z",
        "published_parsed": [
            2026,
            1,
            22,
            17,
            1,
            33,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CL"
        },
        "authors": [
            {
                "name": "Haq Nawaz Malik"
            },
            {
                "name": "Kh Mohmad Shafi"
            },
            {
                "name": "Tanveer Ahmad Reshi"
            }
        ],
        "author_detail": {
            "name": "Tanveer Ahmad Reshi"
        },
        "author": "Tanveer Ahmad Reshi",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "Synthocr-gen：用于低资源语言的合成 OCR 数据集生成器 - 打破数据障碍",
        "abstract_cn": "由于缺乏大规模带注释的训练数据集，低资源语言的光学字符识别 (OCR) 仍然是一个重大挑战。克什米尔语等语言拥有大约 700 万使用者，其复杂的波斯阿拉伯语文字具有独特的变音符号，目前缺乏 Tesseract、TrOCR 和 PaddleOCR 等主要 OCR 系统的支持。为此类语言手动创建数据集非常昂贵、耗时且容易出错，通常需要逐字转录打印或手写文本。\n  我们推出 SynthOCR-Gen，这是一款专为低资源语言设计的开源合成 OCR 数据集生成器。我们的工具通过将数字 Unicode 文本语料库转换为即用型训练数据集，解决了 OCR 开发的根本瓶颈。该系统实现了一个全面的管道，包括文本分段（字符、单词、n-gram、句子和行级别）、具有脚本纯度强制的 Unicode 规范化、具有可配置分布的多字体渲染，以及模拟真实世界文档降级（包括旋转、模糊、噪声和扫描仪伪影）的 25 多种数据增强技术。\n  我们通过生成包含 600,000 个样本的分词克什米尔 OCR 数据集来展示我们方法的有效性，该数据集已在 HuggingFace 上公开发布。这项工作为将资源匮乏的语言带入视觉语言人工智能模型时代提供了一条实用途径，并且该工具可供全球使用服务不足的书写系统的研究人员和从业者公开使用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16125v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16125v1",
        "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing"
        },
        "updated": "2026-01-22T17:26:52Z",
        "updated_parsed": [
            2026,
            1,
            22,
            17,
            26,
            52,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16125v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16125v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.IR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T17:26:52Z",
        "published_parsed": [
            2026,
            1,
            22,
            17,
            26,
            52,
            3,
            22,
            0
        ],
        "arxiv_comment": "Under review",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Tingyu Song"
            },
            {
                "name": "Yanzhao Zhang"
            },
            {
                "name": "Mingxin Li"
            },
            {
                "name": "Zhuoning Guo"
            },
            {
                "name": "Dingkun Long"
            },
            {
                "name": "Pengjun Xie"
            },
            {
                "name": "Siyue Zhang"
            },
            {
                "name": "Yilun Zhao"
            },
            {
                "name": "Shu Wu"
            }
        ],
        "author_detail": {
            "name": "Shu Wu"
        },
        "author": "Shu Wu",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "重新思考组合图像检索评估：图像编辑的细粒度基准",
        "abstract_cn": "组合图像检索（CIR）是多模态理解中的一项关键而复杂的任务。当前的 CIR 基准通常具有有限的查询类别，无法捕获现实场景的多样化需求。为了弥补这一评估差距，我们利用图像编辑来实现对修改类型和内容的精确控制，从而实现跨广泛类别综合查询的管道。使用此管道，我们构建了 EDIR，一种新颖的细粒度 CIR 基准。 EDIR 包含 5,000 个高质量查询，涵盖五个主要类别和十五个子类别。我们对 13 个多模态嵌入模型的综合评估揭示了显着的能力差距；即使是最先进的模型（例如 RzenEmbed 和 GME）也很难在所有子类别中保持一致的表现，这凸显了我们基准的严格性。通过比较分析，我们进一步揭示了现有基准的固有局限性，例如模态偏差和分类覆盖范围不足。此外，域内训练实验证明了我们基准的可行性。该实验通过区分可使用目标数据解决的类别和暴露当前模型架构内在局限性的类别，阐明了任务挑战。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16140v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16140v1",
        "title": "Learning to Watermark in the Latent Space of Generative Models",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Learning to Watermark in the Latent Space of Generative Models"
        },
        "updated": "2026-01-22T17:34:30Z",
        "updated_parsed": [
            2026,
            1,
            22,
            17,
            34,
            30,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16140v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16140v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            },
            {
                "term": "cs.CR",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T17:34:30Z",
        "published_parsed": [
            2026,
            1,
            22,
            17,
            34,
            30,
            3,
            22,
            0
        ],
        "arxiv_comment": "Code and models are available at https://github.com/facebookresearch/distseal",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Sylvestre-Alvise Rebuffi"
            },
            {
                "name": "Tuan Tran"
            },
            {
                "name": "Valeriu Lacatusu"
            },
            {
                "name": "Pierre Fernandez"
            },
            {
                "name": "Tomáš Souček"
            },
            {
                "name": "Nikola Jovanović"
            },
            {
                "name": "Tom Sander"
            },
            {
                "name": "Hady Elsahar"
            },
            {
                "name": "Alexandre Mourachko"
            }
        ],
        "author_detail": {
            "name": "Alexandre Mourachko"
        },
        "author": "Alexandre Mourachko",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "学习在生成模型的潜在空间中添加水印",
        "abstract_cn": "现有的人工智能生成图像水印方法通常依赖于像素空间中应用的事后方法，从而引入计算开销和潜在的视觉伪影。在这项工作中，我们探索了潜在空间水印并引入了 DistSeal，这是一种适用于扩散模型和自回归模型的统一潜在水印方法。我们的方法通过在生成模型的潜在空间中训练事后水印模型来工作。我们证明这些潜在水印可以有效地提取到生成模型本身或潜在解码器中，从而实现模型内水印。由此产生的潜在水印实现了具有竞争力的鲁棒性，同时提供了类似的不易察觉性，并且与像素空间基线相比，速度提高了 20 倍。我们的实验进一步表明，提取潜在水印优于提取像素空间水印，提供了一种更高效、更稳健的解决方案。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16192v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16192v1",
        "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "360Anything: Geometry-Free Lifting of Images and Videos to 360°"
        },
        "updated": "2026-01-22T18:45:59Z",
        "updated_parsed": [
            2026,
            1,
            22,
            18,
            45,
            59,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16192v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16192v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T18:45:59Z",
        "published_parsed": [
            2026,
            1,
            22,
            18,
            45,
            59,
            3,
            22,
            0
        ],
        "arxiv_comment": "Project page: https://360anything.github.io/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Ziyi Wu"
            },
            {
                "name": "Daniel Watson"
            },
            {
                "name": "Andrea Tagliasacchi"
            },
            {
                "name": "David J. Fleet"
            },
            {
                "name": "Marcus A. Brubaker"
            },
            {
                "name": "Saurabh Saxena"
            }
        ],
        "author_detail": {
            "name": "Saurabh Saxena"
        },
        "author": "Saurabh Saxena",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "360Anything：无几何形状地将图像和视频提升至 360°",
        "abstract_cn": "将透视图像和视频提升至 360° 全景，从而生成身临其境的 3D 世界。现有方法通常依赖于透视和等距柱状投影 (ERP) 空间之间的显式几何对齐。然而，这需要已知的相机元数据，从而模糊了对野外数据的应用，而这种校准通常不存在或有噪音。我们提出了 360Anything，这是一个基于预先训练的扩散变压器构建的无几何框架。通过将透视输入和全景目标简单地视为标记序列，360Anything 以纯粹数据驱动的方式学习透视到等距矩形的映射，从而无需相机信息。我们的方法在图像和视频 360° 透视生成方面均实现了最先进的性能，优于使用地面实况相机信息的先前作品。我们还将 ERP 边界处接缝伪影的根本原因追溯到 VAE 编码器中的零填充，并引入循环潜在编码以促进无缝生成。最后，我们在零镜头相机 FoV 和方向估计基准中展示了具有竞争力的结果，展示了 360Anything 深刻的几何理解和在计算机视觉任务中更广泛的实用性。其他结果可在 https://360anything.github.io/ 获取。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16208v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16208v1",
        "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"
        },
        "updated": "2026-01-22T18:58:16Z",
        "updated_parsed": [
            2026,
            1,
            22,
            18,
            58,
            16,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16208v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16208v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T18:58:16Z",
        "published_parsed": [
            2026,
            1,
            22,
            18,
            58,
            16,
            3,
            22,
            0
        ],
        "arxiv_comment": "website: https://rae-dit.github.io/scale-rae/",
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Shengbang Tong"
            },
            {
                "name": "Boyang Zheng"
            },
            {
                "name": "Ziteng Wang"
            },
            {
                "name": "Bingda Tang"
            },
            {
                "name": "Nanye Ma"
            },
            {
                "name": "Ellis Brown"
            },
            {
                "name": "Jihan Yang"
            },
            {
                "name": "Rob Fergus"
            },
            {
                "name": "Yann LeCun"
            },
            {
                "name": "Saining Xie"
            }
        ],
        "author_detail": {
            "name": "Saining Xie"
        },
        "author": "Saining Xie",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "使用表示自动编码器缩放文本到图像扩散变压器",
        "abstract_cn": "通过在高维语义潜在空间中进行训练，表示自动编码器 (RAE) 在 ImageNet 上的扩散建模中显示出明显的优势。在这项工作中，我们研究了该框架是否可以扩展到大规模、自由格式的文本到图像（T2I）生成。我们首先通过对网络、合成和文本渲染数据进行训练，将冻结表示编码器 (SigLIP-2) 上的 RAE 解码器扩展到 ImageNet 之外，发现虽然规模提高了总体保真度，但有针对性的数据组合对于文本等特定领域至关重要。然后，我们对最初为 ImageNet 提出的 RAE 设计选择进行严格的压力测试。我们的分析表明，扩展简化了框架：虽然与维度相关的噪声调度仍然至关重要，但诸如宽扩散头和噪声增强解码之类的架构复杂性在规模上提供的优势可以忽略不计。在这个简化的框架上构建，我们对扩散变压器从 0.5B 到 9.8B 参数范围内的 RAE 与最先进的 FLUX VAE 进行了受控比较。在所有模型规模的预训练过程中，RAE 的表现始终优于 VAE。此外，在对高质量数据集进行微调期间，基于 VAE 的模型在 64 个 epoch 后出现灾难性的过拟合，而 RAE 模型在 256 个 epoch 中保持稳定，并始终获得更好的性能。在所有实验中，基于 RAE 的扩散模型表现出更快的收敛速度和更好的生成质量，使 RAE 成为比 VAE 更简单、更强大的基础，可用于大规模 T2I 生成。此外，由于视觉理解和生成都可以在共享表示空间中运行，因此多模态模型可以直接对生成的潜在变量进行推理，为统一模型开辟了新的可能性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16214v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.16214v1",
        "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback"
        },
        "updated": "2026-01-22T18:59:56Z",
        "updated_parsed": [
            2026,
            1,
            22,
            18,
            59,
            56,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.16214v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.16214v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T18:59:56Z",
        "published_parsed": [
            2026,
            1,
            22,
            18,
            59,
            56,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Wenhang Ge"
            },
            {
                "name": "Guibao Shen"
            },
            {
                "name": "Jiawei Feng"
            },
            {
                "name": "Luozhou Wang"
            },
            {
                "name": "Hao Lu"
            },
            {
                "name": "Xingye Tian"
            },
            {
                "name": "Xin Tao"
            },
            {
                "name": "Ying-Cong Chen"
            }
        ],
        "author_detail": {
            "name": "Ying-Cong Chen"
        },
        "author": "Ying-Cong Chen",
        "journal": "arXiv: AI & Vision (Optics)",
        "title_cn": "CamPilot：通过高效的相机奖励反馈改进视频扩散模型中的相机控制",
        "abstract_cn": "摄像机控制的视频扩散模型的最新进展显着改善了摄像机对准。然而，相机的可控性仍然有限。在这项工作中，我们以奖励反馈学习为基础，旨在进一步提高相机的可控性。然而，直接借用现有的 ReFL 方法面临着一些挑战。首先，当前的奖励模型缺乏评估摄像机对齐的能力。其次，将潜在视频解码为 RGB 视频以进行奖励计算会带来大量的计算开销。第三，3D 几何信息在视频解码过程中通常被忽略。为了解决这些限制，我们引入了一种高效的相机感知 3D 解码器，可将视频潜在解码为 3D 表示以进行奖励量化。具体来说，视频潜伏与相机姿势一起被解码为 3D 高斯。在这个过程中，相机位姿不仅作为输入，还作为投影参数。视频潜伏和相机姿势之间的不对准将导致 3D 结构中的几何扭曲，从而导致渲染模糊。基于这个属性，我们明确优化了渲染的新颖视图和真实视图之间的像素级一致性作为奖励。为了适应随机性，我们进一步引入了一个可见性项，它有选择地仅监督通过几何扭曲导出的确定性区域。在 RealEstate10K 和 WorldScore 基准上进行的大量实验证明了我们提出的方法的有效性。项目页面：\\href{https://a-bigbao.github.io/CamPilot/}{CamPilot 页面}。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15897v1",
        "guidislink": true,
        "link": "https://arxiv.org/abs/2601.15897v1",
        "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling"
        },
        "updated": "2026-01-22T12:24:26Z",
        "updated_parsed": [
            2026,
            1,
            22,
            12,
            24,
            26,
            3,
            22,
            0
        ],
        "links": [
            {
                "href": "https://arxiv.org/abs/2601.15897v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "href": "https://arxiv.org/pdf/2601.15897v1",
                "rel": "related",
                "type": "application/pdf",
                "title": "pdf"
            }
        ],
        "summary": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "",
            "value": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums."
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ],
        "published": "2026-01-22T12:24:26Z",
        "published_parsed": [
            2026,
            1,
            22,
            12,
            24,
            26,
            3,
            22,
            0
        ],
        "arxiv_primary_category": {
            "term": "cs.CV"
        },
        "authors": [
            {
                "name": "Zhaoqi Su"
            },
            {
                "name": "Shihai Chen"
            },
            {
                "name": "Xinyan Lin"
            },
            {
                "name": "Liqin Huang"
            },
            {
                "name": "Zhipeng Su"
            },
            {
                "name": "Xiaoqiang Lu"
            }
        ],
        "author_detail": {
            "name": "Xiaoqiang Lu"
        },
        "author": "Xiaoqiang Lu",
        "journal": "arXiv: Nerf",
        "title_cn": "ThermoSplat：具有特征调制和几何解耦的跨模态 3D 高斯喷射",
        "abstract_cn": "集成 RGB 和热红外数据的多模态场景重建对于在不同照明和天气条件下实现稳健的环境感知至关重要。然而，将 3D 高斯散射 (3DGS) 扩展到多光谱场景仍然具有挑战性。当前的方法通常难以充分利用多模态数据的互补信息，通常依赖于往往忽略跨模态相关性或利用无法自适应处理频谱之间复杂的结构相关性和物理差异的共享表示的机制。为了解决这些限制，我们提出了 ThermoSplat，这是一种新颖的框架，可以通过主动特征调制和自适应几何解耦来实现深度光谱感知重建。首先，我们引入了一种跨模态 FiLM 调制机制，该机制可动态调节热结构先验的共享潜在特征，从而通过可靠的跨模态几何线索有效地指导可见纹理合成。其次，为了适应特定于模态的几何不一致，我们提出了一种模态自适应几何解耦方案，该方案可以学习独立的不透明度偏移并为热分支执行独立的光栅化通道。此外，还采用混合渲染管道将显式球谐函数与隐式神经解码相集成，确保语义一致性和高频细节保留。对 RGBT-Scenes 数据集的大量实验表明，ThermoSplat 在可见光和热光谱方面均实现了最先进的渲染质量。"
    }
]